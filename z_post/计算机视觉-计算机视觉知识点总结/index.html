<!DOCTYPE html>













<html class="theme-next gemini" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">

<meta name="google-site-verification" content="jgw73iXouBAJcOuff0yi9vdSNDecBSOUXacsHJszpmo">
<meta name="baidu-site-verification" content="xyf9WD2vvl">











<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/apple-icon-57x57.png?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_body":"slideDownIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="章节导航篇机器学习: 基本概念, 逻辑回归, KNN, 支持向量机, 决策树, 朴素贝叶斯, 降维, 聚类, XGBoost, Bagging 深度学习: 优化方法, 初始化方法, 损失函数, 激活函数, 正则化, 归一化, 感受野, 全连接层, 卷积层, 反卷积层, 空洞卷积, 池化层, 训练问题 网络结构: AlexNet, VGGNet, InceptionV1, InceptionV2/3">
<meta name="keywords" content="知识点梳理,计算机视觉,面试">
<meta property="og:type" content="article">
<meta property="og:title" content="【置顶】计算机视觉知识点总结">
<meta property="og:url" content="https://hellozhaozheng.github.io/z_post/计算机视觉-计算机视觉知识点总结/index.html">
<meta property="og:site_name" content="从零开始的BLOG">
<meta property="og:description" content="章节导航篇机器学习: 基本概念, 逻辑回归, KNN, 支持向量机, 决策树, 朴素贝叶斯, 降维, 聚类, XGBoost, Bagging 深度学习: 优化方法, 初始化方法, 损失函数, 激活函数, 正则化, 归一化, 感受野, 全连接层, 卷积层, 反卷积层, 空洞卷积, 池化层, 训练问题 网络结构: AlexNet, VGGNet, InceptionV1, InceptionV2/3">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/PR.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/ROC.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/auc1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/auc2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/kmeans.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/optim1.gif">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/optim2.gif">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/adam_vs_sgd.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/adam.png?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_sigmoid.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_tanh.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_relu.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_leaky_relu.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_prelu.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_rrelu.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_elu.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/xor.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/l1l2_1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/l1l2_2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/norm.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/norm.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/GroupNorm_code.png?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/rf1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm_strassen.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm6.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm7.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm8.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm9.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm10.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm11.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm12.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/pool_1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/pool_2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/AlexNet.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/VGGNet.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV1_module.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV3_1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/Xception/fig2.png?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/Xception/fig34.png?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/Xception/fig5.png?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/Inception-ResNet-v1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/Inception-ResNet-v2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNet.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNet_block.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNeXt1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNeXt2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNeXt3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/DenseNet1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/DenseNet2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SqueezeNet/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SqueezeNet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SqueezeNet/tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNets/tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNets/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/Inverted.png?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/tab2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNet/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNet/tab1tab2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/Afig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/tab3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/tab5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/Afig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/tab14.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/form1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/form2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/form3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/BAM/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/BAM/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/bbox_regression1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/bbox_regression2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SPPNet/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FastR-CNN/roi_pooling1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FastR-CNN/roi_pooling3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FasterR-CNN/anchor.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FasterR-CNN/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/roi_pooling.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/tab2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://wx2.sinaimg.cn/large/d7b90c85ly1fx1toyw0xkj20kc0a5wkw.jpg">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FPN/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCN/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCN/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/R-FCN/fig1_tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/R-FCN/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CounpleNet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CounpleNet/tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CounpleNet/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCN/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCN/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCN/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCNv2/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CascadeR-CNN/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CascadeR-CNN/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://wx1.sinaimg.cn/large/d7b90c85ly1g1nf97t0s9j21jf0oyhat.jpg">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SSD/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RefineDet/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RefineDet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/M2Det/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/M2Det/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/M2Det/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/YOLOv1/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/YOLOv1/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/YOLOv2/loss.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FocalLoss/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FocalLoss/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FocalLoss/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet/fig6.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet-Lite/tab1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FSAF/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FSAF/fig5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FSAF/fig6.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FoveaBox/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FoveaBox/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig1.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/ExtremeNet/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/ExtremeNet/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/ExtremeNet/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig2.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig3.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig5.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Points/fig4.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_nvidia-smi.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_bottle.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_RooflineModel.jpg?x-oss-process=style/blog_img">
<meta property="og:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_RooflineModel2.png?x-oss-process=style/blog_img">
<meta property="og:updated_time" content="2019-08-13T04:06:55.955Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【置顶】计算机视觉知识点总结">
<meta name="twitter:description" content="章节导航篇机器学习: 基本概念, 逻辑回归, KNN, 支持向量机, 决策树, 朴素贝叶斯, 降维, 聚类, XGBoost, Bagging 深度学习: 优化方法, 初始化方法, 损失函数, 激活函数, 正则化, 归一化, 感受野, 全连接层, 卷积层, 反卷积层, 空洞卷积, 池化层, 训练问题 网络结构: AlexNet, VGGNet, InceptionV1, InceptionV2/3">
<meta name="twitter:image" content="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/PR.jpg?x-oss-process=style/blog_img">






  <link rel="canonical" href="https://hellozhaozheng.github.io/z_post/计算机视觉-计算机视觉知识点总结/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>【置顶】计算机视觉知识点总结 | 从零开始的BLOG</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?21a4899cc63d3c11a3d90ac58074a19c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">从零开始的BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">与其感慨路难行，不如马上出发</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档<span class="badge">270</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-计算机视觉">
    <a href="/categories/计算机视觉/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tripadvisor"></i> <br>计算机视觉</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-深度学习">
    <a href="/categories/深度学习/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-drupal"></i> <br>深度学习</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-caffe2">
    <a href="/categories/Caffe2/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Caffe2</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-pytorch">
    <a href="/categories/PyTorch/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-free-code-camp"></i> <br>PyTorch</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-c++">
    <a href="/categories/Cpp/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-codiepie"></i> <br>C++</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-python">
    <a href="/categories/Python/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-product-hunt"></i> <br>Python</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-项目">
    <a href="/categories/项目/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-connectdevelop"></i> <br>项目</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-cuda">
    <a href="/categories/CUDA/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-braille"></i> <br>CUDA</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-其他">
    <a href="/categories/其他/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>其他</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签<span class="badge">42</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于我</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>站内搜索(首次加载需3~5秒)</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="站内搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
    
      
    
    <a href="https://github.com/hellozhaozheng" class="github-corner" target="_blank" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#222; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg>
    
      </a>
    



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hellozhaozheng.github.io/z_post/计算机视觉-计算机视觉知识点总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZeroZone">
      <meta itemprop="description" content="吾乃闪耀的芝士蛋挞!">
      <meta itemprop="image" content="/images/avatar_zz.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="从零开始的BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">【置顶】计算机视觉知识点总结
              
            
          </h1>
        

        <div class="post-meta">
	
	     <i class="fa fa-thumb-tack"></i>
	    <font style="color:#999">置顶</font>
	    <span class="post-meta-divider">|</span>
	
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-23 20:32:39" itemprop="dateCreated datePublished" datetime="2018-10-23T20:32:39+08:00">2018-10-23</time>
            

            
          </span>

	  
  	    <span class="post-updated">
    		&nbsp; | &nbsp; 更新于
    		<time itemprop="dateUpdated" datetime="2019-08-13T12:06:55+08:00" content="2019-08-13">
      		  2019-08-13
    		</time>
  	  </span>
	  

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/计算机视觉/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/z_post/计算机视觉-计算机视觉知识点总结/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/z_post/计算机视觉-计算机视觉知识点总结/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">117k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">1:46</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="章节导航篇"><a href="#章节导航篇" class="headerlink" title="章节导航篇"></a>章节导航篇</h1><p><strong>机器学习:</strong> <a href="#基本概念">基本概念</a>, <a href="#逻辑回归">逻辑回归</a>, <a href="#KNN">KNN</a>, <a href="#支持向量机">支持向量机</a>, <a href="#决策树">决策树</a>, <a href="#朴素贝叶斯">朴素贝叶斯</a>, <a href="#降维">降维</a>, <a href="#聚类">聚类</a>, <a href="XGBoost">XGBoost</a>, <a href="#Bagging">Bagging</a></p>
<p><strong>深度学习:</strong> <a href="#优化方法">优化方法</a>, <a href="#初始化方法">初始化方法</a>, <a href="#损失函数">损失函数</a>, <a href="#激活函数">激活函数</a>, <a href="#正则化">正则化</a>, <a href="#归一化">归一化</a>, <a href="#感受野">感受野</a>, <a href="#全连接层">全连接层</a>, <a href="#卷积层">卷积层</a>, <a href="#反卷积层">反卷积层</a>, <a href="#空洞卷积">空洞卷积</a>, <a href="#池化层">池化层</a>, <a href="#训练问题">训练问题</a></p>
<p><strong>网络结构:</strong> <a href="#AlexNet">AlexNet</a>, <a href="#VGGNet">VGGNet</a>, <a href="#InceptionV1">InceptionV1</a>, <a href="#InceptionV2/3">InceptionV2/3</a>, <a href="#InceptionV4">InceptionV4</a>, <a href="#InceptionV4">InceptionResNet</a>,  <a href="#Xception">Xception</a>, <a href="#ResNet">ResNet</a>, <a href="#ResNeXt">ResNeXt</a>, <a href="#DenseNet">DenseNet</a>, <a href="#SqueezeNet">SqueezeNet</a>, <a href="#MobileNet">MobileNet</a>, <a href="#MobileNetV2">MobileNetV2</a>, <a href="#ShuffleNet">ShuffleNet</a>, <a href="#ShuffleNetV2">ShuffleNetV2</a>, <a href="#SENet">SENet</a>,</p>
<p><strong>目标检测:</strong></p>
<ul>
<li>Two-Stage:</li>
<li>One-Stage:</li>
<li>IoU:</li>
<li>FPN:</li>
<li>Anchor Free:<br><a href="#NMS">NMS</a>, <a href="#R-CNN">R-CNN</a>, <a href="#Fast R-CNN">Fast R-CNN</a>, <a href="#Faster R-CNN">Faster R-CNN</a>, <a href="#Mask R-CNN">Mask R-CNN</a>, <a href="#FPN">FPN</a>, <a href="#R-FCN">R-FCN</a>, <a href="#Deformable ConvNets V1">Deformable ConvNets V1</a>, <a href="#Deformable ConvNets V2">Deformable ConvNets V2</a> <a href="#Cascade R-CNN">Cascade R-CNN</a>, <a href="#SSD">SSD</a>, <a href="#YOLOv1">YOLOv1</a>, <a href="#YOLOv2">YOLOv2</a>, <a href="#YOLOv3">YOLOv3</a>, <a href="#Focal Loss">Focal Loss</a>, <a href="#TridentNet">TridentNet</a>, <a href="#DenseBox">DenseBox</a>, <a href="#CornerNet">CornerNet</a>, <a href="#CornerNet-Lite">CornerNet-Lite</a>, <a href="#FSAF">FSAF</a>, <a href="#FoveaBox">FoveaBox</a>, <a href="#FCOS">FCOS</a>, <a href="#ExtremeNet">ExtremeNet</a>, <a href="#CenterNet">CenterNet</a>, <a href="#CenterNet Objects as Points">CenterNet(Objects as Points)</a></li>
</ul>
<h1 id="机器学习篇"><a href="#机器学习篇" class="headerlink" title="机器学习篇"></a>机器学习篇</h1><p><span id="基本概念"></span></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="TP-TN-FP-FN-及各种比值代表的含义"><a href="#TP-TN-FP-FN-及各种比值代表的含义" class="headerlink" title="TP, TN, FP, FN 及各种比值代表的含义"></a>TP, TN, FP, FN 及各种比值代表的含义</h3><p>混淆矩阵(二分类, 多分类的情况类似):</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">-</th>
<th style="text-align:center">Positive Predictions</th>
<th style="text-align:center">Negative Predictions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Positive Label</td>
<td style="text-align:center">TP</td>
<td style="text-align:center">FN</td>
</tr>
<tr>
<td style="text-align:center">Negative Label</td>
<td style="text-align:center">FP</td>
<td style="text-align:center">TN</td>
</tr>
</tbody>
</table>
</div>
<p>Accuracy(准确率):</p>
<script type="math/tex; mode=display">ACC = \frac{TP+TN}{FP+FN+TP+TN} = \frac{预测正确样本数}{总样本数}</script><p>Precision(精度):</p>
<script type="math/tex; mode=display">PRE = \frac{TP}{TP+FP} = \frac{预测正确的正样本数量}{所有预测为正样本(不论对错)的样本数量}</script><p>TPR(召回率):</p>
<script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN} = \frac{预测正确的正样本数量}{总的正样本数量}</script><p>FPR(误诊率, 误报率):</p>
<script type="math/tex; mode=display">FPR = \frac{FP}{FP+TN} = \frac{预测错误的负样本数量}{总的负样本数量}</script><p>FNR(漏报率):</p>
<script type="math/tex; mode=display">FNR = \frac{FN}{FN+TN} = \frac{将正样本错认成负样本的数量}{预测成负样本的总数量}</script><h3 id="PR-ROC-AUC"><a href="#PR-ROC-AUC" class="headerlink" title="PR, ROC, AUC"></a>PR, ROC, AUC</h3><h4 id="PR-曲线"><a href="#PR-曲线" class="headerlink" title="PR 曲线"></a>PR 曲线</h4><p>精度又名查准率, 关心的是 “查出的所有正例中, 哪些正例是查对的”<br>召回率又名查全率, 关心的是 “对于所有的正例, 正确查出了多少个”</p>
<p>这二者是一对矛盾的度量, 因为我们很容易知道:</p>
<ul>
<li>如果我们希望查准率高, 那么可以认为是 “只有当十成把握认为其是正例时, 才将其挑出”.</li>
<li>而如果我们希望召回率高, 那么可以认为是 “宁错杀一百, 不放过一个”. 查准率和查全率的曲线又叫 PR 曲线, 如下图所示.</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/PR.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2FPR.jpg"></div></p>
<p>通常情况下, 如果一个学习器的 PR 曲线被另一个学习器 <strong>完全包住</strong>. 那么我们就认为后者的性能优于前者. 当二者存在交叉时, 我们可以通过四种方式来确定学习器的优劣:</p>
<ol>
<li>计算 PR 曲线与横纵坐标轴围成的面积, 面积越大越好;</li>
<li>利用平衡点 (BEP, 查准率=查全率), BEP 越大越好;</li>
<li>利用 $F_1$ 度量, $F_1$ 越大越好. $F_1$ 度量实际上就是当 $\beta = 1$ 时的 $F_\beta$ 度量, $F_1$ 度量认为查准率和查全率的重要性相同.<script type="math/tex; mode=display">\frac{1}{F_1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R}), F_1 = \frac{2\times P \times R}{P+R}</script></li>
<li>利用 $F_\beta$ 度量, 当 $\beta &lt; 1$ 时, 查准率权(精度)重更大, 当 $\beta &gt; 1$ 时, 查全率(召回率)权重更大. $F_\beta$ 的计算公式来自于加权调和平均数.<script type="math/tex; mode=display">\frac{1}{F_\beta} = \frac{1}{1+\beta^2}(\frac{1}{P} + \frac{\beta^2}{R}), F_\beta = \frac{(1+\beta^2)\times P \times R}{\beta^2 \times P + R}</script></li>
</ol>
<h4 id="ROC-曲线"><a href="#ROC-曲线" class="headerlink" title="ROC 曲线"></a>ROC 曲线</h4><p>很多学习器是为测试样本产生一个实值或概率预测, 然后将这个预测值与一个分类阈值进行比较, 若大于阈值分为正例, 否则分为负例, 因此分类过程可以看做是选取一个合适的截断点, 那么到底什么样的截断点更合适. ROC 正是从这个角度来研究学习器好坏的工具.</p>
<p>ROC 曲线的纵坐标和横坐标分别是召回率(查全率)和假正率(误诊率), 下图为 ROC 曲线图, 实际任务中会利用有限个测试样本来绘制 ROC 图, 所以产生的大多不是平滑的曲线.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/ROC.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2FROC.jpg"></div></p>
<p>和 PR 曲线类似, 如果一个学习器的 ROC 曲线被另一个学习器 “完全包住”, 则后者的性能优于前者. 对于 ROC 曲线来说, 我们需要先观察其是否没有剧烈的波动, 如果曲线不够平滑, 波动距离, 那么猜测可能发生了过拟合现象, 如果 ROC 是光滑的, 这个时候就可以通过曲线的 AUC (area under curve) 来判断模型的好坏, AUC 越大的模型越好. 因为 AUC 越大, 说明模型可以在较低的误诊率下达到较高的召回率.</p>
<h4 id="绘制-ROC-曲线"><a href="#绘制-ROC-曲线" class="headerlink" title="绘制 ROC 曲线"></a>绘制 ROC 曲线</h4><p>假设已经得出一系列样本被划分为正类的概率，然后按照大小排序，下图是一个示例，图中共有20个测试样本，”Class” 一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），”Score” 表示每个测试样本属于正样本的概率。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/auc1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fauc1.jpg"></div></p>
<p>接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/auc2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fauc2.jpg"></div></p>
<p>计算 FPR 和 TPR 的方法:</p>
<ol>
<li>先统计 20 组样本中, 有多少个正样本, 有多少个负样本, 假设为别为 $N$ 和 $M$, $N+M=20$.</li>
<li>对于每一组样本, 选取它的 socre 作为阈值, 那么在 score 之上的预测结果中(包括当前样本), 我们都认为将其预测成正样本, 那么, 假设这些样本的实际正样本数量为 $n$, 实际负样本数量为 $m$, 则 TPR 和 FPR 分别为: $n/N$, $m/M$.</li>
<li>按照 score 的大小从高到低重复执行 2 过程</li>
</ol>
<p><strong>注意1:</strong> 上面的 Score 使用了经过 softmax 转换后的值, 也可以看做是概率, 但是实际上, <strong>我们在画 ROC 图的时候, 只需要获取到样本之间的相对大小即可</strong>, 所以我们可以直接使用为经过 softmax 转换的 socre 来画图, 得到的 ROC 和 AUC 不会发生变化.<br><strong>注意2:</strong> 上图中, 我们可以看到 ROC 曲线是根据一组组的 FPR 和 TPR 的值得到的, 因此呈现出 “阶梯状”, 并且面积均为矩形. <strong>但是如果预测出来的 score 存在有相同分数的情况, 那么就会出现梯形, 此时不利用直接计算面积.</strong></p>
<ul>
<li>形成矩形的原因: 每新增一个样本, 它要么只增加 FPR, 要么只增加 TPR, 所以曲线要么向右延伸, 要么向上延伸;</li>
<li>形成梯形的原因: 如果说, 有多个样本的 score 相同, 那么当选择该 score 作为阈值时, 就会同时增加 TPR 和 FPR, 因此曲线就会想右上方延伸, 故而形成梯形.</li>
</ul>
<h4 id="AUC-的含义及计算"><a href="#AUC-的含义及计算" class="headerlink" title="AUC 的含义及计算"></a>AUC 的含义及计算</h4><p><strong>含义:</strong> 首先, AUC 的值是处于 [0, 1] 区间内的, 实际上, 从 ROC 的绘制过程中我们就可以看出, AUC 可以看做是一个概率值, 它代表着当我们随机挑选一个正样本和负样本时, 当前学习器对正样本的预测值大于负样本的概率, 也就是说当前学习器将这个正样本排在负样本前面的概率. 我们通常希望学习器的 AUC 的值越大越好, 实际上也就是希望当我们随机拿出一个正样本和负样本时, 学习器都能够将这个正样本排在负样本的前面, 很容易知道, 当 AUC 的值为 1 时, 我们按照 score 排列正负样本, 所有的正样本都会处在负样本的前面, 这个时候我们很容易找到一个阈值使得学习器的分类完全正确; 当 AUC 的值为 0 时, 此时按照 score 排列, 所有的负样本都处在正样本的前面, 这个时候学习器的性能最差.(我们这里讲的 socre 代表样本是正样本的概率, 因此不能反过来用). 也就是说, 它衡量的是模型将一堆样本进行分类的能力.</p>
<p><strong>计算:</strong><br>一般情况下, AUC 的计算都是指 ROC 曲线的 AUC, 其计算方式有以下三种</p>
<ol>
<li>计算每一段小矩形的面积, 之后求和. 这种计算方式只适用于 score 均不相等时, ROC 曲线只有小矩形构成的情况, 如果 score 有相等的情况出现, 那么就需要计算梯形, 比较麻烦.</li>
<li>根据 AUC 的含义, 我们可以从另一个角度来计算 AUC. 那就是随机挑选一个正样本和一个负样本, 正样本排在负样本前面的概率就是 AUC 的值. 对于有限的样本数量, 我们认为频率可以近似概率. 因此, 对于具有 $N$ 个正样本, $M$ 个负样本组成的测试集合, 我们总共有 $N\times M$ 组不同的正负样本组合, 然后只需要统计这 $N\times M$ 个组合中, 正样本 score 大于负样本 score 的数量, 将其除以 $N\times M$ 即可. 算法时间复杂度为 $O(N^2M^2)$</li>
<li>方法二的复杂度较高, 方法三用一种更高效的方式来计算. 我们先将样本按照 score 排列, 那么可以知道, 第一个样本与任意的样本组合, 都是前者的 score 大, 于是, 我们按照排列的顺序, 为每一个样本赋予 rank, 其中第一位的 rank 为 $N+M$, 最后一位的 rank 为 1. 然后, 利用下面的公式计算 AUC:</li>
</ol>
<script type="math/tex; mode=display">AUC = \frac{\sum_{i\in positive} rank_i - \frac{N(1+N)}{2}}{M\times N}</script><p>上面的公式非常好理解, 其中分母 $N\times M$ 代表了所有可能的 (正样本, 负样本) 的组合数量, 分子中 rank 的值实际上代表了该样本能够产生多少种 <strong>前大后小</strong> 的组合, 而这些组合中需要减去 $\frac{N(1+N)}{2}$ 中 (正样本, 正样本) 的组合情况. 另外, 需要特别注意的是, 在存在 score 相等的情况时, 需要赋予其相同的 rank (不论正负样本), 具体操作就是将这些样本原来的 rank 求和去平均.</p>
<p><span id="逻辑回归"></span></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p><a href="../机器学习-逻辑回归">逻辑回归与线性回归</a></p>
<h3 id="逻辑回归和线性回归的定义"><a href="#逻辑回归和线性回归的定义" class="headerlink" title="逻辑回归和线性回归的定义"></a><a href="../机器学习-逻辑回归/#逻辑回归和线性回归的定义">逻辑回归和线性回归的定义</a></h3><p><strong>逻辑回归定义</strong><br>逻辑回归通常用来解决二分类问题(也可以解决多分类问题), 用于估计某种事物的可能性. 我通过 Logistic 函数将拟合函数的输出值归一化到 (0, 1) 之间, 我们可以将其认为是分类为 1 类的预测概率. Logistic 函数公式(和 Sigmoid 函数形式形式相同)如下:</p>
<script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}</script><p>Logistic(Sigmoid) 函数的求导公式有一个特性: $g’(z) = g(z)(1 - g(z))$.</p>
<p><strong>线性回归定义:</strong><br>线性回归通常是解决连续数值预测问题, 利用数理统计的回归分析, 来确定变量之间的相互依赖关系. 其公式通常表示如下:</p>
<script type="math/tex; mode=display">y = \theta^T x + e</script><h3 id="逻辑回归和线性回归的区别和联系"><a href="#逻辑回归和线性回归的区别和联系" class="headerlink" title="逻辑回归和线性回归的区别和联系"></a><a href="../机器学习-逻辑回归/#逻辑回归与线性回归的联系和区别">逻辑回归和线性回归的区别和联系</a></h3><p><strong>联系</strong><br>逻辑回归本质上还是线性回归, 只是在特征到结果的映射中加入了一层函数映射, 即先把特征线性求和, 然后使用函数 $g(z)$ 将连续结果值映射到 (0, 1) 之间, 我们将线性回归模型的表达式代入到 Logistic(Sigmoid) 函数之中, 就得到了逻辑回归的表达式:</p>
<script type="math/tex; mode=display">h_\theta (x) = g(\theta^T x) = \frac{1}{1 + e^{-\theta^Tx}}</script><p>实际上, 我们将逻辑回归的公式整理一下, 就可以得到 $log\frac{p}{1-p} = \theta^T x$, 其中, $p = P(y=1 | x)$, 也就是将给定输入 $x$ 预测为正样本的概率. 那也就是说, 逻辑回归实际上也可以看做是对 $log\frac{p}{1-p}$ 的线性回归. 但是在关于逻辑回归的讨论中, 我们均认为 $y$ 是因变量, 而不是 $\frac{p}{1-p}$, 这便引出逻辑回归与线性回归最大的区别, 即 <strong>逻辑回归中的因变量是离散的</strong>, 而 <strong>线性回归中的因变量是连续的</strong>. 并且在自变量 $x$ 和超参数 $\theta$ 确定的情况下, 逻辑回归可以看做是广义线性模型在因变量 $y$ 服从二元分布时的一个特殊情况, 而使用最小二乘法求解线性回归时, 我们认为因变量 $y$ 服从正态分布.</p>
<p><strong>区别:</strong><br>最本质区别: 逻辑回归处理的是分类问题, 线性回归处理的是回归问题. 在逻辑回归中, 因变量的取值是一个 <strong>二元分布(不是二项分布)</strong>. 而线性回归中实际上求解的是对真实函数关系的一个近似拟合.</p>
<h3 id="对于一个二分类问题-如果数据集中存在一些离异值-在不清洗数据的情况下-选择逻辑回归还是-SVM-为什么"><a href="#对于一个二分类问题-如果数据集中存在一些离异值-在不清洗数据的情况下-选择逻辑回归还是-SVM-为什么" class="headerlink" title="对于一个二分类问题, 如果数据集中存在一些离异值, 在不清洗数据的情况下, 选择逻辑回归还是 SVM? 为什么?"></a><a href="../机器学习-逻辑回归/#对于一个二分类问题">对于一个二分类问题, 如果数据集中存在一些离异值, 在不清洗数据的情况下, 选择逻辑回归还是 SVM? 为什么?</a></h3><p>用 SVM, 因为 SVM 的分类只与支持向量有关, 所以对离异值的忍受能力更强.</p>
<h3 id="逻辑回归与-SVM-的区别是什么"><a href="#逻辑回归与-SVM-的区别是什么" class="headerlink" title="逻辑回归与 SVM 的区别是什么"></a><a href="../机器学习-逻辑回归/#逻辑回归和 SVM 的区别是什么">逻辑回归与 SVM 的区别是什么</a></h3><p>两种方法都是常见的分类算法, 从目标函数上看, 区别在于逻辑回归采用的是 log 损失, 而 SVM 采用的是 hinge 损失. 这两个损失函数的目的都是增加对分类影响较大的数据点的权重, 减少与分类关系较小的数据点的权重. SVM 的处理方法是只考虑支持向量, 也就是和分类最相关的少数点, 去学习分类器. 而逻辑回归通过非线性映射, 大大减小了离分类平面较远的点的权重, 相对提升了与分类最相关的数据点的权重. 两者的根本目的都是一样的. 此外, 根据需要, 两个方法都可以增加不同的正则化项, 如 L1, L2 等. 所以在很多实验中, 两种算法的结果是很接近的.<br>但是逻辑回归相对来说模型更加简单, 并且实现起来, 特别是大规模线性分类时比较方便. 而 SVM 的实现和优化相对来说复杂一些, 但是 SVM 的理论基础更加牢固, 有一套结构化风险最小化的理论基础, 另外, SVM 转化成对偶问题后, 分类只需要计算与少数几个支持向量的距离即可, 这在进行复杂核函数计算时有时很明显, 能够大大简化模型和计算量</p>
<p>损失函数: 逻辑回归和 SVM 的损失函数分别为:</p>
<script type="math/tex; mode=display">\text{Logistic: } \frac{1}{n} \sum^n_{i=1} - \log g(y_i [ w_0 + x^T_i w_1]) + \frac{\lambda}{2}\| w_1 \|</script><script type="math/tex; mode=display">\text{SVM: } \frac{1}{n}\sum^n_{i=1}(1 - y_i[w_0 + x^T_i w_1])^{+} + \frac{\lambda}{2}\| w_1 \|</script><p>上式中, $g(z) = \frac{1}{1 + exp^(-z)}$. 可以看出, 逻辑回归采用的是对数损失(log loss), 而 SVM 采用的是铰链损失(hinge loss), 即:</p>
<ul>
<li>LR 损失: $Loss(z) = log(1 + exp(-z))$</li>
<li>SVM 损失: $Loss(z) = (1 - z)^{+}$</li>
</ul>
<p>逻辑回归产出的是概率值, 而 SVM 只能产出正负类, 因此 LR 的预估结果更容易解释.<br>SVM 主要关注的是 “支持向量”, 也就是和分类最相关的少数点, 即关注局部关键信息; 而逻辑回归是在全局进行优化的, 这导致 SVM 天然比逻辑回归有更好的泛化能力, 防止过拟合.</p>
<h3 id="逻辑回归和-SVM-哪个是参数模型-哪个是非参数模型"><a href="#逻辑回归和-SVM-哪个是参数模型-哪个是非参数模型" class="headerlink" title="逻辑回归和 SVM 哪个是参数模型, 哪个是非参数模型"></a>逻辑回归和 SVM 哪个是参数模型, 哪个是非参数模型</h3><p><strong>LR 是参数模型, SVM 是非参数模型</strong></p>
<p>定义: 参数模型通常假设总体随机变量服从某一个分布, 该分布由一些参数确定(比如正态分布的均值和方差), 在此基础上构建的模型称为参数模型; 非参数模型对于总体的分布不做任何假设, 只是知道总体是一个随机变量, 其分布是存在的(分布中也可能存在参数), 但是无法知道其分布的形式, 更不知道分布的相关参数, 只有在给定一些样本的条件下, 能够依据非参数统计的方法进行推断. 因此, <strong>问题中有没有参数, 并不是参数模型和非参数模型的区别. 其主要区别在于总体的分布形式是否已知.</strong> 为何强调 “参数” 与 “非参数”, 主要原因在于参数模型的分布可以由参数直接确定.</p>
<p>参数算法包括两部分: (1) 选择目标函数的形式; (2) 从训练数据中学习目标函数的系数. LR 会预先假设目标函数(直线或其他), 因此它是参数模型. 其他参数模型还有: 线性成分分析, 感知机.<br>参数模型的优点:</p>
<ul>
<li>简单: 理论容易理解, 结果容易解释</li>
<li>快速: 参数模型的学习和训练速度较快</li>
<li>数据更少: 通常不需要大量的数据也可以较好的拟合?</li>
</ul>
<p>参数模型的缺点:</p>
<ul>
<li>约束: 以选定函数形式的方式来学习本身就限制了模型的解空间</li>
<li>有限的复杂度: 通常只能应对简单的问题</li>
<li>拟合度小: 实际中通常无法和潜在的目标函数温和.</li>
</ul>
<p>非参数算法: 对于目标函数的形式不作过多的假设. 当有用许多数据而先验知识很少时, 非参数学习通常很有用, 因为此时不需要关注参数的选取. 常用的非参数算法包括: K 最近邻, 决策树, SVM, 朴素贝叶斯, 神经网络.<br>非参数算法的优点:</p>
<ul>
<li>可变性: 可以拟合许多不同的函数形式</li>
<li>模型强大: 对于目标函数不作假设或者作微小的假设</li>
<li>表现良好: 对于预测结果表现通常较好</li>
</ul>
<p>非参数算法的局限性:</p>
<ul>
<li>需要更多数据: 对于拟合目标函数需要更多的训练数据</li>
<li>速度慢: 参数更多, 所以训练通常较慢</li>
</ul>
<h3 id="逻辑回归和-SVM-分别适合在什么情况下使用"><a href="#逻辑回归和-SVM-分别适合在什么情况下使用" class="headerlink" title="逻辑回归和 SVM 分别适合在什么情况下使用"></a>逻辑回归和 SVM 分别适合在什么情况下使用</h3><p>令 $n = 特征数量$, $m = 训练样本数量$, 则:</p>
<ul>
<li>如果 $n &gt; m$, 则使用 LR 或者不带核函数的 SVM, 因为特征数相对于训练样本数已经够大了, 使用线性模型就能取得不错的效果, 不需要过于复杂的模型;</li>
<li>如果 $n &lt; m$, 则使用 SVM(高斯核函数), 因为在训练样本数量足够大而特征数量较小的情况下, 可以通过复杂核函数的 SVM 来获得更好的预测性能, 而且因为训练样本数量并没有达到百万级, 使用复杂核函数的 SVM 也不会导致运算过慢;</li>
<li>如果 $n &lt;&lt; m$, 此时因为训练样本数量特别大, 使用复杂核函数的 SVM 会导致训练过慢, 因此应该考虑通过引入更多特征, 然后使用 LR 或者不带核函数的 SVM 来训练更好的模型</li>
</ul>
<p>在实际使用中, 通常当数据非常非常大(几个 G, 几万维度特征), 跑不动 SVM 时, 用 LR. 如今数据量大幅增加, 相比来说 LR 反而用的更多了.</p>
<p><span id="KNN"></span></p>
<h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h3 id="简述-KNN-算法的原理"><a href="#简述-KNN-算法的原理" class="headerlink" title="简述 KNN 算法的原理"></a>简述 KNN 算法的原理</h3><h3 id="KNN-算法进行分类和回归时的区别"><a href="#KNN-算法进行分类和回归时的区别" class="headerlink" title="KNN 算法进行分类和回归时的区别"></a>KNN 算法进行分类和回归时的区别</h3><p>KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。而KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。</p>
<h3 id="KNN-算法的三要素"><a href="#KNN-算法的三要素" class="headerlink" title="KNN 算法的三要素"></a>KNN 算法的三要素</h3><ol>
<li>K 值的选取: 对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。</li>
<li>距离的度量方式: 曼哈顿距离($p=1$), 欧氏距离($p=2$), 闵可夫斯基距离($p=3$)<script type="math/tex; mode=display">D(x, y) = \sqrt[p] {(\vert x_1 - y_1 \vert)^p + (\vert x_2 - y_2 \vert)^p + ... + (\vert x_n - y_n \vert)^p} = \sqrt[p] {\sum_{i=1}^n (\vert x_i - y_i \vert)^p}</script></li>
<li>分类决策规则: 一般使用 “多数表决法”</li>
</ol>
<h3 id="KNN-算法是否可微"><a href="#KNN-算法是否可微" class="headerlink" title="KNN 算法是否可微"></a>KNN 算法是否可微</h3><p>不可微, 因为求 K 近邻(argmax)的操作是不可微的</p>
<h3 id="编程实现-KNN-算法"><a href="#编程实现-KNN-算法" class="headerlink" title="编程实现 KNN 算法"></a>编程实现 KNN 算法</h3><p><strong>暴力实现:</strong>(实际算法中会采用更高效的实现, 如 KD 树实现, Ball 树实现等)<br>计算预测样本和所有训练集中的样本的距离，然后选出最小的k个距离即可，接着多数表决，很容易做出预测. 复杂度过大, 通常样本量成千上万.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.train_data = <span class="keyword">None</span></span><br><span class="line">        self.train_label = <span class="keyword">None</span></span><br><span class="line">        self.dists = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, train_label)</span>:</span></span><br><span class="line">        <span class="comment"># train_data: [N x d] 的数组, N 为样本数量, d 为样本维度</span></span><br><span class="line">        <span class="comment"># train_label: [N x 1] 的数组, N 为样本数量, 1 代表类别标签维度(1维)</span></span><br><span class="line">        self.train_data = train_data</span><br><span class="line">        self.train_label = train_label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, test_data, k=<span class="number">3</span>, distance=<span class="string">'l2'</span>)</span>:</span></span><br><span class="line">        test_num = test_data.shape[<span class="number">0</span>]</span><br><span class="line">        preds = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(test_num):</span><br><span class="line">            <span class="keyword">if</span> distance == <span class="string">'l1'</span>:</span><br><span class="line">                self.dists = L1Distance(test_data[i])</span><br><span class="line">            <span class="keyword">elif</span> distance == <span class="string">'l2'</span>:</span><br><span class="line">                self.dists = L2Distance(test_data[i])</span><br><span class="line">            dists_argsort = np.argsort(self.dists)</span><br><span class="line">            knearest_class = self.train_label(dists_argsort)[<span class="number">1</span>:k+<span class="number">1</span>]</span><br><span class="line">            class_count = np.bincount(knearest_class)</span><br><span class="line">            pred = np.argmax(class_count)</span><br><span class="line">            preds.append(pred)</span><br><span class="line">        <span class="keyword">return</span> np.array(preds)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">L1Distance</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(np.abs(self.train_data - x), axis=<span class="number">1</span>)<span class="comment"># 千万不要忘了求和时 axis=1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">L2Distance</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sqrt(np.sum(np.square(self.train_data-x), axis=<span class="number">1</span>))<span class="comment"># 千万不要忘了求和时 axis=1</span></span><br></pre></td></tr></table></figure></p>
<p><span id="支持向量机"></span></p>
<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><p><a href="../机器学习-SVM深入解析">SVM深入解析</a></p>
<h3 id="简述-SVM-的基本概念和原理"><a href="#简述-SVM-的基本概念和原理" class="headerlink" title="简述 SVM 的基本概念和原理"></a><a href="../机器学习-SVM深入解析/#简述 SVM 的基本概念和原理">简述 SVM 的基本概念和原理</a></h3><p>最简单的 SVM 从线性分类器导出, 根据最大化样本点分类间隔的目标, 我们可以得到线性可分问题的 SVM 目标函数. 然后可以利用拉格朗日乘子法得到其对偶问题, 并根据 KKT 条件和 SMO 算法就可以高效的求出超平面的解. 但是实际任务中, 原始样本空间内也许并不存在一个能正确划分两类样本的超平面. 因此, 我们需要利用核函数将样本从原始空间映射到一个更高为的特征空间, 使得样本在这个特征空间内线性可分. 核函数的选择对于支持向量机的性能至关重要. 但是现实任务中往往很难确定合适的核函数使得训练样本在特征空间内线性可分, 因此, 我们引入了 “软间隔” 的概念, 也就是松弛变量和惩罚因子, 其基本思想就是, 允许支持向量机在一些样本上出错, 并对违反约束条件的训练样本进行惩罚. 所以, 最终的优化目标就是在最大化间隔的同时, 使得不满足约束的样本尽可能地少.</p>
<h3 id="SVM-推导过程"><a href="#SVM-推导过程" class="headerlink" title="SVM 推导过程"></a><a href="../机器学习-SVM深入解析/#SVM 推导过程">SVM 推导过程</a></h3><p>过长, 建议点击题目链接查看</p>
<h3 id="SVM-如何解决线性不可分问题"><a href="#SVM-如何解决线性不可分问题" class="headerlink" title="SVM 如何解决线性不可分问题"></a><a href="../机器学习-SVM深入解析/#SVM 如何解决线性不可分问题">SVM 如何解决线性不可分问题</a></h3><p>解决线性不可分的基本思路有两个:</p>
<ul>
<li>加入松弛变量和惩罚因子, 找到 <strong>相对较好</strong> 的超平面, 这里的 <strong>相对较好</strong> 可以理解为 <strong>尽可能</strong> 的将数据正确分类</li>
<li>使用核函数, 将低维的数据映射到更高维的空间, 使得高维空间中的数据是线性可分的, 那么在高维空间吗使用线性分类模型即可.</li>
</ul>
<h3 id="为什么SVM的分类结果仅依赖于支持向量"><a href="#为什么SVM的分类结果仅依赖于支持向量" class="headerlink" title="为什么SVM的分类结果仅依赖于支持向量?"></a><a href="../机器学习-SVM深入解析/#为什么SVM的分类结果仅依赖于支持向量?">为什么SVM的分类结果仅依赖于支持向量?</a></h3><p>百机p53</p>
<h3 id="如何选取核函数"><a href="#如何选取核函数" class="headerlink" title="如何选取核函数"></a><a href="../机器学习-SVM深入解析/#如何选取核函数">如何选取核函数</a></h3><p>最常用的是线性核与高斯核, 也就是 Linear 核与 RBF 核. 一般情况下 RBF 效果不会差于 Linear, 但是时间上 RBF 会耗费更多.</p>
<ul>
<li>Linear 核: 主要用于线性可分的情形. 参数少, 速度快, 对于一般数据, 分类效果已经很理想了.</li>
<li>RBF 核: 主要用于线性不可分的情况. 参数多, 分类结果非常依赖于参数. 有很多人是通过训练数据的交叉验证来寻找合适的参数, 不过这个过程比较耗时. 个人体会是: 使用 libsvm, 默认参数, RBF 核比 Linear 核效果稍差. 通过进行大量参数的尝试, 一般能找到比 linear 核更好的效果. 至于到底该采用哪种核, 要根据具体问题和数据分析, 需要多尝试不同核以及不同参数. 如果特征提取的好, 包含的信息量足够大, 很多问题都是线性可分的. 当然, 如果有足够的时间去寻找合适的 RBF 核参数, 应该能取得更好的效果.</li>
</ul>
<p>吴恩达的观点:</p>
<ol>
<li>如果 Feature 的数量很大, 跟样本数量差不多, 这时候可以使用 LR 或者是 Linear Kernel 的 SVM. (因为核函数需要计算内积, 两两样本都得算, 所以样本过多的话时间消耗太大, 很明显高斯核比线性核复杂的多)</li>
<li>如果 Feature 的数量比较小, 样本数量一般, 不算大也不算小, 就选用 SVM + Gaussian Kernel</li>
<li>如果 Feature 的数量比较小, 而样本数量比较多, 就需要手工添加一些 feature, 使之变成第一种情况.</li>
</ol>
<h3 id="为什么说高斯核函数将原始特征空间映射成了无限维空间"><a href="#为什么说高斯核函数将原始特征空间映射成了无限维空间" class="headerlink" title="为什么说高斯核函数将原始特征空间映射成了无限维空间?"></a><a href="../机器学习-SVM深入解析/#为什么说高斯核函数将原始特征空间映射成了无限维空间?">为什么说高斯核函数将原始特征空间映射成了无限维空间?</a></h3><p><a href="https://blog.csdn.net/lin_limin/article/details/81135754" target="_blank" rel="noopener">https://blog.csdn.net/lin_limin/article/details/81135754</a></p>
<h3 id="核函数中不同参数的影响"><a href="#核函数中不同参数的影响" class="headerlink" title="核函数中不同参数的影响"></a><a href="../机器学习-SVM深入解析/#核函数中不同参数的影响">核函数中不同参数的影响</a></h3><p><a href="https://blog.csdn.net/lin_limin/article/details/81135754" target="_blank" rel="noopener">https://blog.csdn.net/lin_limin/article/details/81135754</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247484495&amp;idx=1&amp;sn=4f3a6ce21cdd1a048e402ed05c9ead91&amp;chksm=fdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&amp;mid=2247484495&amp;idx=1&amp;sn=4f3a6ce21cdd1a048e402ed05c9ead91&amp;chksm=fdb699d8cac110ce53f4fc5e417e107f839059cb76d3cbf640c6f56620f90f8fb4e7f6ee02f9&amp;scene=21#wechat_redirect</a></p>
<h3 id="既然深度学习技术性能表现以及全面超越-SVM-SVM-还有存在的必要吗"><a href="#既然深度学习技术性能表现以及全面超越-SVM-SVM-还有存在的必要吗" class="headerlink" title="既然深度学习技术性能表现以及全面超越 SVM, SVM 还有存在的必要吗?"></a><a href="../机器学习-SVM深入解析/#既然深度学习技术性能表现已经全面超越 SVM, SVM 还有存在的必要吗?">既然深度学习技术性能表现以及全面超越 SVM, SVM 还有存在的必要吗?</a></h3><p>待补充</p>
<p><span id="决策树"></span></p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p><span id="朴素贝叶斯"></span></p>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p><span id="降维"></span></p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p><span id="聚类"></span></p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="简述-K-Means-聚类的原理"><a href="#简述-K-Means-聚类的原理" class="headerlink" title="简述 K-Means 聚类的原理"></a>简述 K-Means 聚类的原理</h3><p>K-Means算法是无监督的聚类算法，它对于给定的样本集, 会按照样本之间的距离大小, 将样本集划分为 K 个簇, 让簇内的样本尽量紧密的连在一起, 而让簇间的距离尽量的大.<br>如果用数据表达式表示, 则假设簇划分为 $(C_1, C_2, …, C_k)$, 则我们的目标是最小化平方误差 $E$:</p>
<script type="math/tex; mode=display">E = \sum_{i=1}^{k} \sum_{x\in C_i} \Vert x - u_i \Vert_2^2</script><p>其中 $\mu_i$ 是簇 $C_i$ 的均值向量, 有时也称为质心, 表达式为:</p>
<script type="math/tex; mode=display">\mu_i = \frac{1}{C_i} \sum_{x\in C_i} x</script><h3 id="K-Means-算法的优点和缺点"><a href="#K-Means-算法的优点和缺点" class="headerlink" title="K-Means 算法的优点和缺点"></a>K-Means 算法的优点和缺点</h3><p>K-Means 算法对于初始点非常敏感, 如果初始点全部处于一点, 则最终可能无法迭代到合适的解, 原因在于所有的样本都会属于某一个点类, 此时其他的点会无法更新.</p>
<h3 id="K-Means-实现流程"><a href="#K-Means-实现流程" class="headerlink" title="K-Means 实现流程"></a>K-Means 实现流程</h3><p>想直接求上式的最小值并不容易，这是一个NP难的问题，因此只能采用启发式的迭代方法, 如下图所示。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/kmeans.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fkmeans.jpg"></div></p>
<ol>
<li>上图a表达了初始的数据集，假设k=2。</li>
<li>在图b中，我们 <strong>随机</strong> 选择了两个k类所对应的类别质心，即图中的红色质心和蓝色质心</li>
<li>然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，如图c所示，经过计算样本和红色质心和蓝色质心的距离，我们得到了所有样本点的第一轮迭代后的类别。</li>
<li>此时我们对我们当前标记为红色和蓝色的点分别求其新的质心，如图d所示，新的红色质心和蓝色质心的位置已经发生了变动。</li>
<li>图e和图f重复了我们在图c和图d的过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。最终我们得到的两个类别如图f。</li>
</ol>
<p><strong>算法实现流程:</strong> 输入是样本集 $D = \{x_1, x_2, …, x_m\}$, 聚类的簇为 $k$, 最大的迭代次数为 $N$.</p>
<ol>
<li>从数据集 $D$ 中随机选择 $k$ 个样本作为初始的 $k$ 个质心向量: $\{\mu_1, \mu_2, …, \mu_k \}$</li>
<li>对于 $n = 1, 2, …, N$<ol>
<li>将簇的划分 $C$ 初始化为 $C_t = \emptyset, t = 1, 2, …, k$;</li>
<li>对于 $i=1,2,…,m$, 计算样本 $x_i$ 和各个质心向量 $\mu_j(j=1, 2, …, k)$ 的距离, $d_{ij} = \Vert x_i - \mu_j \Vert_2^2$, 将 $x_i$ 标记为距离最小的簇所对应的类别 $\lambda_i$. 更新 $C_{\lambda_i} = C_{\lambda_i} \cup \{x_i\}$</li>
<li>对于 $j = 1, 2, …, k$, 对 $C_j$ 中的所有样本点重新计算新的质心 $\mu_j = \frac{1}{\vert C_j \vert} \sum_{x\in C_j} x$</li>
<li>如果所有的 $k$ 个质心向量都不再发生变化, 则可提前跳出循环, 无序执行 N 次迭代</li>
</ol>
</li>
<li>输出簇划分 $C = \{C_1, C_2, …, C_k \}$</li>
</ol>
<h3 id="K-Means-常规实现代码"><a href="#K-Means-常规实现代码" class="headerlink" title="K-Means 常规实现代码"></a>K-Means 常规实现代码</h3><p>输入为 <code>[N, d]</code> 维度的样本集合, 其中, <code>N</code>代表样本的数量, <code>d</code>代表每个样本的维度, 算法的实现思路为:</p>
<ol>
<li><code>__init__</code>函数初始化相关变量</li>
<li><code>fit</code>函数首先确定最初的<code>n_cluster</code>个<code>center</code>;</li>
<li><code>fit</code>函数循环执行以下两个过程:<ol>
<li>更新各个点到<code>centers</code>的距离</li>
<li>更新各个<code>centers</code>的位置</li>
</ol>
</li>
</ol>
<p>PyTorch 实现: <a href="https://www.jianshu.com/p/1c000d9296ae" target="_blank" rel="noopener">https://www.jianshu.com/p/1c000d9296ae</a></p>
<p>Numpy 实现:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KMeans</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k, max_iter=<span class="number">1000</span>, stop_var=<span class="number">1e-03</span>, dist_type=<span class="string">'l1'</span>)</span>:</span></span><br><span class="line">        self.num_cluster = k</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.stop_var = stop_var</span><br><span class="line">        self.dist_type = dist_type</span><br><span class="line">        self.variance = <span class="number">10</span> * stop_var</span><br><span class="line">        self.dists = <span class="keyword">None</span></span><br><span class="line">        self.labels = <span class="keyword">None</span></span><br><span class="line">        self.centers = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, samples)</span>:</span></span><br><span class="line">        <span class="comment"># 随机初始化 centers 点, 更好的方法可以使用 sklearn 的 kmeans++ 初始化方法</span></span><br><span class="line">        init_row = np.random.randint(<span class="number">0</span>, samples.shape[<span class="number">0</span>], self.num_cluster)</span><br><span class="line">        init_points = samples[init_row]</span><br><span class="line">        self.centers = init_points</span><br><span class="line">        <span class="keyword">for</span> cur_iter <span class="keyword">in</span> range(self.max_iter):</span><br><span class="line">            self.update_dists(samples) <span class="comment"># 更新样本到各个 centers 的距离, 同时更新每个样本点对应的center类</span></span><br><span class="line">            self.update_centers(samples) <span class="comment"># 更新各个 centers</span></span><br><span class="line">            <span class="keyword">if</span> self.variance &lt; self.stop_var: <span class="comment"># 如果 centers 更新停止, 则提前退出</span></span><br><span class="line">                print(<span class="string">"cur_iter:"</span>, cur_iter)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">l1_distance</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(np.abs(sample - self.centers), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">l2_distance</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sqrt(np.sum(np.square(sample - self.centers), axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_dists</span><span class="params">(self, samples)</span>:</span></span><br><span class="line">        labels = np.empty(samples.shape[<span class="number">0</span>]) <span class="comment"># shape: [N, 1]</span></span><br><span class="line">        dists = np.empty((<span class="number">0</span>, self.num_cluster)) <span class="comment"># shape: [N, n_cluster]</span></span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">            <span class="keyword">if</span> self.dist_type == <span class="string">'l1'</span>:</span><br><span class="line">                dist = self.l1_distance(sample)</span><br><span class="line">            <span class="keyword">elif</span> self.dist_type == <span class="string">'l2'</span>:</span><br><span class="line">                dist = self.l2_distance(sample)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">assert</span> <span class="keyword">True</span></span><br><span class="line">            labels[i] = np.argmin(dist) <span class="comment"># 距离最小的center就是该样本对应的类</span></span><br><span class="line">            dists = np.vstack((dists, dist[np.newaxis, :])) <span class="comment"># 将该样本对应的各个center距离加入到dists中</span></span><br><span class="line">        <span class="keyword">if</span> self.dists <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.variance = np.sum(np.abs(self.dists - dists))</span><br><span class="line">        self.dists = dists <span class="comment"># 更新</span></span><br><span class="line">        self.labels = labels <span class="comment"># 更新</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_centers</span><span class="params">(self, samples)</span>:</span></span><br><span class="line">        centers = np.empty((<span class="number">0</span>, samples.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_cluster):</span><br><span class="line">            mask = (self.labels == i)</span><br><span class="line">            center_samples = samples[mask]</span><br><span class="line">            <span class="keyword">if</span> len(center_samples) != <span class="number">0</span>:</span><br><span class="line">                center = np.mean(center_samples, axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">else</span>:<span class="comment"># 说明 centers 点出现了重复情况, 导致某个点类没有样本与它对应, 这种情况要报警告, 因为最终解通常无法形成合理的簇划分</span></span><br><span class="line">                center = self.centers[i]</span><br><span class="line">            centers = np.vstack((centers, center[np.newaxis, :]))</span><br><span class="line">        self.centers = centers</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    samples = np.random.rand(<span class="number">1000</span>, <span class="number">4</span>) <span class="comment"># 样本量 N=1000, 每个样本特征维度为 d=10</span></span><br><span class="line">    print(samples.shape)</span><br><span class="line">    num_cluster = <span class="number">5</span> <span class="comment"># 簇个数为 5</span></span><br><span class="line">    kmeans = KMeans(num_cluster)</span><br><span class="line">    kmeans.fit(samples)</span><br><span class="line">    print(kmeans.centers)</span><br></pre></td></tr></table></figure></p>
<h3 id="K-Means-实现-anchor-划分"><a href="#K-Means-实现-anchor-划分" class="headerlink" title="K-Means 实现 anchor 划分"></a>K-Means 实现 anchor 划分</h3><p>实现堆 anchor 的 K-Means 聚类算法, 核心思想和常规的聚类实现相同, 不同之处在于 “距离” 的定义, 和更新<code>centers</code>的方式, 具体来说就是:</p>
<ul>
<li>距离: 用<code>1-iou</code>定义每个框与<code>centers</code>之间的距离, iou越大, 距离越近</li>
<li>更新: <code>centers</code>更新时, 采用<code>np.median</code>选择当前类中的中位数box作为新的 center.</li>
</ul>
<p><strong>注意, 由于确定 anchor 时, 我们仅仅只需要样本框的宽和高这两个信息即可, 不需要知道样本的具体location, 所以, 在传入<code>boxes</code>时, 我们传入的是 [N, 2] 维度的数据, 其中 N 代表 box 的数量, 2 代表每个 box 的 (w, h)</strong></p>
<p>K-Means 算法 numpy 实现:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_iou</span><span class="params">(box, clusters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculates the Intersection over Union (IoU) between a box and k clusters.</span></span><br><span class="line"><span class="string">    :param box: tuple or array, shifted to the origin (i. e. width and height)</span></span><br><span class="line"><span class="string">    :param clusters: numpy array of shape (k, 2) where k is the number of clusters</span></span><br><span class="line"><span class="string">    :return: numpy array of shape (k, 0) where k is the number of clusters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    min_w = np.minimum(box[<span class="number">0</span>], clusters[:, <span class="number">0</span>])</span><br><span class="line">    min_h = np.minimum(box[<span class="number">1</span>], clusters[:, <span class="number">1</span>])</span><br><span class="line">    intersection = min_w * min_h</span><br><span class="line">    <span class="keyword">if</span> (np.count_nonzero(intersection == <span class="number">0</span>) &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Boxes has no areaes"</span>)</span><br><span class="line">    box_area = box[<span class="number">0</span>] * box[<span class="number">1</span>]</span><br><span class="line">    clusters_area = clusters[:, <span class="number">0</span>] * clusters[:, <span class="number">1</span>]</span><br><span class="line">    iou = (intersection) / (box_area + clusters_area - intersection)</span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_avg_iou</span><span class="params">(boxes, clusters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculates the average Intersection over Union (IoU) between a numpy array of boxes and k clusters.</span></span><br><span class="line"><span class="string">    :param boxes: numpy array of shape (r, 2), where r is the number of rows</span></span><br><span class="line"><span class="string">    :param clusters: numpy array of shape (k, 2) where k is the number of clusters</span></span><br><span class="line"><span class="string">    :return: average IoU as a single float</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> np.mean([np.max(cal_iou(boxes[i], clusters)) <span class="keyword">for</span> i <span class="keyword">in</span> range(boxes.shape[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans_anchor</span><span class="params">(boxes, n_clusters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculates k-means clustering with the Intersection over Union (IoU) metric.</span></span><br><span class="line"><span class="string">    :param boxes: numpy array of shape (r, 2), where r is the number of rows</span></span><br><span class="line"><span class="string">    :param n_clusters: number of clusters</span></span><br><span class="line"><span class="string">    :return: numpy array of shape (k, 2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n_boxes = boxes.shape[<span class="number">0</span>]</span><br><span class="line">    distances = np.empty([n_boxes, n_clusters]) <span class="comment"># 占位, 记录每个box与center的iou距离</span></span><br><span class="line">    last_clusters = np.empty([n_boxes])</span><br><span class="line">    clusters = boxes[np.random.choice(n_boxes, n_clusters, replace=<span class="keyword">False</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_boxes):</span><br><span class="line">            distances[i] = <span class="number">1</span> - cal_iou(boxes[i], clusters) <span class="comment"># 距离定义为 1 - iou, iou越大, 距离越近</span></span><br><span class="line"></span><br><span class="line">        nearest_clusters = np.argmin(distances, axis = <span class="number">1</span>) <span class="comment"># 选择距离最近的作为box的类别</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (last_clusters == nearest_clusters).all(): <span class="comment"># 如果距离不再更新, 则退出</span></span><br><span class="line">            <span class="keyword">return</span> clusters</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>  i <span class="keyword">in</span> range(n_clusters): <span class="comment"># 更新 centers, 使用所有属于该类的中位数box作为新center</span></span><br><span class="line">            clusters[i] = np.median(boxes[(nearest_clusters==i)], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        last_clusters = nearest_clusters</span><br></pre></td></tr></table></figure></p>
<p>下面我们以 VOC 数据格式为例, 来使用上面的 K-Means 算法<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> kmeans_anchor <span class="keyword">import</span> kmeans_anchor, cal_avg_iou</span><br><span class="line"></span><br><span class="line"><span class="comment"># VOC 格式的数据标注</span></span><br><span class="line">ANNOTATIONS_PATH = <span class="string">"/media/zerozone/WinD/DataSets/VOC/VOCdevkit/VOC2007/Annotations/"</span></span><br><span class="line"><span class="comment"># ANNOTATIONS_PATH = "/media/zerozone/WinD/Ubuntu/BackUp/Works/Competition/DC/VOC_trans/Annotations"</span></span><br><span class="line">CLUSTERS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(path)</span>:</span></span><br><span class="line">	dataset = []</span><br><span class="line">	<span class="keyword">for</span> xml_file <span class="keyword">in</span> glob.glob(<span class="string">"&#123;&#125;/*xml"</span>.format(path)):</span><br><span class="line">		tree = ET.parse(xml_file)</span><br><span class="line"></span><br><span class="line">		height = int(tree.findtext(<span class="string">"./size/height"</span>))</span><br><span class="line">		width = int(tree.findtext(<span class="string">"./size/width"</span>))</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> obj <span class="keyword">in</span> tree.iter(<span class="string">"object"</span>):</span><br><span class="line">			xmin = int(obj.findtext(<span class="string">"bndbox/xmin"</span>)) / width</span><br><span class="line">			ymin = int(obj.findtext(<span class="string">"bndbox/ymin"</span>)) / height</span><br><span class="line">			xmax = int(obj.findtext(<span class="string">"bndbox/xmax"</span>)) / width</span><br><span class="line">			ymax = int(obj.findtext(<span class="string">"bndbox/ymax"</span>)) / height</span><br><span class="line"></span><br><span class="line">			dataset.append([xmax - xmin, ymax - ymin])</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.array(dataset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"loading dataset:"</span>, ANNOTATIONS_PATH)</span><br><span class="line">data = load_dataset(ANNOTATIONS_PATH)</span><br><span class="line">print(<span class="string">"loaded successfully!"</span>)</span><br><span class="line">print(<span class="string">"calculating kmeans: cluster_num=&#123;&#125;"</span>.format(CLUSTERS))</span><br><span class="line">out = kmeans_anchor(data, CLUSTERS) <span class="comment"># 计算 kmeans</span></span><br><span class="line">print(<span class="string">"Accuracy: &#123;:.2f&#125;%"</span>.format(cal_avg_iou(data, out) * <span class="number">100</span>)) <span class="comment"># 计算 avg_iou</span></span><br><span class="line">print(<span class="string">"Boxes:\n &#123;&#125;"</span>.format(out))</span><br><span class="line"></span><br><span class="line">ratios = np.around(out[:, <span class="number">0</span>] / out[:, <span class="number">1</span>], decimals=<span class="number">2</span>).tolist()</span><br><span class="line">print(<span class="string">"Ratios:\n &#123;&#125;"</span>.format(sorted(ratios)))</span><br></pre></td></tr></table></figure></p>
<p><span id="XGBoost"></span></p>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p><span id="Bagging"></span></p>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><ul>
<li><a href="../机器学习-聚类分析/#简单介绍常用的聚类方法">简单介绍常用的聚类方法</a></li>
</ul>
<h2 id="待整理"><a href="#待整理" class="headerlink" title="待整理"></a>待整理</h2><p>各种机器学习算法的应用场景分别是什么(比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型)<br><a href="https://www.zhihu.com/question/26726794/answer/151282052" target="_blank" rel="noopener">https://www.zhihu.com/question/26726794/answer/151282052</a></p>
<h1 id="深度学习篇"><a href="#深度学习篇" class="headerlink" title="深度学习篇"></a>深度学习篇</h1><p><span id="优化方法"></span></p>
<h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p><a href="../深度学习-各种优化方法整理总结">各种优化方法整理总结</a></p>
<p><strong>梯度下降:</strong> SGD, Momentum, Nesterov, Adagrad, Adadelta, RMSprop, Adam, Adamax<br><strong>牛顿法:</strong><br><strong>拟牛顿法:</strong><br><strong>共轭梯度法:</strong></p>
<h3 id="简述各种优化方法的概念及其优缺点"><a href="#简述各种优化方法的概念及其优缺点" class="headerlink" title="简述各种优化方法的概念及其优缺点"></a><a href="../深度学习-各种优化方法整理总结/#简述各种优化方法的概念及其优缺点">简述各种优化方法的概念及其优缺点</a></h3><script type="math/tex; mode=display">\theta_t = \theta_{t-1} + \Delta \theta_t</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">公式</th>
<th style="text-align:left">优化方法简述</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SGD</td>
<td style="text-align:center">$g_t = \nabla_{\theta_{t-1}} f(\theta_{t-1})$ <br> $\Delta \theta_t = - \eta \times g_t$</td>
<td style="text-align:left">每一次都计算mini-batch的梯度, 然后对参数进行更新. 公式中的 $\eta$ 是学习率, $g_t$ 是当前 batch 的梯度</td>
<td style="text-align:left">在合理的学习率和相应的衰减策略下, 通常能够优化到一个不错的点, 配合下面的 Momentum, 通常可以获得比自适应方法更优的点</td>
<td style="text-align:left">(1) 因为要兼顾整个神经网络中所有参数的训练效果, 因此对学习率的选择比较敏感. (2) SGD 容易收敛到局部最优, 并且在某些情况下容易被困在鞍点( <strong>这句话是不对的, 只有在特定的 inital point 时才会被困在鞍点, 通常情况下, 我们使用 random inital point, 被困在鞍点的概率非常小, 当使用合适的初始化和步长时, 几乎不会出现鞍点问题</strong> ); (3) 参数的更新仅仅依赖于当前 batch 中的数据, 当数据分布波动较大时, 更新往往不够稳定</td>
</tr>
<tr>
<td style="text-align:center">SGD+Momentum</td>
<td style="text-align:center">$g_t = \nabla_{\theta_{t-1}} f(\theta_{t-1})$ <br> $m_t = \mu \times m_{t-1} + g_t$ <br> $\Delta \theta_t = -\eta \times m_t$</td>
<td style="text-align:left">公式中的 $\mu$ 为动量因子, 通常取值 0.9 或 0.99, 借助于物理学里面动量的概念, 通过动量的积累来在相关方向上加速 SGD 优化速度, 抑制震荡, 同时有助于跳出局部最优, 进而加快收敛. <strong>注意, 这里动量的添加并不是滑动平均</strong></td>
<td style="text-align:left">(1) 下降初期, 动量因子可以加速网络的训练速度; (2) 当遇到鞍点时, 梯度虽然为零, 但是动量不为零, 可以跳出鞍点(局部最优) ; (3) 在梯度改变方向时, 能够降低更新幅度, 减小震荡, 加速网络收敛; 总之, momentum 项能够在相关方向加速 SGD, 抑制震荡, 从而加快收敛</td>
<td style="text-align:left">需要人工设置学习率</td>
</tr>
<tr>
<td style="text-align:center">SGD+Nesterov</td>
<td style="text-align:center">$g_t=\nabla_{\theta_{t-1}} f(\theta_{t-1} - \eta \times \mu \times m_{t-1})$ <br> $m_t = \mu \times m_{t-1} + g_t$ <br> $\Delta \theta_t = -\eta \times m_t$</td>
<td style="text-align:left">可以看出, Nesterov 与 Momentum 公式的区别在于, 前者不是在当前的位置上求梯度, 而是根据本来计划要走的那一步提前前进一步以后, 再在新的位置上求梯度, 然后对这个新求得的梯度进行 Momentum 梯度下降计算</td>
<td style="text-align:left">(1) 先站在下一步的位置看看, 再进行更新, 使得梯度更新方向更具前瞻性; (2) 实际使用中, NAG 会比 Momentum 收敛的速度更快</td>
<td style="text-align:left">(1) 需要人工设置学习率</td>
</tr>
<tr>
<td style="text-align:center">AdaGrad</td>
<td style="text-align:center">$n_t = n_{t-1} + g^2_t$ <br> $\Delta \theta_t = -\frac{\eta}{\sqrt{n_t + \epsilon}} \times g_t$</td>
<td style="text-align:left">Adagrad相当于在学习率前面乘了一个约束项 $\frac{1}{\sqrt {n_t + \epsilon}}$, 该约束项会随着算法的不断迭代而越来越大, 那么对应学习率就会越来越小, 也就是说 Adagrad 算法在开始时是大步前进的, 而在后面则会减小步伐, 缓慢收敛</td>
<td style="text-align:left">(1) 在整个更新期间学习率不是固定的, 会随着训练过程变化; (2) 适合面对稀疏梯度; (3) 对于每一个不同的参数, 其具有不同的学习率, 当参数梯度较大时, 其约束项会使得步长变小, 返回, 会令步长变大, 动态调节</td>
<td style="text-align:left">(1) 仍然依赖于一个人工设置的全局学习率; (2) 中后期, 分母上的梯度累加和会越来越大, 使得更新提早停滞, 训练提前结束</td>
</tr>
<tr>
<td style="text-align:center">AdaDelta</td>
<td style="text-align:center">$Eg^2_t = \rho\times Eg^2_{t-1} + (1-\rho)\times g^2_t$ <br> $\Delta \theta_t = -\frac{\eta}{\sqrt{Eg^2_t + \epsilon}} g_t$ <br> $= -\frac{\eta}{RMS[g]_t} g_t$ <br> $= -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t} g_t$ <br> $RMS[\Delta \theta]_t = \sqrt{E[\Delta \theta^2]_t + \epsilon}$ <br> $E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1-\gamma)\Delta \theta^2_t$</td>
<td style="text-align:left">Adadelta是对Adagrad的扩展, 和 Adagrad 相比, 其改进是将分母约束项换成了 <strong>过去的梯度平方的衰减平均值</strong>, 相当于梯度的均方根(root mean squared, RMS), 此外, 如果将学习率也换成 $RMS[\Delta \theta]$ 的话, 甚至可以不用设置学习率了</td>
<td style="text-align:left">(1) 对 Adagrad 的扩展, 约束项只计算梯度平方一段时间内的平均值, 而不是累计值, 不容易产生太大值而使得更新提早结束; (2) 无需人工设置学习率, 可以动态改变学习率的大小;</td>
<td style="text-align:left">(1) 训练后期会反复在局部最小值附近抖动(why?)</td>
</tr>
<tr>
<td style="text-align:center">RMSprop</td>
<td style="text-align:center">$Eg^2_t = \rho\times Eg^2_{t-1} + (1-\rho)\times g^2_t$ <br> $\Delta \theta_t = -\frac{\eta}{\sqrt{Eg^2_t + \epsilon}} g_t$</td>
<td style="text-align:left">RMSprop可以算作是Adadelta的一个特例, 可以看出 RMSprop 仍然需要设置全局学习率</td>
<td style="text-align:left">(1) Adadelta 的特例, 也是对学习率添加约束, 适合处理非平稳目标, 对 RNN 效果较好</td>
<td style="text-align:left">(1) …</td>
</tr>
<tr>
<td style="text-align:center">Adam</td>
<td style="text-align:center">$m_t = \beta_1 \times m_{t-1} + (1-\beta_1) \times g_t$ <br> $n_t = \beta_2 \times n_{t-1} + (1 - \beta_2) \times g^2_t$ <br> $\hat m_t = \frac{m_t}{1-\beta_1^t}$ <br> $\hat n_t = \frac{n_t}{1- \beta_2^t}$ <br> $\Delta \theta_t = -\frac{\hat m_t}{\sqrt{\hat n_t} + \epsilon} \times \eta$</td>
<td style="text-align:left">Adam本质上是带有 <strong>动量项(分子部分)</strong> 的 RMSprop, 它利用 <strong>修正后的</strong> 梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率. 公式中, $m_t, n_t$ 分别是对梯度的一阶矩估计和二阶矩估计, 可以看做是对期望 $E g_t$, $E g^2_t$ 的估计, $\hat m_t$, $\hat n_t$ 是对 $m_t$, $n_t$ 的校正, 这样可以近似为对期望的无偏估计</td>
<td style="text-align:left">(1) 经过偏置校正后, 每一次迭代学习率都有一个确定的范围, 使得参数更新比较平稳; (2) 结合了动量 RMSprop 的优点; 既可以加速收敛, 又可以根据梯度的大小动态调节每个参数的学习步长 (3) 对内存需求较小; (4) 适用于大多非凸优化, 适用于大数据集和高维空间; (5) 超参数可以比较直观的解释, 同时只需要极少量的调参</td>
<td style="text-align:left">最终的收敛点通常比经过精心调参后的 SGD+Momentum 的收敛点差一些. 常取参数值: $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$</td>
</tr>
<tr>
<td style="text-align:center">Adamax</td>
<td style="text-align:center">$n_t = max(\nu \times, abs(g_t))$ <br> $\Delta x = -\frac{\hat m_t}{n_t + \epsilon}\times\eta$</td>
<td style="text-align:left">Adamax 是 Adam 的一种变体, 此方法对学习率的上限提供了一个更简单的范围, 可以看出, 学习率的边界范围更加简单</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
<tr>
<td style="text-align:center">Nadam(PyTorch 目前未实现该接口)</td>
<td style="text-align:center">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
</tbody>
</table>
</div>
<h3 id="各损失函数更新动画"><a href="#各损失函数更新动画" class="headerlink" title="各损失函数更新动画"></a>各损失函数更新动画</h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/optim1.gif" alt="SummaryOfComputerVision%2Foptim1.gif"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/optim2.gif" alt="SummaryOfComputerVision%2Foptim2.gif"></div></p>
<h3 id="如何选择合适的优化方法"><a href="#如何选择合适的优化方法" class="headerlink" title="如何选择合适的优化方法"></a>如何选择合适的优化方法</h3><ul>
<li>SGD+Momentum 相比于自适应优化器通常训练时间长, 但是经过多次调节后, 在好的学习率和衰减方案的情况下, 结果更优</li>
<li>AdaGrad, RMSprop, Adam 等适合希望得到快速结果的情况下使用</li>
<li>在训练较深层的网络时, 也推荐先使用 Adam 方法进行正确性验证, 然后再使用 SGD+Momentum 微调.</li>
<li>在使用 RMSprop 和 Adam 的地方, 大多可以使用 Nadam 取得更好的效果.</li>
<li>在实际训练中比较好的方法是: 先用 Adam 预训练一段时间, 然后使用 SGD+Momentum, 以达到最佳性能. Adam vs SGD 的表现通常如下图所示, 由于鲁棒性和自适应的学习速率, Adam 在一开始表现更好, 而 SGD 最终更容易达到全局最优.</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/adam_vs_sgd.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fadam_vs_sgd.jpg"></div></p>
<h3 id="一阶矩-二阶矩的计算方法及其代表的含义"><a href="#一阶矩-二阶矩的计算方法及其代表的含义" class="headerlink" title="一阶矩, 二阶矩的计算方法及其代表的含义"></a>一阶矩, 二阶矩的计算方法及其代表的含义</h3><p>原点矩: 设 $X$ 是随机变量, 则称 $E(X^k)$ 为 $X$ 的 $k$ 阶矩估计; 零阶原点矩恒为 1, 一阶原点矩代表期望, 二阶原点距为平方的期望;<br>中心距: 设 $X$ 是随机变量, 则称 $E\{ [X - E(X)]^k\}$ 为随机变量 $X$ 的 $k$ 阶中心矩; 零阶中心矩恒为 1, 一阶中心矩恒为 0, 二阶中心距为方差;</p>
<p>期望: 一阶样本原点矩来估计总体的期望<br>方差: 二阶样本中心距来估计总体的方差</p>
<p>由此可看出, Adam, RMSprop 等算法, 使用的都是一阶原点矩和二阶原点矩. 并且是利用滑动平均法来对一阶矩和二阶矩进行估计.</p>
<h3 id="简述-Adam-中使用的指数加权滑动平均法"><a href="#简述-Adam-中使用的指数加权滑动平均法" class="headerlink" title="简述 Adam 中使用的指数加权滑动平均法"></a><a href="../深度学习-各种优化方法整理总结/#简述 Adam 中使用的指数加权滑动平均法">简述 Adam 中使用的指数加权滑动平均法</a></h3><p>加权滑动平均法, 就是对观察值分别属于不同的权重, 按不同的权重来求最终的滑动平均值.  而指数加权滑动平均法就是指各个观察值的加权系数随着时间呈指数递减, 越靠近当前时刻的观察值权重越大. 公式如下:</p>
<script type="math/tex; mode=display">v_t = \beta v_{t-1} + (1 - \beta) \theta_t</script><p>上式中, $\theta_t$ 代表当前时刻的观察值, 系数 $\beta$ 代表加权下降的速率, 其值越小下降的越快, $v_t$ 代表当前时刻的指数加权滑动平均值.</p>
<p>PS: 在数学中一般会以 $\frac{1}{e}$ 来作为一个临界值, 小于该值的加权系数对应的值不作考虑. 因此, 当 $\beta = 0.9$ 时, $0.9^{10}$ 约等于 $\frac{1}{e}$, 认为此时是约 10 个数值的加权平均.</p>
<p><strong>偏差修正:</strong> 当初始化 $v_0 = 0$ 时, 由于初始化的值太小, 导致初期的滑动平均值偏小, 随着时间的增长, 初期的值影响减小, 滑动平均值才逐渐正常. 为了让初期的滑动平均值也相对正常, 我们利用下面的式子进行修正:</p>
<script type="math/tex; mode=display">v_t = \frac{\beta v_{t-1} + (1 - \beta)\theta_t}{1 - \beta^t}</script><h3 id="Adam-算法的原理机制"><a href="#Adam-算法的原理机制" class="headerlink" title="Adam 算法的原理机制"></a>Adam 算法的原理机制</h3><p>Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。</p>
<p>Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。</p>
<p>Adam 算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即：</p>
<p>适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。<br>均方根传播（RMSprop）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。</p>
<h3 id="Adam-算法与相关的-AdaGrad-和-RMSprop-方法有什么区别"><a href="#Adam-算法与相关的-AdaGrad-和-RMSprop-方法有什么区别" class="headerlink" title="Adam 算法与相关的 AdaGrad 和 RMSprop 方法有什么区别"></a>Adam 算法与相关的 AdaGrad 和 RMSprop 方法有什么区别</h3><p>Adam 算法同时获得了 AdaGrad 和 RMSprop 算法的优点。Adam 不仅如 RMSprop 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。具体来说，算法计算了梯度的指数移动均值（exponential moving average），超参数 beta1 和 beta2 控制了这些移动均值的衰减率。</p>
<p>移动均值的初始值和 beta1、beta2 值接近于 1（推荐值），因此矩估计的偏差接近于 0。该偏差通过首先计算带偏差的估计而后计算偏差修正后的估计而得到提升。</p>
<p>事实上，Insofar、RMSprop、Adadelta 和 Adam 算法都是比较类似的优化算法，他们都在类似的情景下都可以执行地非常好。但是 Adam 算法的偏差修正令其在梯度变得稀疏时要比 RMSprop 算法更快速和优秀。Insofar 和 Adam 优化算法基本是最好的全局选择。同样在 CS231n 课程中，Adam 算法也推荐作为默认的优化算法。</p>
<p>虽然 Adam 算法在实践中要比 RMSprop 更加优秀，但同时我们也可以尝试 SGD+Nesterov 动量来作为 Adam 的替代。即我们通常推荐在深度学习模型中使用 Adam 算法或 SGD+Nesterov 动量法。</p>
<h3 id="Adam-算法如何调参-其常用的参数配置"><a href="#Adam-算法如何调参-其常用的参数配置" class="headerlink" title="Adam 算法如何调参, 其常用的参数配置"></a>Adam 算法如何调参, 其常用的参数配置</h3><ul>
<li>alpha：同样也称为学习率或步长因子，它控制了权重的更新比率（如 0.001）。较大的值（如 0.3）在学习率更新前会有更快的初始学习，而较小的值（如 1.0E-5）会令训练收敛到更好的性能。</li>
<li>beta1：一阶矩估计的指数衰减率（如 0.9）。</li>
<li>beta2：二阶矩估计的指数衰减率（如 0.999）。该超参数在稀疏梯度（如在 NLP 或计算机视觉任务中）中应该设置为接近 1 的数。</li>
<li>epsilon：该参数是非常小的数，其为了防止在实现中除以零（如 10E-8）。</li>
</ul>
<p>另外，学习率衰减同样可以应用到 Adam 中。原论文使用衰减率 alpha = alpha/sqrt(t) 在 logistic 回归每个 epoch(t) 中都得到更新。</p>
<p>Adam 论文建议的参数设定, 默认参数设定为：alpha=0.001、beta1=0.9、beta2=0.999 和 epsilon=10E−8。</p>
<p>我们也可以看到流行的深度学习库都采用了该论文推荐的参数作为默认设定。</p>
<ul>
<li>TensorFlow：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08.</li>
<li>Keras：lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.</li>
<li>Blocks：learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-08, decay_factor=1.</li>
<li>Lasagne：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08</li>
<li>Caffe：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08</li>
<li>MxNet：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8</li>
<li>Torch：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8</li>
</ul>
<h3 id="Adam-实现优化的过程和权重更新规则"><a href="#Adam-实现优化的过程和权重更新规则" class="headerlink" title="Adam 实现优化的过程和权重更新规则"></a>Adam 实现优化的过程和权重更新规则</h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/adam.png?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fadam.png"></div></p>
<p>如上图所示, 首先确定超参 $\eta$ (学习率步长, 也就是上面的 $\alpha$), $\beta_1$, $\beta_2$, 以及目标函数 $f(\theta)$. 然后初始化参数量 $\theta = \theta_0$, 一阶矩向量 $m_0$ 和而结局向量 $n_0$ (也就是上图中的 $v_0$). 然后进行循环迭代, 直至 $\theta_t$ 收敛, 更新过程如下:</p>
<ol>
<li>时间戳 $t$ 增 1;</li>
<li>获取目标函数在当前时间戳 $t$ 上对各个参数 $\theta$ 的梯度;</li>
<li>利用当前时间戳的梯度 $g_t$ 和滑动平均公式更新当前时间戳的有偏一阶矩估计和二阶矩估计.</li>
<li>将一阶矩估计和二阶矩估计更新成无偏的;</li>
<li>有矩估计更新参数 $\theta$, 对于每一个参数, 其矩估计的值都不尽相同, 因此起到了自适应调节学习率的作用.</li>
</ol>
<h3 id="Adam-的初始化偏差修正的推导"><a href="#Adam-的初始化偏差修正的推导" class="headerlink" title="Adam 的初始化偏差修正的推导"></a>Adam 的初始化偏差修正的推导</h3><p><a href="https://www.jiqizhixin.com/articles/2017-07-12" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-07-12</a></p>
<p>我们以二阶原点矩的无偏估计为例进行推导, 一阶原点矩的推导完全类似.</p>
<p>首先我们可以求得目标函数 $f$ 的梯度, 然后我们利用 <strong>梯度平方</strong> 的衰减率为 $beta_2$ 的指数滑动平均值来近似估计该梯度的 <strong>有偏的二阶原点矩</strong>.<br>令 $g_1, …, g_T$ 为时间序列上的梯度, 其中每个梯度都服从一个潜在的梯度分布 $g_t \sim P(g_t)$. 我们将指数滑动平均值初始化为 $n_0 = 0$, 然后用下面的公式更新滑动平均值($g_t^2$ 代表 element-wise multiply):</p>
<script type="math/tex; mode=display">n_t = \beta_2 \times n_{t-1} + (1-\beta_2)\times g_t^2 = (1-\beta_2) \sum_{i=1}{t} \beta_2^{t-i} \times g_i^2</script><p>我们希望知道时间戳 $t$ 上的指数滑动平均值的期望 $E[n_t]$ 如何与真实的二阶矩 $E[g_t^2]$ 相关联, 于是我们先对上式的两边分别取期望, 如下:</p>
<script type="math/tex; mode=display">E[n_t] = E[(1-\beta_2) \sum_{i=1}^{t} \beta_2^{t-i} \times g_i^2] = E[g_t^2]\times (1 - \beta_2) \sum_{i=1}^{t} \beta_2^{t-i} + C = E[g_t^2]\times (1 - \beta_2^t) + C'</script><p>通常 $C’$ 的值很小, 因此可以将其去除, 于是乎就得到了无偏估计的式子.</p>
<h3 id="Adam-的扩展形式-AdaMax"><a href="#Adam-的扩展形式-AdaMax" class="headerlink" title="Adam 的扩展形式: AdaMax"></a>Adam 的扩展形式: AdaMax</h3><h3 id="各种优化方法的源码实现"><a href="#各种优化方法的源码实现" class="headerlink" title="各种优化方法的源码实现"></a><a href="../深度学习-各种优化方法整理总结/#各种优化方法的源码实现">各种优化方法的源码实现</a></h3><h3 id="参考文献及其他"><a href="#参考文献及其他" class="headerlink" title="参考文献及其他"></a>参考文献及其他</h3><p>Adam 无法收敛?: <a href="https://www.jiqizhixin.com/articles/2017-12-06" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-12-06</a><br>SGD 的参数设置</p>
<p><a href="https://www.cnblogs.com/happylion/p/4172632.html" target="_blank" rel="noopener">https://www.cnblogs.com/happylion/p/4172632.html</a></p>
<p><a href="https://www.cnblogs.com/shixiangwan/p/7532830.html" target="_blank" rel="noopener">https://www.cnblogs.com/shixiangwan/p/7532830.html</a></p>
<p><a href="https://www.cnblogs.com/hlongch/p/5734105.html" target="_blank" rel="noopener">https://www.cnblogs.com/hlongch/p/5734105.html</a></p>
<p><a href="https://www.baidu.com/link?url=8EyCqGYnldJzHuqBBGagV9juEA_nhCYvRElM2Tw0lBdewSmc0qshAy_AHAEegO-wT3vLsrcY1xSDdyLOmL09Ltm_UICAFX_C02QdkkSCcWW&amp;wd=&amp;eqid=ce9adcb10004685c000000035b5d4fb6" target="_blank" rel="noopener">https://www.baidu.com/link?url=8EyCqGYnldJzHuqBBGagV9juEA_nhCYvRElM2Tw0lBdewSmc0qshAy_AHAEegO-wT3vLsrcY1xSDdyLOmL09Ltm_UICAFX_C02QdkkSCcWW&amp;wd=&amp;eqid=ce9adcb10004685c000000035b5d4fb6</a></p>
<p><a href="https://mp.weixin.qq.com/s/lh4jTYJroq6AKb2fYsP-GQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/lh4jTYJroq6AKb2fYsP-GQ</a></p>
<p><span id="初始化方法"></span></p>
<h2 id="初始化方法"><a href="#初始化方法" class="headerlink" title="初始化方法"></a>初始化方法</h2><p><a href="../深度学习-各种初始化方法深入分析">深度学习-各种初始化方法深入分析</a></p>
<p>constant, uniform, gaussian, xavier, msra(kaiming), bilinear</p>
<h3 id="各个初始化方法的形式"><a href="#各个初始化方法的形式" class="headerlink" title="各个初始化方法的形式,"></a>各个初始化方法的形式,</h3><div class="table-container">
<table>
<thead>
<tr>
<th>初始化方法</th>
<th>服从分布</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>均匀分布</td>
<td>…</td>
<td>将权值与偏置进行均匀分布的初始化</td>
</tr>
<tr>
<td>高斯分布</td>
<td>…</td>
<td>初始化为服从 $N(\mu, \sigma ^2)$ 的高斯分布</td>
</tr>
<tr>
<td>Xavier</td>
<td>$W \sim U\Big[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}  \Big]$, <br> 服从均值为 0, 方差为 $\frac{2}{n_i + n_{i+1}}$ 的均匀分布</td>
<td>公式中, $n_i$ 为本层输入的神经元个数, $n_{i+1}$ 为本层输出的神经元个数, 适合于线性激活函数(原文公式推导的假设)</td>
</tr>
<tr>
<td>MSRA(Kaiming)</td>
<td>基于均值为0, 方差为 $\sqrt{\frac{2}{(1+a^2)\times fan_{in}}}$ 的高斯分布</td>
<td>它特别适合 ReLU 激活函数(非线性)</td>
</tr>
<tr>
<td>双线性初始化</td>
<td>…</td>
<td>常用在反卷积网络里的权值初始化</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Xavier-初始化推导"><a href="#Xavier-初始化推导" class="headerlink" title="Xavier 初始化推导"></a>Xavier 初始化推导</h3><p><strong>核心理念是: 优秀的初始化方法应该使得各层的激活值和状态梯度在传播过程中的方差保持一致</strong></p>
<p>再继续推导之前, 需要先提出以下假设:</p>
<ul>
<li>首先,输入数据来说,其均值和方差应满足: $E(x)=0, Var(x)=1$ (通过BN,较容易满足)</li>
<li>权重矩阵 $W$ 和 网络输入 $x$ 互相独立</li>
<li>每层输入的每个特征方差一样</li>
<li>激活函数对称: 这主要是为了满足均值为0的假设</li>
<li>激活函数是线性的, 也就是说其导数为1</li>
<li>初始时, 状态值落在激活函数的线性区域, 即此时导数为1</li>
</ul>
<p>现在假设有一个 $n$ 维的输入向量 $\vec X$ 和一个单层的线性神经网络, 它的权重向量是 $\vec W$, 网络的输出是 $Y$, 则有:</p>
<script type="math/tex; mode=display">Y = W_1 X_1 + W_2 X_2 + ... + W_n X_n</script><p>对于每个 $W_i X_i$, 它对应的方差为:</p>
<script type="math/tex; mode=display">Var(W_i X_i) = E(X_i)^2 Var(W_i) + E(W_i)^2 Var(X_i) + Var(X_i) Var(W_i)</script><p>当输入的 $X$ 均值为 0 时(通过 BN, 较容易满足), 输出的方差就是:</p>
<script type="math/tex; mode=display">Var(W_i X_i) = Var(W_i)Var(X_i)</script><p>进一步假设 $W_i$ 和 $X_i$ 是独立同分布的, 就可以得到:</p>
<script type="math/tex; mode=display">Var(Y) = n\times Var(W_i) Var(X_i)</script><p>也就是说输出的方差跟输入的方差只是相差了一个倍数 $nVar(W_i)$, 因此, 为了保证前向传播和反向传播时每一层的方差一致, 则有下面的公式成立:</p>
<script type="math/tex; mode=display">\forall i, n_i Var[W^i] = 1</script><p>同时考虑反向传播时输入输出刚好相反, 于是就有:</p>
<script type="math/tex; mode=display">\forall i, n_{i+1} Var[W^i] =1</script><p>权衡上面两个公式, 最终给出的权重方差为:</p>
<script type="math/tex; mode=display">\forall, Var[W^i] = \frac {2}{n_i + n_{i+1}}</script><p>再由概率统计中均匀分布方差的性质反推,可以得到Xavier最终的初始化分布如下:</p>
<script type="math/tex; mode=display">W \sim U\Big[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}  \Big]</script><p><strong>Xavier在Caffe中的具体实现:</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XavierFiller</span> :</span> <span class="keyword">public</span> Filler&lt;Dtype&gt; &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">XavierFiller</span><span class="params">(<span class="keyword">const</span> FillerParameter&amp; param)</span></span></span><br><span class="line">      : Filler&lt;Dtype&gt;(param) &#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Fill</span><span class="params">(Blob&lt;Dtype&gt;* blob)</span> </span>&#123;</span><br><span class="line">    CHECK(blob-&gt;count());</span><br><span class="line">    <span class="keyword">int</span> fan_in = blob-&gt;count() / blob-&gt;num();</span><br><span class="line">    <span class="keyword">int</span> fan_out = blob-&gt;count() / blob-&gt;channels();</span><br><span class="line">    Dtype n = fan_in;  <span class="comment">// default to fan_in</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;filler_param_.variance_norm() ==</span><br><span class="line">        FillerParameter_VarianceNorm_AVERAGE) &#123;</span><br><span class="line">      n = (fan_in + fan_out) / Dtype(<span class="number">2</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;filler_param_.variance_norm() ==</span><br><span class="line">        FillerParameter_VarianceNorm_FAN_OUT) &#123;</span><br><span class="line">      n = fan_out;</span><br><span class="line">    &#125;</span><br><span class="line">    Dtype scale = <span class="built_in">sqrt</span>(Dtype(<span class="number">3</span>) / n);</span><br><span class="line">    caffe_rng_uniform&lt;Dtype&gt;(blob-&gt;count(), -scale, scale,</span><br><span class="line">        blob-&gt;mutable_cpu_data());</span><br><span class="line">    CHECK_EQ(<span class="keyword">this</span>-&gt;filler_param_.sparse(), <span class="number">-1</span>)</span><br><span class="line">         &lt;&lt; <span class="string">"Sparsity not supported by this Filler."</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>可以看出, Caffe的Xavier实现有三种选择:</p>
<p>(1) FAN_IN, 方差只考虑输入个数:</p>
<script type="math/tex; mode=display">Var[W^i] = \frac{1}{n_i}</script><p>(2) FAN_OUT, 方差只考虑输出个数:</p>
<script type="math/tex; mode=display">Var[W^i] = \frac{1}{n_{i+1}}</script><p>(3) AVERAGE, 方差同时考虑输入和输出个数:</p>
<script type="math/tex; mode=display">Var[W^i] = \frac{2}{n_i + n_{i+1}}</script><h3 id="训练时是否可以将全部参数初始化为-0"><a href="#训练时是否可以将全部参数初始化为-0" class="headerlink" title="训练时是否可以将全部参数初始化为 0"></a>训练时是否可以将全部参数初始化为 0</h3><p>不能<br>首先, 在神经网络中, 每一层中的任意神经元都是同构的, 它们拥有相同的输入, 如果再将参数全部初始化为同样的值(如0), 那么输出也就是相同的, 反过来它们的梯度也都是相同的. 那么无论是前向传播还是反向传播的取值都是完全相同的, 那么每一个神经元都是基于input做相同的事情, 这样一来, 不同的神经元根本无法学到不同的特征, 这样就失去网络学习特征的意义了</p>
<p><span id="损失函数"></span></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><a href="../深度学习-各种损失函数深入解析">深度学习-各种损失函数深入解析</a></p>
<h3 id="绝对值损失-L1"><a href="#绝对值损失-L1" class="headerlink" title="绝对值损失(L1)"></a>绝对值损失(L1)</h3><h3 id="平方损失-L2"><a href="#平方损失-L2" class="headerlink" title="平方损失(L2)"></a>平方损失(L2)</h3><script type="math/tex; mode=display">J(\theta) = \frac{1}{2} \sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script><h3 id="Softmax-交叉熵"><a href="#Softmax-交叉熵" class="headerlink" title="Softmax 交叉熵"></a>Softmax 交叉熵</h3><script type="math/tex; mode=display">y_i = softmax(z_j) = \frac{e^{z_j}}{\sum_j e^{z_j}}</script><script type="math/tex; mode=display">E(t,y) = -\sum_j t_j log y_j</script><p>上式中, $t$ 和 $y$ 分别表示神经网络的真实标签和预测输出, 第一个公式代表 softmax 激活函数.</p>
<h3 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h3><p>首先定义符号说明:</p>
<ul>
<li>$p^{(i)}$: 第i个样本类别为1的真实概率(如第i个样本真实类别为1, 则概率为1, 否则为0)</li>
<li>$o^{(i)}$: 第i个样本预测类别为1的概率</li>
<li>$p_k^{(i)}$: 第i个样本类别为k的真实概率(如第i个样本真实类别为k, 则概率为1, 否则为0)</li>
<li>$o_k^{(i)}$: 第i个样本预测类别为k的概率</li>
</ul>
<p>面对二分类问题, 损失函数形式为:</p>
<script type="math/tex; mode=display">J(W,b) = -\Big[\frac{1}{m} \sum_{i=1}{m}\big(y^{(i)}logo^{(i)} + (1-y^{(i)})log(1-o^{(i)})  \big) \Big]</script><p>面对多分类问题, 损失函数形式为:</p>
<script type="math/tex; mode=display">J(W,b) = -\Big[\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{n} y_k^{(i)} log o_k^{(i)}  \Big]</script><p>交叉熵衡量了两个分布之间的差异性, 当概率相等时, 交叉熵最大, 则损失函数达到最小(因为加了负号)</p>
<h3 id="Smooth-L1"><a href="#Smooth-L1" class="headerlink" title="Smooth L1"></a>Smooth L1</h3><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><h3 id="DR-Loss"><a href="#DR-Loss" class="headerlink" title="DR Loss"></a>DR Loss</h3><h3 id="写出多层感知机的平方误差和交叉熵误差损失函数"><a href="#写出多层感知机的平方误差和交叉熵误差损失函数" class="headerlink" title="写出多层感知机的平方误差和交叉熵误差损失函数"></a>写出多层感知机的平方误差和交叉熵误差损失函数</h3><h3 id="推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式"><a href="#推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式" class="headerlink" title="推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式"></a>推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式</h3><p><span id="激活函数"></span></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>Sigmoid, Tanh, ReLU, Leaky ReLU, PReLU, RReLU, ELU, Maxout</p>
<p><a href="../深度学习-各种激活函数深入解析">深度学习-各种激活函数深入解析</a></p>
<h3 id="写出常用的激活函数的公式及其导数形式"><a href="#写出常用的激活函数的公式及其导数形式" class="headerlink" title="写出常用的激活函数的公式及其导数形式"></a>写出常用的激活函数的公式及其导数形式</h3><div class="table-container">
<table>
<thead>
<tr>
<th>激活函数</th>
<th>形式</th>
<th>导数形式</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid</td>
<td>$f(x) =\frac{1}{1+e^{-x}}$</td>
<td>$f’(x) = f(x)(1-f(x))$</td>
</tr>
<tr>
<td>Tanh</td>
<td>$f(x) = tanh(x)= \frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>
<td>$f’(x) = 1-(f(z))^2$</td>
</tr>
<tr>
<td>ReLU</td>
<td>$f(x)=max(0,x)=\begin{cases} 0 &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}$</td>
<td>$f’(x)=\begin{cases} 0 &amp; x\leq 0 \\ 1 &amp; x&gt;0 \end{cases}$</td>
</tr>
<tr>
<td>Leaky ReLU</td>
<td>$f(x)=max(0.001 x,x)=\begin{cases} 0.001x &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}a$</td>
<td>$f(x)=max(0.001 x,x)=\begin{cases} 0.001  &amp; x \leq 0 \\ 1 &amp; x&gt;0 \end{cases}$</td>
</tr>
<tr>
<td>PReLU</td>
<td>$f(x)=max(\alpha x,x)=\begin{cases} \alpha x &amp; x \leq 0 \\ x &amp; x&gt;0 \end{cases}$</td>
<td>$f(x)=max(\alpha x,x)=\begin{cases} \alpha  &amp; x \leq 0 \\ 1 &amp; x&gt;0 \end{cases}$</td>
</tr>
<tr>
<td>RReLU</td>
<td>PReLU中的 $\alpha$ 随机取值</td>
<td>$f(x)=max(\alpha x,x)=\begin{cases} \alpha  &amp; x \leq 0 \\ 1 &amp; x&gt;0 \end{cases}$</td>
</tr>
<tr>
<td>ELU</td>
<td>$f(x) = \begin{cases} x &amp; x \geq 0 \\ \alpha(e^x - 1) &amp; x&lt;0 \end{cases}$</td>
<td>$f(x) = \begin{cases} 1 &amp; x \geq 0 \\ \alpha e^x &amp; x&lt;0 \end{cases}$</td>
</tr>
<tr>
<td>Maxout</td>
<td>$f(x) = max(w_1^T x + b_1, w_2^T x + b_2)$</td>
<td>$f(x) = max(w_1, w_2)$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="简单画出常用激活函数的图像"><a href="#简单画出常用激活函数的图像" class="headerlink" title="简单画出常用激活函数的图像"></a>简单画出常用激活函数的图像</h3><p>sigmoid:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_sigmoid.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_sigmoid.jpg"></div></p>
<p>tanh:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_tanh.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_tanh.jpg"></div></p>
<p>relu</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_relu.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_relu.jpg"></div></p>
<p>leaky_relu</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_leaky_relu.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_leaky_relu.jpg"></div></p>
<p>prelu:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_prelu.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_prelu.jpg"></div></p>
<p>rrelu:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_rrelu.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_rrelu.jpg"></div></p>
<p>elu</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/act_elu.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fact_elu.jpg"></div></p>
<h3 id="为什么需要激活函数"><a href="#为什么需要激活函数" class="headerlink" title="为什么需要激活函数?"></a>为什么需要激活函数?</h3><h4 id="标准说法"><a href="#标准说法" class="headerlink" title="标准说法"></a>标准说法</h4><p>这是由激活函数的性质所决定来, 一般来说, 激活函数都具有以下性质:</p>
<ul>
<li><strong>非线性:</strong> 首先,线性函数可以高效可靠对数据进行拟合, 但是现实生活中往往存在一些非线性的问题(如XOR), 这个时候, 我们就需要借助激活函数的非线性来对数据的分布进行重新映射, 从而获得更强大的拟合能力. (这个是最主要的原因, 其他还有下面这些性质也使得我们选择激活函数作为网络常用层)</li>
<li><strong>可微性:</strong> 这一点有助于我们使用梯度下降发来对网络进行优化</li>
<li><strong>单调性:</strong> 激活函数的单调性在可以使单层网络保证网络是凸优化的</li>
<li><strong>$f(x) \approx x:$</strong> 当激活满足这个性质的时候, 如果参数初值是很小的值, 那么神经网络的训练将会很高效(参考ResNet训练残差模块的恒等映射); 如果不满足这个性质, 那么就需要用心的设值初始值( <strong>这一条有待商榷</strong> )</li>
</ul>
<p>如果不使用激活函数, 多层线性网络的叠加就会退化成单层网络,因为经过多层神经网络的加权计算，都可以展开成一次的加权计算</p>
<h4 id="更形象的解释"><a href="#更形象的解释" class="headerlink" title="更形象的解释"></a>更形象的解释</h4><p>对于一些线性不可分的情况, 比如XOR, 没有办法直接画出一条直线来将数据区分开, 这个时候, 一般有两个选择.</p>
<p>如果已知数据分布规律, 那么可以对数据做线性变换, 将其投影到合适的坐标轴上, 然后在新的坐标轴上进行线性分类</p>
<p>而另一种更常用的办法, 就是使用激活函数, 以XOR问题为例, XOR问题本身不是线性可分的,</p>
<p><a href="https://www.zhihu.com/question/22334626" target="_blank" rel="noopener">https://www.zhihu.com/question/22334626</a></p>
<h3 id="举例说明为什么激活函数可以解决-XOR-问题"><a href="#举例说明为什么激活函数可以解决-XOR-问题" class="headerlink" title="举例说明为什么激活函数可以解决 XOR 问题"></a>举例说明为什么激活函数可以解决 XOR 问题</h3><p>首先, XOR问题如下所示:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>$x_1$</th>
<th>$x_2$</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>首先构造一个简单的神经网络来尝试解决XOR问题, 网络结构如下图所示:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/xor.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fxor.jpg"></div></p>
<p>先来看看不使用激活函数时的情况, 当不使用激活函数时, 整个网络的函数表达式如下所示:</p>
<script type="math/tex; mode=display">y = f(x_1, x_2; W, c, w ,b) = [w_1, w_2] \bigg( \bigg[\begin{matrix} W_{11} & W_{12} \\ W_{21} & W_{22} \end{matrix} \bigg] \Big[\begin{matrix} x_1 \\ x_2 \end{matrix} \Big]+ \Big[\begin{matrix} c_1 \\ c_2 \end{matrix} \Big] \bigg) + b = (w^TW^T)x + (w^Tc+b) = w'^Tx+b'</script><p>可以看到, 多层无激活函数的网络叠加, 首先是会退化成单层网络, 而对于单层网络, 求解出来的参数 $w’$ 和 $b’$ 无法对非线性的数据进行分类.</p>
<p>再来看看进入ReLU以后, 是如何解决XOR问题的, 首先, 引入后的公式如下所示:</p>
<script type="math/tex; mode=display">y = f(x_1, x_2; W, c, w ,b) = [w_1, w_2] max \bigg(0 , \bigg[\begin{matrix} W_{11} & W_{12} \\ W_{21} & W_{22} \end{matrix} \bigg] \Big[\begin{matrix} x_1 \\ x_2 \end{matrix} \Big]+ \Big[\begin{matrix} c_1 \\ c_2 \end{matrix} \Big] \bigg) + b</script><p>可以看到, 此时函数是无法化简, 因为此时引入了非线性的ReLU函数, 于是 ,就可以求得一个参数组合${w,W,c,b}$ 使得对于特定的输入$x_1, x_2$ ,能够得到正确的分类结果 $y$. 至于这个参数组合具体是什么, 这是需要通过梯度下降来不断学习的, 假如我们现在找到了一组参数如下(当然不一定是最优的), 来看看这组参数具体是如何解决XOR问题的:</p>
<script type="math/tex; mode=display">W=\bigg[ \begin{matrix} 1 & 1 \\ 1 & 1 \end{matrix} \bigg]</script><script type="math/tex; mode=display">c =\Big[ \begin{matrix} 0 \\ -1 \end{matrix}  \Big]</script><script type="math/tex; mode=display">w =\Big[ \begin{matrix} 1 \\ -1 \end{matrix} \Big]</script><script type="math/tex; mode=display">b = 0</script><p>然后, 分别将4种 $x_1, x_2$的值代入上式, 可以得到, y的值如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>$x_1$</th>
<th>$x_2$</th>
<th>计算过程</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>$[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 1 \\ 0 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>$[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 0 \\ 1 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>$[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 1 \\ 1 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>$[1, -2] max \bigg(0 , \bigg[\begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \bigg] \Big[\begin{matrix} 0 \\ 0 \end{matrix} \Big]+ \Big[\begin{matrix} 0 \\ -1 \end{matrix} \Big] \bigg) + 0$</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="各个激活函数的优缺点和适用场景是什么"><a href="#各个激活函数的优缺点和适用场景是什么" class="headerlink" title="各个激活函数的优缺点和适用场景是什么"></a>各个激活函数的优缺点和适用场景是什么</h3><p><strong>神经元饱和问题:</strong> 当输入值很大或者很小时, 其梯度值接近于0, 此时, 不管从深层网络中传来何种梯度值, 它向浅层网络中传过去的, 都是趋近于0的数, 进而引发梯度消失问题</p>
<p><strong>zero-centered:</strong> 如果数据分布不是zero-centered的话就会导致后一层的神经元接受的输入永远为正或者永远为负, 因为 $\frac{\partial f}{\partial w} = x$ , 所以如果x的符号固定,那么 $\frac{\partial f}{\partial w}$ 的符号也就固定了, 这样在训练时, weight的更新只会沿着一个方向更新, 但是我们希望的是类似于zig-zag形式的更新路径 (关于非0均值问题, 由于通常训练时是按batch训练的, 所以每个batch会得到不同的信号, 这在一定程度上可以缓解非0均值问题带来的影响, 这也是ReLU虽然不是非0 均值, 但是却称为主流激活函数的原因之一)</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>激活函数</th>
<th>优势</th>
<th>劣势</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid</td>
<td>可以将数据值压缩到[0,1]区间内</td>
<td>1. 神经元饱和问题  <br> 2.sigmoid的输出值域不是zero-centered的  <br> 3. 指数计算在计算机中相对来说比较复杂</td>
<td>在logistic回归中有重要地位</td>
</tr>
<tr>
<td>Tanh</td>
<td>1. zero-centered: 可以将 $(-\infty, +\infty)$ 的数据压缩到 $[-1,1]$ 区间内 <br> 2.完全可微分的，反对称，对称中心在原点</td>
<td>1. 神经元饱和问题 <br> 2. 计算复杂</td>
<td>在分类任务中，双曲正切函数（Tanh）逐渐取代 Sigmoid 函数作为标准的激活函数</td>
</tr>
<tr>
<td>ReLU</td>
<td>1. 在 $(0,+\infty)$ ,梯度始终为1, 没有神经元饱和问题 <br> 2. 不论是函数形式本身,还是其导数, 计算起来都十分高效 3. 可以让训练过程更快收敛(实验结果表明比sigmoid收敛速度快6倍) <br> 4. 从生物神经理论角度来看, 比sigmoid更加合理</td>
<td>1. 非zero-centered   <br> 2. 如果输入值为负值, ReLU由于导数为0, 权重无法更新, 其学习速度可能会变的很慢,很容易就会”死”掉(为了克服这个问题, 在实际中, 人们常常在初始化ReLU神经元时, 会倾向于给它附加一个正数偏好,如0.01)</td>
<td>在卷积神经网络中比较主流</td>
</tr>
<tr>
<td>LeakyReLU</td>
<td>1. 没有神经元饱和问题 <br> 2. 计算高效 <br> 3. 收敛迅速(继承了ReLU的优点) <br> 4. 神经元不会”死”掉(因为在负值时, 输出不为0, 而是x的系数0.001)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>PReLU</td>
<td>1. 没有神经元饱和问题 <br> 2. 计算高效 <br> 3. 收敛迅速(继承了ReLU的优点) <br> 4. 神经元不会”死”掉(因为在负值时, 输出不为0, 而是x的系数 $\alpha$ ) <br> 5. 相对于Leaky ReLU需要通过先验知识人工赋值, PReLU通过迭代优化来自动找到一个较好的值, 更加科学合理, 同时省去人工调参的麻烦</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ELU</td>
<td>1. 拥有ReLU所有的优点 <br> 2. 形式上更接近于zero-centered <br> 3. 在面对负值输入时,更加健壮</td>
<td>1. 引入了指数计算, 使计算变的复杂</td>
<td></td>
</tr>
<tr>
<td>Maxout</td>
<td>1. 跳出了点乘的基本形式 <br> 2. 可以看作是ReLU和Leaky ReLU 的一般化形式 3. linear Regime(啥意思?)! <br> 4. 在所有输入范围上都没有神经元饱和问题 <br> 5. 神经元永远不会”死”掉 <br> 6. 拟合能力非常强，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个maxout节点就可以拟合任意的凸函数了(相减)，前提是”隐含层”节点的个数可以任意多</td>
<td>1. 使得神经元个数和参数个数加倍, 导致优化困难</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Sigmoid-激活函数和-Softmax-激活函数的区别"><a href="#Sigmoid-激活函数和-Softmax-激活函数的区别" class="headerlink" title="Sigmoid 激活函数和 Softmax 激活函数的区别"></a>Sigmoid 激活函数和 Softmax 激活函数的区别</h3><p>sigmoid是将一个正负无穷区间的值映射到(0,1)区间, 通常用作二分类问题,而softmax把一个k维的实值向量映射成一个$(b_1,b_2,…,b_k)$ ,其中$b_i$为一个0~1的常数, 且它们的和为1, 可以看作是属于每一类的概率,通常用作多分类问题. 在二分类问题中, sigmoid和softmax是差不多的, 都是求交叉熵损失函数, softmax可以看作是sigmoid的扩展, 当类别k为2时, 根据softmax回归参数冗余的特点, 可以将softmax函数推导成sigmoid函数</p>
<p><a href="https://www.jianshu.com/p/22d9720dbf1a" target="_blank" rel="noopener">https://www.jianshu.com/p/22d9720dbf1a</a></p>
<h3 id="什么情况下-ReLU-的神经元会死亡-为什么-可以复活吗"><a href="#什么情况下-ReLU-的神经元会死亡-为什么-可以复活吗" class="headerlink" title="什么情况下 ReLU 的神经元会死亡? 为什么? 可以复活吗?"></a>什么情况下 ReLU 的神经元会死亡? 为什么? 可以复活吗?</h3><p>对于 ReLU 来说, <strong>如果某次更新过程中, 梯度值过大, 同时学习率又不小心设置的过大, 就会导致权重一下走更新过多, 那么就有一定概率出现这种情况: 对于任意的训练样本 $x_i$, 当前神经元的输出都是小于 0</strong>, 那么该神经元流向 ReLU 时, 其所有的激活值都会小于 0, 那么对应的激活输出就均为 0. 此时, 反向传播回去的梯度也都变成了 0. 此时我们就认为该神经元死亡.</p>
<p>举个确切的例子说明, 现在假设，这个神经元已经经过若干次迭代，其参数 $(\vec w, b)$ 已经迭代得趋于稳定。现在，神经元接收到了一个异常的输入 $\vec x$ 。比方说，它的某一维特征 $x_i$ 与对应的权重 $w_i$ 的乘积 $w_i x_i$ 非常大。一般来说，这意味着 $x_i$ 的绝对值非常大。于是，ReLU 的输入就会很大，对应 ReLU 的输出 $y$ 也就会很大。好了，假设这个 ReLU 神经元期望的输出（ground truth）是 $\hat y$ ，这个时候损失就会很大——损失一般是 $\vert y − \hat y\vert$ 的增函数，记为 $f(\vert y − \hat y \vert)$。</p>
<p>于是，在反向传播过程中，传递到 ReLU 的输入时的梯度就 $g$ 相应的就会很大。考虑对于偏置 $b$ 有更新</p>
<script type="math/tex; mode=display">b \rightarrow b - g\eta</script><p>考虑到 $g$ 是一个很大的正数，于是 $b$ 可能被更新为一个远小于 0 的负数。此后，对于常规输入来说，ReLU 的输入大概率是个负数。这也就是说，ReLU 大概率是关闭的。这时，梯度无法经 ReLU 反向传播至 ReLU 的输入函数。也就是说，这个神经元的参数再也不会更新了。这就是所谓的「神经元死亡」。</p>
<p>复活问题: 如果是 ReLU 的话, 几乎就无法复活了, 因为由于此时大部分的输入都是小于 0 的, 这样导致反向传播回来的梯度一直为 0, 那么就无法一点点的更新权重, 使之回归正常值, 也就无法复活神经元了. 由此也可以看出, LeakyReLU 可以在负半区一点点的更新权重, 使之有可能复活.</p>
<p>为什么实际使用中不使用 LeakyReLU?(死亡的神经元也可以起到一定的正则作用, 实际使用效果较好)</p>
<p>谈谈由异常输入导致的 ReLU 神经元死亡的问题: <a href="https://liam.page/2018/11/30/vanishing-gradient-of-ReLU-due-to-unusual-input/" target="_blank" rel="noopener">https://liam.page/2018/11/30/vanishing-gradient-of-ReLU-due-to-unusual-input/</a><br>深度学习中，使用relu存在梯度过大导致神经元“死亡”，怎么理解？: <a href="https://www.zhihu.com/question/67151971" target="_blank" rel="noopener">https://www.zhihu.com/question/67151971</a></p>
<h3 id="如何解决-ReLU-神经元死亡问题"><a href="#如何解决-ReLU-神经元死亡问题" class="headerlink" title="如何解决 ReLU 神经元死亡问题"></a>如何解决 ReLU 神经元死亡问题</h3><ol>
<li>把 ReLU 换成 LReLU 或者 PReLU，保证让激活函数在输入小于零的情况下也有非零的输出。</li>
<li>采用较小的学习率</li>
<li>采用自适应的优化算法，动态调整学习率</li>
</ol>
<h3 id="谈谈-ReLU6"><a href="#谈谈-ReLU6" class="headerlink" title="谈谈 ReLU6"></a>谈谈 ReLU6</h3><p>在Mobile v2里面使用 ReLU6，ReLU6 就是普通的ReLU但是限制最大输出值为6（对输出值做clip）</p>
<p>看mobilenetv2的论文注意到激活函数是 relu6，查了一下，有人说是方便后面参数的定点化Why the 6 in relu6?，　也有说relu6可以让模型学到稀疏性？<br>首先说明一下ReLU6，卷积之后通常会接一个ReLU非线性激活，在Mobile v1里面使用ReLU6，ReLU6就是普通的ReLU但是限制最大输出值为6（对输出值做clip），这是为了在移动端设备float16的低精度的时候，也能有很好的数值分辨率，如果对ReLU的激活范围不加限制，输出范围为0到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的float16无法很好地精确描述如此大范围的数值，带来精度损失。</p>
<p>本文提出，最后输出的ReLU6去掉，直接线性输出，理由是：ReLU变换后保留非0区域对应于一个线性变换，仅当输入低维时ReLU能保留所有完整信息。</p>
<p>在看MobileNet v1的时候，我就疑问为什么没有把后面的ReLU去掉，因为Xception已经实验证明了Depthwise卷积后再加ReLU效果会变差，作者猜想可能是Depthwise输出太浅了应用ReLU会带来信息丢失，而MobileNet还引用了Xception的论文，但是在Depthwise卷积后面还是加了ReLU。在MobileNet v2这个ReLU终于去掉了，并用了大量的篇幅来说明为什么要去掉（各种很复杂的证明，你不会想自己推一遍的= =）。</p>
<p>总之，结论就是最后那个ReLU要去掉，效果更好。</p>
<h3 id="激活函数的使用原则"><a href="#激活函数的使用原则" class="headerlink" title="激活函数的使用原则"></a>激活函数的使用原则</h3><ol>
<li>优先使用ReLU, 同时要谨慎设置初值和学习率 ( 实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都 “dead” 了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁 )</li>
<li>尝试使用LeakyReLU/PReLU/Maxout/ELU等激活函数</li>
<li>可以试下tanh, 但是一般不会有太好的结果</li>
<li><strong>不要使用sigmoid</strong></li>
</ol>
<p><span id="正则化"></span></p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>L1, L2</p>
<p><a href="../深度学习-正则化方法深入解析">深度学习-正则化方法深入解析</a></p>
<h3 id="简述-L1-正则和-L2-正则的形式"><a href="#简述-L1-正则和-L2-正则的形式" class="headerlink" title="简述 L1 正则和 L2 正则的形式"></a>简述 L1 正则和 L2 正则的形式</h3><h4 id="L1-正则"><a href="#L1-正则" class="headerlink" title="L1 正则"></a>L1 正则</h4><script type="math/tex; mode=display">\vert w\vert_1 = |w_1| + |w_2| + ... + |w_n|</script><p>L1正则项如下所示, 其中 $L_0$ 代表原始的不加正则项的损失函数, $L$ 代表加了正则项以后的损失函数, <del>$m$ 则代表训练batch的样本大小</del> :</p>
<script type="math/tex; mode=display">L = L_0 + \lambda\vert w\vert_1 = L_0 + \lambda \sum_{w}|w|</script><p>将上式对参数 $w$ 求导如下(由于正则项与 $b$ 无关, 因此参数 $b$ 的导数不变):</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + \lambda sign(w)</script><p>上式中 $sign(w)$ 表示 $w$ 的符号, 当 $w&gt;0$ 时, $sign(w)=1$ , 当 $w&lt;0$ 时, $sign(w)=-1$, 为了实现方便, 我们特意规定, 当 $w=0$ 时,  $sign(w) = 0$ , 相当于去掉了正则项.</p>
<p>因此, 权重 $w$ 的更新表达式可如下表示:</p>
<script type="math/tex; mode=display">w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta \lambda sign(w)</script><h4 id="L2-正则"><a href="#L2-正则" class="headerlink" title="L2 正则"></a>L2 正则</h4><script type="math/tex; mode=display">\vert w\vert_1 = \sqrt {w_1^2 + w_2^2 + ... + w_n^2 }</script><p>L2正则项如下所示, 其中 $L_0$ 代表原始的不加正则项的损失函数, $L$ 代表加了正则项以后的损失函数, <del>式中的系数 $\frac{1}{2}$ 主要是为了消去求导后产生的常数 $2$, 方便表示 (因为可以根据 $\lambda$ 的值来替代这些常数)</del>:</p>
<script type="math/tex; mode=display">L = L_0 + \lambda\vert w\vert^2_2 =L_0 + \lambda \sum_{w}w^2</script><p>将上式对参数 $w$ 求导如下:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + 2\lambda w</script><p>则, 权重 $w$ 的更新表达式可如下表示:</p>
<script type="math/tex; mode=display">w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta 2\lambda w</script><p>由于, $\eta, \lambda, m$ 三个值都是正的, 因此, 加上 $L2$ 正则化以后, 权重整体减小了, 这也是”权重衰减”的由来.</p>
<h3 id="L1-正则和-L2-正则的特点是什么-各有什么优势"><a href="#L1-正则和-L2-正则的特点是什么-各有什么优势" class="headerlink" title="L1 正则和 L2 正则的特点是什么? 各有什么优势?"></a>L1 正则和 L2 正则的特点是什么? 各有什么优势?</h3><p>二者共同的特点都是能够防止过拟合问题.</p>
<p>L1 的优点: 能够获得更加稀疏的模型, 权重参数最终大部分会变成 0<br>L1 的缺点: 加入 L1 后会使得目标函数在原点不可导, 需要做特殊处理</p>
<p>L2 的有点: 在任意位置都可导, 优化求解过程比较方便, 而且更加稳定<br>L2 的缺点: 无法获得真正的稀疏模型, 参数值只是缓慢趋近于0, 不是直接变成 0</p>
<p><strong>在实际应用过程中, 大部分情况下都是 L2 正则的效果更好, 因此推荐优先使用 L2 正则</strong></p>
<h3 id="L1-和-L2-的区别有哪些"><a href="#L1-和-L2-的区别有哪些" class="headerlink" title="L1 和 L2 的区别有哪些?"></a>L1 和 L2 的区别有哪些?</h3><ol>
<li>L1 相对于 L2 能够产生更加稀疏的模型</li>
<li>L2 相比于 L1 对于离异值更敏感(因为平方的原因, L2 对于大数的乘法比对小数的惩罚大)</li>
<li>L1 和 L2 梯度下降速度不同: 前者梯度恒定, 并且接接近于 0 的时候会很快将参数更新成0, 后者在接近于0 时, 权重的更新速度放缓, 使得不那么容易更新为0 (这也解释了为什么 L1 具有稀疏性)</li>
<li>二者解空间性状不同</li>
</ol>
<p>下图是 L1 和 L2 对向量中值的分布的先验假设, L1 是蓝色的线, L2 是红色的线, 可以看出, L1 的分布对于极端值更能容忍. L1 和 L2 分别对应拉普拉斯先验和高斯先验(why)</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/l1l2_1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fl1l2_1.jpg"></div></p>
<p>下图是 L1 和 L2 的示意图</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/l1l2_2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fl1l2_2.jpg"></div></p>
<h3 id="L1正则化使模型参数稀疏的原理是什么"><a href="#L1正则化使模型参数稀疏的原理是什么" class="headerlink" title="L1正则化使模型参数稀疏的原理是什么?"></a>L1正则化使模型参数稀疏的原理是什么?</h3><p>角度一: 函数图像</p>
<p>L1 在 0 处迅速下降到 0, L2 在 0 处会变得缓慢, 并不会直接更新为 0.</p>
<p>角度二: 函数叠加(梯度下降更新公式)</p>
<script type="math/tex; mode=display">w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta \lambda sign(w) \tag{L1}</script><script type="math/tex; mode=display">w \to w' = w - \eta \frac{\partial L_0}{\partial w} - \eta 2\lambda \tag{L2}w</script><p>从以上的更新表达式我们可以看出, 当 $w$ 为正时, L1正则化会将更新后的 $w$ 变的再小一点, 而当 $w$ 为负时, L1正则化会将其变的更大一点—-<strong>因此L1的正则化效果就是让 $w$ 尽可能的向 $0$ 靠近, 即最终的 $w$ 参数矩阵会变的更加稀疏</strong></p>
<p>角度三: 贝叶斯先验, “百面机器学习”<br>角度四: 解空间性状, “百面机器学习”</p>
<h3 id="为什么-L1-和-L2-分别对应拉普拉斯先验和高斯先验"><a href="#为什么-L1-和-L2-分别对应拉普拉斯先验和高斯先验" class="headerlink" title="为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验?"></a>为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验?</h3><h3 id="为什么权重矩阵稀疏可以防止过拟合"><a href="#为什么权重矩阵稀疏可以防止过拟合" class="headerlink" title="为什么权重矩阵稀疏可以防止过拟合?"></a>为什么权重矩阵稀疏可以防止过拟合?</h3><p>可以从两个方面来理解:</p>
<p>1）特征选择(Feature Selection)：稀疏性可以实现特征的自动选择, 可以在进行预测时减少无用信息的干扰</p>
<p>大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，$x_i$ 的大部分元素（也就是特征）都是和最终的输出 $y_i$ 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑 $x_i$ 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确 $y_i$ 的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为 0。</p>
<p>2）可解释性(Interpretability)：较稀疏的模型表示最终的预测结果只与个别关键特征有关, 这符合实际生活中的历史经验</p>
<p>另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型： $y=w_1 \times x_1+w_2 \times x_2+…+w_{1000} \times x_{1000}+b$ （当然了，为了让 $y$ 限定在 $[0,1]$ 的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的 $w$ 就只有很少的非零元素，例如只有 5 个非零的 $wi$ ，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果 1000 个 $w_i$ 都非 0，医生面对这 1000 种因素，累觉不爱.</p>
<h3 id="为何权重参数-w-减小就可以防止过拟合"><a href="#为何权重参数-w-减小就可以防止过拟合" class="headerlink" title="为何权重参数 $w$ 减小就可以防止过拟合?"></a>为何权重参数 $w$ 减小就可以防止过拟合?</h3><p>直观解释:</p>
<p>更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果</p>
<p>“数学一点”的解释:</p>
<p>过拟合的时候，拟合函数的系数往往非常大，为什么？因为过拟合说明拟合函数会顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大. 而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</p>
<p>就比如二维平面上的直线拟合, 过拟合时, 由于会顾及每一个点, 最终会形成非常复杂的曲线, 而较好的你和方式是一条顾及大多数点的直线即可.</p>
<h3 id="L0-范式和-L1-范式都能实现稀疏-为什么不选择用-L0-而要用-L1"><a href="#L0-范式和-L1-范式都能实现稀疏-为什么不选择用-L0-而要用-L1" class="headerlink" title="L0 范式和 L1 范式都能实现稀疏, 为什么不选择用 L0 而要用 L1?"></a>L0 范式和 L1 范式都能实现稀疏, 为什么不选择用 L0 而要用 L1?</h3><p>L0范数是指向量中非零元素的个数</p>
<p>一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解</p>
<h3 id="为什么说-L2-范式可以优化计算"><a href="#为什么说-L2-范式可以优化计算" class="headerlink" title="为什么说 L2 范式可以优化计算?"></a>为什么说 L2 范式可以优化计算?</h3><p><strong>防止过拟合:</strong></p>
<p>最基本的好处是可以提高模型泛化能力, 防止过拟合</p>
<p><strong>优化计算:</strong></p>
<p>从优化或者数值计算的角度来说, L2正则化有利于提高模型训练速度, 加快计算</p>
<p>原因: <a href="https://www.cnblogs.com/callyblog/p/8094745.html" target="_blank" rel="noopener">https://www.cnblogs.com/callyblog/p/8094745.html</a></p>
<h3 id="正则项前面的系数一般怎么设置"><a href="#正则项前面的系数一般怎么设置" class="headerlink" title="正则项前面的系数一般怎么设置?"></a>正则项前面的系数一般怎么设置?</h3><p>通常做法是一开始将正则项系数 $\lambda$ 设置为 0, 然后先确定出一个比较好的 learning rate, 接着固定该 learning rate, 给 $lambda$ 一个初始值, 如 1e-4. 然后根据验证集上的准确率, 将 $\lambda$ 增大或者缩小 10 倍, 这里增减 10 倍是粗调节, 当确定了 $\lambda$ 合适的数量级以后, 再进一步的细调节.</p>
<p><span id="归一化"></span></p>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/norm.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnorm.jpg"></div></p>
<h3 id="为什么要进行归一化"><a href="#为什么要进行归一化" class="headerlink" title="为什么要进行归一化"></a><a href="../深度学习-Batch-Normalization深入解析/#为什么要进行归一化">为什么要进行归一化</a></h3><p>在神经网络中, 数据分布对训练会产生影响. 当数据较大时, 有的激活函数比如 sigmoid 或者 tanh 的输出就会接近于 1, 此时的梯度就会接近于0, 处于激活函数的饱和阶段, 也就是说无论输入再怎么扩大, sigmoid 或者 tanh 激励函数输出值也还是接近1. 换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围敏感了. 因此我们需要用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 需要注意的是, 这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生.<br>当没有进行normalizatin时,数据的分布是任意的,那么就会有大量的数据处在激活函数的敏感区域外, 对这样的数据分布进行激活后, 大部分的值都会变成1或-1, 造成激活后的数据分布不均衡, 而如果进行了Normallizatin,  那么相对来说数据的分布比较均衡.<br><strong>一句话总结就是: 通过Normalization让数据的分布始终处在激活函数敏感的区域</strong></p>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><h4 id="简述-BN-的原理"><a href="#简述-BN-的原理" class="headerlink" title="简述 BN 的原理"></a><a href="../深度学习-Batch-Normalization深入解析/#简述 BN 的原理">简述 BN 的原理</a></h4><p>在训练深度神经网络时, 每个隐藏层的参数变化会使得后一层的输入发生变化, 从而每一批训练数据的分布也随之改变, 导致网络在每次迭代中都需要拟合不同的数据分布, 增大了训练的复杂度以及过拟合的风险. 这使得我们需要非常谨慎的去设定学习率, 初始化权重等参数更新策略. 因此, 为了保证网络层中的每一批数据都处于相同的数据分布下, 我们需要在网络的每一层输入之前增加归一化处理, 具体来说, 就是对当前 batch 中的所有元素减去均值再除以标准差. 这样的归一化操作可以对原始数据添加额外的约束, 从而可以增强模型的泛化能力, 但同时, 由于简单归一化之后的数据分布被强制为 0 均值和 1 标准差, 因此可能会破坏原始数据本身的特征. 为了能够还原原始数据分布, BN 的第二个关键操作就是引入了用于变换重构的线性偏移参数 $\gamma$ 和 $\beta$, 它们分别对简单归一化后的数据执行 scale 和 shift 操作, 可以在一定程度还原数据本身的分布特别. 总体来说, BN 的作用可以简单概括为两步, 第一步是进行归一化, 用于统一不同网络层的数据分布; 第二步变换重构, 用于恢复原始数据的特征信息.</p>
<script type="math/tex; mode=display">\mu = \frac{1}{m}\sum_{i=1}^{m}{x_i}</script><script type="math/tex; mode=display">\sigma^2 = \frac{1}{m} \sum_{i=1}^{m}{(x_i - \mu)}</script><script type="math/tex; mode=display">\hat x_i = \frac{x_i - \mu}{\sqrt{\sigma^2}+\varepsilon}</script><script type="math/tex; mode=display">\hat y_i = \gamma \hat x_i + \beta</script><h4 id="BN-解决了什么问题"><a href="#BN-解决了什么问题" class="headerlink" title="BN 解决了什么问题"></a><a href="../深度学习-Batch-Normalization深入解析/#BN 解决了什么问题">BN 解决了什么问题</a></h4><p>BN 主要解决的是深层网络中不同网络数据分布不断发生变化的问题, 也就是 Internal Covariate Shift. 该问题是指在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift. ICS 带来了两个问题:</p>
<ul>
<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>
<li>网络的训练过程容易陷入梯度饱和区,减缓网络收敛速度</li>
</ul>
<h4 id="使用-BN-有什么好处"><a href="#使用-BN-有什么好处" class="headerlink" title="使用 BN 有什么好处"></a><a href="../深度学习-Batch-Normalization深入解析/#使用 BN 有什么好处">使用 BN 有什么好处</a></h4><p>总的来说，BN通过将每一层网络的输入进行归一化, 保证输入分布的均值与方差固定在一定范围内, 减少了网络中的 Internal Covariate Shift问题, 加速了模型收敛; 并且 BN 可以使得网络对参数的设置如学习率, 初始权重等不那么敏感, 简化了调参过程, 使得网络学习更加稳定; 同时 BN 使得网络可以使用饱和性激活函数如 Sigmoid, tahh 等, 从而缓解了梯度消失问题; 最后 BN 在训练过程中由于使用的 mini-batch 的均值和方差每次都不同，因此引入了随机噪声，在一定程度上对模型起到了正则化的效果, 也就是说, BN 可以起到和 Dropout 类似的作用, 因此在使用 BN 时可以去掉 Dropout 层而不会降级模型精度.</p>
<p>原因如下: <a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34879333</a><br><strong>(1) BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</strong><br>BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，更加有利于优化的过程,提高整个神经网络的学习速度。</p>
<p><strong>(2) BN使得模型对初始化方法和网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</strong><br>在神经网络中，我们经常会谨慎地采用一些权重初始化方法（例如Xavier）或者合适的学习率来保证网络稳定训练。当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛…</p>
<p><strong>(3) BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</strong><br>在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习 $\gamma$ 与 $\beta$ 又让数据保留更多的原始信息。</p>
<p><del><strong>(4)BN具有一定的正则化效果</strong><br>在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。原作者也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。</del></p>
<p>BN 的文章中说使用 BN 后可以丢弃 Dropout, 所以大家会误认为 BN 本身具有防止过拟合的作用, 实际上这一点并没有被严格证明, 因为有可能是因为 BN 和 Dropout 之间有冲突, 其中一方带来的收益会被另一方面削弱, 16 年有一篇 paper 讨论了 BN 在防止过拟合上的作用, 其结论是 <strong>BN 没有防止过拟合的作用</strong> 它最多可以使 Overfitting 来的晚一些, 但是却无法防止它.<br><a href="https://www.zhihu.com/question/275788133" target="_blank" rel="noopener">https://www.zhihu.com/question/275788133</a></p>
<p>BN 可以看做是对参数搜索空间做的一种约束, Dropout 可以看做是另一种约束, 这两种约束有相似的作用, 但是有时候又会相互冲突, 毕竟二者所限定的约束不能完全相容, 所以共存时无法将受益叠加.</p>
<p>补充: 旷视的笔试题任务 BatchNorm 具有防止过拟合的作用</p>
<p><span id="BN 放在不同位置的区别"></span></p>
<h4 id="BN-放在不同位置的区别"><a href="#BN-放在不同位置的区别" class="headerlink" title="BN 放在不同位置的区别"></a>BN 放在不同位置的区别</h4><p>BN 通常应用于网络中的非线性激活层之前, 将卷积层的输出归一化, 使得激活层的输入服从 (0, 1) 正态分布, 避免梯度消失的问题.</p>
<p>结论: 由于目前我们对于神经网络内部网络层之间的影响机制还不是特别清楚, 所以在实际中通常就是前面放着试一下, 后面放着试一下, 然后取一个在具体场景下效果最好的. 目前在 <strong>实际上</strong>, Conv-ReLU-BN 的组合方式效果较好.<br>个人拙见: BN 放在激活层之前还是之后取决于你想要进行归一化的对象, 它更像是一个超参数, 需要通过实验结合实际场景来决定. 做了一些简单的实验, 发现小模型使用 BN-ReLU, 大模型使用 ReLU-BN 效果较好.</p>
<p>有一个观点是: BN 放在非线性激活函数的前面还是后面取决于你要 normalize 的对象, 更像一个超参数.</p>
<ol>
<li><p>Conv-BN-ReLU: 这是比较常见的使用方式, 这种实现方法有一个直接的好处就是可以在网络做前向 inference 的时候, 将 BN 融合到 Conv 中进行加速. 还有另一种好处是个人理解, 就是 BN 在 ReLU 的激活之前, 可以防止某一层的激活值全部被抑制(及某一层的值均小于0), 从而防止从这一层传播的梯度全是 0, 进而可以防止梯度消失现象. (BN 的减均值处理会使得相当一部分响应值变为正, 进而解决了零梯度问题)</p>
</li>
<li><p>Conv-ReLU-BN: 在具体的实验中, 通常 BN 放在最后面效果最好. 个人见解: BN 实际上就是一种归一化, 而归一化通常是对于输入层使用的, 因此, 把 BN 放在最后, 实际上就是对下一个卷积段的输入进行归一化, 从这个角度看, 将 BN 放在激活层之后, 是比较自然的一种做法. 另外, BN 的原文使用的是 sigmoid 和 tanh 激活函数, 但是对于 ReLU 激活来说, 其曲线图像有较大区别, 而 BN 层会起到一定的平滑隐藏层输入分布的作用, 因此, 对于不同的激活函数, BN 的最佳位置或许有些许不同.</p>
</li>
</ol>
<h4 id="BN-中-batch-的大小对网络性能有什么影响"><a href="#BN-中-batch-的大小对网络性能有什么影响" class="headerlink" title="BN 中 batch 的大小对网络性能有什么影响"></a><a href="../深度学习-Batch-Normalization深入解析/#BN 中 batch 的大小对网络性能有什么影响">BN 中 batch 的大小对网络性能有什么影响</a></h4><p>由于 BN 在计算均值和方差时是在当前的 batch 上进行计算的, 因此, 当 batch 较小时, 求出来的均值和方差就会有较大的随机性, 从而导致效果下降, 具体来说, 当 batch 的大小低于 16 时, 就不建议使用 BN, 当 batch 低于 8 时, 网络的性能就会有非常明显的下降.</p>
<h4 id="BN-中线性偏移的参数个数怎么计算的"><a href="#BN-中线性偏移的参数个数怎么计算的" class="headerlink" title="BN 中线性偏移的参数个数怎么计算的"></a><a href="../深度学习-Batch-Normalization深入解析/#BN 中线性偏移的参数个数怎么计算的">BN 中线性偏移的参数个数怎么计算的</a></h4><p>对于 BN 层来说, 如果它的输入 shape 均为为 $(N, C, H, W)$, 则其输出 shape 也为 $(N, C, H, W)$, <strong>即保持输入输出 shape 不变.</strong> BN 中的线性偏移参数 $\gamma$ 和 $\beta$ 的个数 <strong>与输入 shape 的通道数相同, 均为 $C$</strong>. PyTorch 中 $\gamma$ 和 $\beta$ 参数分别对应着<code>weight</code>和<code>bias</code>, 下面是 BN 的声明.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>, track_running_stats=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p>
<h4 id="BN-中的使用的均值和方差是如何求得的"><a href="#BN-中的使用的均值和方差是如何求得的" class="headerlink" title="BN 中的使用的均值和方差是如何求得的"></a><a href="../深度学习-Batch-Normalization深入解析/#BN 中使用的均值和方差是如何求得的">BN 中的使用的均值和方差是如何求得的</a></h4><p>在训练阶段, 就是利用当前 batch 中的均值和方差进行计算.<br>在测试阶段, 采用的是网络中维护的滑动平均值进行计算的, 滑动平均值的维护方式是用当前的滑动平均值乘以一个 $decay$ 系数, 然后再加上 $(1 - decay)$ 倍的当前 batch 的统计值. $decay$ 决定了数值的更新速度, 通常 $decay$ 会设成一个非常接近于 1 的数, 比如, 0.99 或 0.999.</p>
<script type="math/tex; mode=display">shadow_{variable} = decay \times shadow_{variable} + (1 - decay) \times variable</script><p>在训练的时候, 在最后的几个 epoch, 我们通常会固定住 BN 层的参数多训练一会, 这样可以确保 training 和 inference 之间的一致性</p>
<h4 id="在多卡训练使用-BN-时-需要注意什么问题"><a href="#在多卡训练使用-BN-时-需要注意什么问题" class="headerlink" title="在多卡训练使用 BN 时, 需要注意什么问题"></a><a href="../深度学习-Batch-Normalization深入解析/#在多卡训练使用 BN 时, 需要注意什么问题">在多卡训练使用 BN 时, 需要注意什么问题</a></h4><p>需要注意多卡之间的通信同步问题<br>如果对于 BatchNorm 的实现只考虑了 single GPU, <strong>也就是说 BN 使用的均值和标准差是单个 GPU 计算的, 这相当于缩小了 mini-batch size</strong>. 目前很多主流框架已经支持多卡通信了</p>
<p>为什么不支持多卡通信: 至于为什么这样实现: (1) 因为没有 sync 的需求, 因为对于大多数 vision 问题, 单 GPU 上的 mini-batch 已经够大了, 完全不会影响结果. (2) 影响训练速度, BN layer 通常是在网络结构里面广泛使用的, 这样每次都同步一下 GPUs, 十分影响训练速度.</p>
<p><span id="BN 在 Inference 阶段的加速"></span></p>
<h4 id="BN-在-Inference-阶段的加速"><a href="#BN-在-Inference-阶段的加速" class="headerlink" title="BN 在 Inference 阶段的加速"></a>BN 在 Inference 阶段的加速</h4><p>在 Inference 阶段, 我们已经确定了 BN 层所需的 mean, std, $\gamma$, $\beta$ 等参数, 不用再单独的计算这些参数的值, 又因为 BN 层的运算实际上就相当于两次 Scale (缩放平移) 操作, 因此当 BN 层和 Conv 层相邻时, 我们可以将 BN 层融合到 Conv 层中, 这对于 Conv 层来说只是改变了一些计算规则, 并没有增加卷积层的计算量, 因此可以起到一定的加速作用.</p>
<p>具体在融合时分为两个情况:</p>
<ol>
<li><p>Conv-BN:<br>卷积层操作: $Y = \vec w X + \vec b$<br>BN层操作: $Y’ = \gamma \frac{Y - \hat \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$<br>将上面两个公式融合后可变为: $Y’ = \gamma \frac{Y - \hat \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta = \gamma \frac{\vec w X + \vec b - \hat \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta = (\frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \vec w)\cdot X + \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \cdot \vec b - \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}}\hat \mu + \beta = k\vec w \cdot X + \vec b’$<br>可见, 融合后的 BN 就相当于是给卷积层多乘了一个常量, 同时多加了一个常量.</p>
</li>
<li><p>BN-Conv: BN-Conv 无法进行融合, 始终都需要再单独执行 BN 的 Scale 操作, 然后再执行 Conv 的卷积操作</p>
</li>
<li><p>Conv 和 BN 中间有激活层: 无法融合, 需要单独执行每一个网络层</p>
</li>
</ol>
<h4 id="使用-BN-时-前一层的卷积网络需不需要偏置项-为什么"><a href="#使用-BN-时-前一层的卷积网络需不需要偏置项-为什么" class="headerlink" title="使用 BN 时, 前一层的卷积网络需不需要偏置项, 为什么"></a><a href="../深度学习-Batch-Normalization深入解析/#使用 BN 时, 前一层的卷积网络需不需要偏置项, 为什么">使用 BN 时, 前一层的卷积网络需不需要偏置项, 为什么</a></h4><p>使用 BN 时的前一层卷积网络可以不加偏置项(降低模型参数量)<br>当使用 BN 时, 无论加偏置还是不加偏置, 效果都是一样的, 公式证明如下:</p>
<p>bn 操作的关键一步可以简写为:</p>
<script type="math/tex; mode=display">y_i = \frac{x_i - \bar x}{\sqrt{D(x)}}</script><p>当给卷积层加上偏置后, 就变成了:</p>
<script type="math/tex; mode=display">y_i^b = \frac{x_i^b - \bar x^b}{\sqrt{D(x^b)}}</script><p>其中:</p>
<script type="math/tex; mode=display">x_i^b = x_i + b</script><p>然后, 我们推导此时的 $y_i^b$, 将其化简为:</p>
<script type="math/tex; mode=display">y_i^b = \frac{x_i+b - (\bar x + b)}{\sqrt{D(x^b)}} = \frac{x_i - \bar x}{\sqrt{D(x^b)}}</script><p>而 方差 = 平方的期望 - 期望的平方:</p>
<script type="math/tex; mode=display">D(x^b) = E[(x^b)^2] - [E(x^b)]^2</script><script type="math/tex; mode=display">D(x^b) = \frac{1}{N} \sum_{i=1}^{N} \Big ((x_i^b)^2 - (\bar x + b)^2 \Big )</script><script type="math/tex; mode=display">D(x^b) = \frac{1}{N} \sum_{i=1}^{N} \Big ((x_i + b)^2 - (\bar x + b)^2 \Big)</script><script type="math/tex; mode=display">D(x^b) = \frac{1}{N} \sum_{i=1}^{N} \Big (x_i^2 + 2b\bar x + b^2 - (\bar x + b)^2 \Big)</script><script type="math/tex; mode=display">D(x^b) = \frac{1}{N} \sum_{i=1}^{N} \Big (x_i^2 - (\bar x)^2 \Big)</script><script type="math/tex; mode=display">D(x^b) = D(x)</script><p>所以, 可以得到 $y_i^b = y_i$, 故而, 当使用 BN 时, 前面的卷积层无需使用偏置, 节省参数量. 另外, 在 BN 中的 $\beta$ 参数也可以起到一定的偏置作用.</p>
<p>当卷积层后跟batchnormalization层时为什么不要<br><a href="https://blog.csdn.net/u010698086/article/details/78046671" target="_blank" rel="noopener">https://blog.csdn.net/u010698086/article/details/78046671</a></p>
<p>CC<a href="https://zhuanlan.zhihu.com/p/36222443" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36222443</a></p>
<h3 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h3><h4 id="简述-GN-的原理"><a href="#简述-GN-的原理" class="headerlink" title="简述 GN 的原理"></a><a href="../计算机视觉-GroupNormalization-ECCV2018/#简述 GN 的原理">简述 GN 的原理</a></h4><p>BN 在很多任务上都取得了很好的效果(BN通常会在每一层都执行), 但是, BN 依赖于 batch 的平均值和方差, 这使得 batch size 的大小对BN的效果有较大的影响, 同时, 在测试阶段, 单个的图片无法提供良好的均值和方差进行归一化, 所以只能用整个数据集的均值和方差来代替, 通常会使用滑动平均来维护这两个变量, 也就是说, 在使用 BN 时, 如果数据集改变了, 则均值和方差就会有较大改变, 这就造成了训练阶段和测试阶段的不一致性, 由此也会带来一些问题. 因此, GN 为了解决 BN 对 batch 大小的依赖问题, 转而从另一个角度来进行归一化, GN 更像是介于 LN 和 IN 中间的一种归一化方法, 它会将通道分成不同的组, 同时在固定下标 N 的同时, 求取当前组内的均值和方差来进行归一化. 通过实验分析和论证, GN 可以取得不错的效果, 避免了对 batch 的依赖问题.</p>
<h4 id="为什么-GN-效果好"><a href="#为什么-GN-效果好" class="headerlink" title="为什么 GN 效果好"></a><a href="../计算机视觉-GroupNormalization-ECCV2018/#为什么 GN 效果好">为什么 GN 效果好</a></h4><p>GN 是从 LN 和 IN 中变化来的, 组的划分实际上可以看做是一种对数据分布的假设, 以 LN 为例, 它实际上假设了每张图片所有通道的特征都是同分布的, 而 GN 则是假设每个组的分布不同, 条件没有那么苛刻, 因此 GN 的表现力和包容性会更强, 而 IN 只依赖与独立的某一维, 没有探究不同通道之间特征的关联性. 相对于 BN 来说, 当 batch 的大小足够时, BN 的性能表现依然很不错, 因此, GN 充当的角色更像是当 batch 较小, 无法使用 BN 时的一种替代措施.</p>
<p>传统角度来讲，在深度学习没有火起来之前，提取特征通常是使用SIFT，HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。而更高维的特征比如VLAD和Fisher Vectors(FV)也可以看作是group-wise feature，此处的group可以被认为是每个聚类（cluster）下的子向量sub-vector。</p>
<p>从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。<br>导致分组（group）的因素有很多，比如频率、形状、亮度和纹理等，HOG特征根据orientation分组，而对神经网络来讲，其提取特征的机制更加复杂，也更加难以描述，变得不那么直观。另在神经科学领域，一种被广泛接受的计算模型是对cell的响应做归一化，此现象存在于浅层视觉皮层和整个视觉系统。<br>作者基于此，尝试使用了组归一化（Group Normalization）的方式进行模型训练, 结果意外发现训练效果非常不错, 显著优于BN、LN、IN等。</p>
<h4 id="简述-BN-LN-IN-GN-的区别"><a href="#简述-BN-LN-IN-GN-的区别" class="headerlink" title="简述 BN, LN, IN, GN 的区别"></a><a href="../计算机视觉-GroupNormalization-ECCV2018/#简述 BN, LN, IN, GN 的区别">简述 BN, LN, IN, GN 的区别</a></h4><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/norm.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnorm.jpg"></div></p>
<p>这些 Norm 方法的不同之处就在于计算均值和方差时使用的像素集合不同(如图2所示), 假设输入的 tensor 的 shape 为 $(N, C, H, W)$:</p>
<ul>
<li>BN 是固定 $C$ 不变, 求固定 $C$ 时所有 $(N,H,W)$ 像素点的均值和方差, 这个均值和方差会用来归一化所有处于当前通道 $C$ 上的像素.</li>
<li>LN 是固定 $N$ 不变, 求固定 $N$ 时所有 $(C,H,W)$ 像素点的均值和方差, 这个均值和方差会用来归一化所有处于当前 $N$ 上的像素. 可以看出, 这里 LN 在求取均值和方差时, 由于固定了 $N$, 所以与 batch 的大小无关.</li>
<li>IN 是同时固定 $N$ 和 $C$ 不变, 求固定 $N$ 和 $C$ 时所示 $(H,W)$ 像素点的均值和方差.</li>
<li>GN 是介于 LN 和 IN 中的一种 Norm 方法, 它首先也是固定 $N$ 不变, 然后会将 $C$ 分成若干个 Group, 然后分别求取每个 Group 的均值和方差, 并对 Group 中的像素进行归一化</li>
</ul>
<p><strong>注意, 无论是哪种 Norm 方法, 它们使用的线性偏移的参数个数都等于通道 $C$ 的大小.</strong></p>
<h4 id="GN-中线性偏移的参数个数怎么计算的"><a href="#GN-中线性偏移的参数个数怎么计算的" class="headerlink" title="GN 中线性偏移的参数个数怎么计算的"></a><a href="../计算机视觉-GroupNormalization-ECCV2018/#GN 中线性偏移的参数个数怎么计算的">GN 中线性偏移的参数个数怎么计算的</a></h4><p>对于 GN 层来说, 如果它的输入 shape 均为为 $(N, C, H, W)$, 则其输出 shape 也为 $(N, C, H, W)$, <strong>即保持输入输出 shape 不变.</strong> GN 中的线性偏移参数 $\gamma$ 和 $beta$ 的个数 <strong>与输入 shape 的通道数相同, 均为 $C$</strong>. GN 除了需要确定输入层的通道数以外, 还需要确定 Goup 的数量. 下面给 PyTorch 中 GN 的声明.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.GroupNorm(num_groups, num_channels, eps=<span class="number">1e-05</span>, affine=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>注意, GN, 在进行归一化时, 使用的mean和var是按照组进行划分的, 但是, 在进行偏移时的gamma和beta参数, 仍然是与 channel 数量保持一致的</strong></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/GroupNorm_code.png?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2FGroupNorm_code.png"></div></p>
<h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><h3 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h3><h3 id="Switchable-Normalization"><a href="#Switchable-Normalization" class="headerlink" title="Switchable Normalization"></a>Switchable Normalization</h3><p><span id="感受野"></span></p>
<h2 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h2><h3 id="感受野的计算公式"><a href="#感受野的计算公式" class="headerlink" title="感受野的计算公式"></a>感受野的计算公式</h3><script type="math/tex; mode=display">RF_{l+1} = RF_l + (\text{kernel_size}_l - 1) \times \text{feature_stride}_l</script><p>其中: $RF$ 表示特征感受野的大小, $l$ 表示当前层级, $\text{feature_stride}_l$ 表示当前特征图谱相对于原图的总 stride</p>
<p>如果有 dilated conv 的话, 计算公式为:</p>
<script type="math/tex; mode=display">RF_{l+1} = RF_l + (\text{kernel_size}_l - 1) \times \text{feature_stride}_l \times (dilation_{l+1} - 1)</script><h3 id="理论感受野和有效感受野的区别"><a href="#理论感受野和有效感受野的区别" class="headerlink" title="理论感受野和有效感受野的区别"></a>理论感受野和有效感受野的区别</h3><p>特征的有效感受野（实际起作用的感受野）实际上是远小于理论感受野的.</p>
<p>以一个两层 kernel_size = 3, stride = 1 的网络为例，该网络的理论感受野为 5，计算流程可以参见下图。其中 $x$ 为输入，$w$ 为卷积权重，$o$为经过卷积后的输出特征。</p>
<p>很容易可以发现，$x_{1,1}$ 只能影响第一层 feature map 中的 $o_{1, 1}^1$；而 $x_{3,3}$ 会影响第一层 feature map 中的所有特征，即 $o^1_{1,1}, o^1_{1,2}, o^1_{1,3}, o^1_{2,1}, o^1_{2,2}, o^1_{2,3}, o^1_{3,1}, o^1_{3,2}, o^1_{3,3}$。</p>
<p>第一层的输出全部会影响第二层的 $o^2_{1,1}$</p>
<p>于是 $x_{1,1}$ 只能通过 $o^1_{1,1}$ 来影响 $o^2_{1,1}$；而 $x_{3,3}$ 能通过 $o^1_{1,1}, o^1_{1,2}, o^1_{1,3}, o^1_{2,1}, o^1_{2,2}, o^1_{2,3}, o^1_{3,1}, o^1_{3,2}, o^1_{3,3}$ 来影响 $o^2_{1,1}$。显而易见，虽然 $x_{1,1}$ 和 $x_{3,3}$ 都位于第二层特征感受野内，但是二者对最后的特征的影响却大不相同，输入中越靠感受野中间的元素对特征的贡献越大。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/rf1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Frf1.jpg"></div></p>
<h3 id="目标检测中的-anchor-的设置和感受野的大小之间有什么关系"><a href="#目标检测中的-anchor-的设置和感受野的大小之间有什么关系" class="headerlink" title="目标检测中的 anchor 的设置和感受野的大小之间有什么关系?"></a>目标检测中的 anchor 的设置和感受野的大小之间有什么关系?</h3><p>现在流行的目标检测网络大部分都是基于anchor的，比如SSD系列，v2以后的yolo，还有faster rcnn系列。</p>
<p>基于anchor的目标检测网络会预设一组大小不同的anchor，比如32x32、64x64、128x128、256x256，这么多anchor，我们应该放置在哪几层比较合适呢？这个时候感受野的大小是一个重要的考虑因素。</p>
<p>放置anchor层的特征感受野应该跟anchor大小相匹配，感受野比anchor大太多不好，小太多也不好。如果感受野比anchor小很多，就好比只给你一只脚，让你说出这是什么鸟一样。如果感受野比anchor大很多，则好比给你一张世界地图，让你指出故宫在哪儿一样。</p>
<p><span id="全连接层"></span></p>
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><h3 id="全连接层的作用是什么"><a href="#全连接层的作用是什么" class="headerlink" title="全连接层的作用是什么"></a>全连接层的作用是什么</h3><p><a href="https://www.zhihu.com/question/41037974" target="_blank" rel="noopener">https://www.zhihu.com/question/41037974</a></p>
<ol>
<li>最直观的作用, 起到 “分类器” 的作用. 通常在网络的最后, 会利用全连接层将网络学习到的 “分布式特征表示” 映射到 <strong>样本标记空间</strong>. 在实际使用中, 全连接层也可以利用卷积操作来实现, 如果前一层是全连接层, 那么可以把前一层的全连接层当做是 $h=1, w=1, c = len(FC)$ 的特征图谱, 然后利用核大小为 $1\times 1$, 通道数为前一层神经元个数的卷积层进行计算, 卷积核的个数根据当前层的神经元个数决定. 如果前一层是卷积层, 则只需核的大小设置为前一层的特征图谱的大小, 进行全局卷积即可, 核的通道数由前一层的卷积结果决定, 核的个数由当前层的神经元个数决定.</li>
<li>特征融合, 全连接的任意一个神经元, 都能够 “看到” 前一层网络层输出的所有特征信息(FC 认为下一层的输出与上一层所有输入都有关, 实际上这样很容易 overfitting), 全连接层会根据这些信息, 决定它当前某一个神经元的输出. 这样也就弥补了卷积层只能 “看到” 局部信息的缺点.</li>
<li>不太直观的作用, 目前由于 FC 存在大量的参数冗余, 所有大多数时候我们会用全局平均池化来代替 FC. 但是我之前有看过一篇论文说 FC 的参数冗余也并不是一无是处, 它可以在一定程度上保证模型的迁移能力, 当原模型和目标数据集相差较大的时候, 使用 FC 的模型比不使用 FC 的模型的迁移效果好. 原因可能是冗余的参数对于特征的表示可能更加丰富.</li>
</ol>
<h3 id="将全连接层转换成卷积层由什么好处"><a href="#将全连接层转换成卷积层由什么好处" class="headerlink" title="将全连接层转换成卷积层由什么好处"></a>将全连接层转换成卷积层由什么好处</h3><p><a href="https://www.cnblogs.com/liuzhan709/p/9356960.html" target="_blank" rel="noopener">https://www.cnblogs.com/liuzhan709/p/9356960.html</a></p>
<ol>
<li>可以接受更多尺寸的图片输入, 我们只需要固定网络的通道数符合要求, 然后利用卷积层即可完全最终的分类.</li>
<li>高效, 当我们需要在一张图片上以一定大小的滑动窗口进行多次计算时, 由于这些窗口之间有大量的重合区域, 因此直接使用全连接层会造成会多的计算浪费, 而卷积操作在大多数框架中都得到了性能优化, 十分擅长处理这种操作, 因此在时间上更占优势.</li>
</ol>
<h3 id="推导两层全连接网络的反向传播公式"><a href="#推导两层全连接网络的反向传播公式" class="headerlink" title="推导两层全连接网络的反向传播公式"></a>推导两层全连接网络的反向传播公式</h3><p><a href="https://zhuanlan.zhihu.com/p/39195266" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/39195266</a></p>
<p>全连接层失宠原因之一: 目前大多数的任务，如目标检测，或是分割，并不要求提取全图特征，只需要提取能够覆盖目标物体的大小的感受野内特征即可。尤其是小物体检测问题，感受野很小即可，如果还去接全连接提取全图特征，我们待检测的目标会被淹没在和其它背景的平均特征之中变得不可识别。</p>
<p><span id="卷积层"></span></p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic</a></p>
<h3 id="卷积层的计算公式"><a href="#卷积层的计算公式" class="headerlink" title="卷积层的计算公式"></a>卷积层的计算公式</h3><p>不带 Dilation 的计算:</p>
<script type="math/tex; mode=display">H_{out} = \Bigg \lfloor \frac{H_{in} + 2\times padding[0] - kernel_size[0]}{stride[0]} + 1 \Bigg \rfloor</script><script type="math/tex; mode=display">W_{out} = \Bigg \lfloor \frac{W_{in} + 2\times padding[1] - kernel_size[1]}{stride[1]} + 1 \Bigg \rfloor</script><p>带有 Dilation 的计算:</p>
<script type="math/tex; mode=display">H_{out} = \Bigg \lfloor \frac{H_{in} + 2\times padding[0] - dilation[0] \times (kernel_size[0] - 1) - 1}{stride[0]} + 1 \Bigg \rfloor</script><script type="math/tex; mode=display">W_{out} = \Bigg \lfloor \frac{W_{in} + 2\times padding[1] - dilation[1] \times (kernel_size[1] - 1) - 1}{stride[1]} + 1 \Bigg \rfloor</script><p>卷积层的作用?</p>
<h3 id="简述-1x1-卷积层的作用"><a href="#简述-1x1-卷积层的作用" class="headerlink" title="简述 1x1 卷积层的作用"></a><a href="../深度学习-各种网络层/#简述 1x1 卷积层的作用">简述 1x1 卷积层的作用</a></h3><ul>
<li>改变特征图谱的深度: 通常用作降维</li>
<li>xception 和 mobilenet 系列将其用作解耦: 将 cross-channel correlation 和 spatial correlation 的学习进行解耦. 大大降低计算量</li>
<li>实现了跨通道的信息组合: 使用1<em>1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3</em>3，64channels的卷积核前面添加一个1<em>1，28channels的卷积核，就变成了3</em>3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互。因为1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep，增加非线性特性。</li>
</ul>
<h3 id="卷积操作的本质特性包括稀疏交互和参数共享-具体解释这两种特性及其作用"><a href="#卷积操作的本质特性包括稀疏交互和参数共享-具体解释这两种特性及其作用" class="headerlink" title="卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用"></a><a href="../深度学习-各种网络层/#卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用">卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用</a></h3><p><strong>稀疏连接(稀疏交互):</strong></p>
<ul>
<li>定义：传统的全连接网络，每一个输出都与每一个输入单元产生交互，卷积使用了稀疏交互：每个输出神经元只与前一层的特定局部区域内的神经元产生交互</li>
<li>好处：<ul>
<li>参数更少，降低模型的复杂度，防止过拟合</li>
<li>提高模型的统计效率，原本一幅图像只能提供少量特征，现在每个像素区域都可以提供一部分特征</li>
</ul>
</li>
</ul>
<p><strong>参数共享:</strong><br>定义：在模型的不同模块中（也可以说是多个函数中）使用相同的参数。也可以叫作一个网络含有绑定的权重。<br>传统的全连接网络中，在计算一层的输出时，权重矩阵的每一个元素只使用一次，乘以输入的一个元素之后，再也不会用到了。而在卷积神经网络中，卷积核的每个元素将作用于每一次局部输入的特定位置上。<br>只需要学习一个参数集合，而不是对于每一个位置都学习一个单独的参数集合。</p>
<h3 id="卷积层实现如何优化"><a href="#卷积层实现如何优化" class="headerlink" title="卷积层实现如何优化"></a><a href="../深度学习-各种网络层/#卷积层底层是如何实现的">卷积层实现如何优化</a></h3><p><a href="https://jackwish.net/convolution-neural-networks-optimization.html" target="_blank" rel="noopener">https://jackwish.net/convolution-neural-networks-optimization.html</a></p>
<h4 id="im2col-优化"><a href="#im2col-优化" class="headerlink" title="im2col 优化"></a>im2col 优化</h4><p>给定一个卷积层 C in x C out x H k x W k，  以及输入 feature map C in x H x W，<br>im2col<br>Mat A: (H x W) x (C in x  H k x W k ）<br>Mat B: (C in x  H k x W k ）x (C out )</p>
<p><strong>一种比较方便也是比较偷懒的卷积层实现方法都是将图片或者特征图谱利用 im2col 方法展开成矩阵, 将卷积操作变成通用矩阵乘法(GEMM), 然后利用 cuBLAS 或者 OpenBLAS 的库函数进行计算(这些库里面的矩阵乘法是经过高度优化的, 所以速度也不慢).</strong></p>
<p>具体来说, 对于任意的输入图谱, 根据卷积核的大小在特征图谱上获得一个 patch, 将这个 patch 里面的元素拿出来变成矩阵的一列, 按照卷积操作, 取出所有的 path 组成一个新的矩阵. 然后将卷积核展开成一个矩阵, 矩阵的每一行都是卷积核中的元素, 总共的行数和输出图谱的通道数相关. 这样, 卷积的计算操作就变成了普通的矩阵乘法.</p>
<p>这里可以看出, 对于卷积核大于 1 的卷积层来说, 我们需要按照卷积核的大小对图谱重新进行排列. <strong>但是, 当卷积核大小为 1 时, 我们就无需排列, 直接将其展开即可.</strong> 这也解释了 MobileNet 中提到的 $1\times 1$ 卷积在实现上执行速度很快的原因.</p>
<p><strong>im2col+GEMM 的卷积实现方法有一个很明显的问题就是, 会存储大量的冗余元素, 使得内存消耗比较大.</strong></p>
<h4 id="空间组合优化算法"><a href="#空间组合优化算法" class="headerlink" title="空间组合优化算法"></a>空间组合优化算法</h4><p>空间组合主要是采用了分治的思想, 它基于空间特性将卷积计算划分为若干份, 分别处理, 虽然划分后的计算总量保持不变, 但是计算小矩阵时的访问内存的局部性更好, 可以借由计算机存储层次结果获得性能提升.<br>对于不同规模的卷积, 寻找合适的划分方法不是一件容易的事情. 该划分也可以通过 AutoTVM 自动化来完成.</p>
<h4 id="Winograd-优化算法"><a href="#Winograd-优化算法" class="headerlink" title="Winograd 优化算法"></a>Winograd 优化算法</h4><h4 id="量化神经网络的优化方法"><a href="#量化神经网络的优化方法" class="headerlink" title="量化神经网络的优化方法"></a>量化神经网络的优化方法</h4><p>概念, 简介缓冲区, 向量化卷积计算, 卷积计算工作流</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>其他计算卷积的方法:</p>
<ul>
<li>FFT: 大卷积核时使用, 时域卷积等于频域相乘, 因此可以将问题转化成简单的乘法问题. cuFFT</li>
<li>Winograd: 据说在 GPU 上效率更高, 貌似是针对 $2\times2$ 和 $3\times 3$ 的卷积核专门使用的?</li>
<li>NNPACK: FFT 和 Winograd 方法的结合</li>
<li>MEC(17年): 一种内存利用率高且速度较快的卷积计算方法, <a href="http://cn.arxiv.org/pdf/1706.06873v1" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1706.06873v1</a>. 主要改进了 im2col+GEMM 的策略, 目的主要是减少内存消耗的同时顺便提升速度. 由于同样可以利用现有的矩阵运算库, 因此算法的实现难度并不大.</li>
</ul>
<p>CNN 基础之卷积及其矩阵加速  <a href="http://shuokay.com/2016/06/08/convolution" target="_blank" rel="noopener">http://shuokay.com/2016/06/08/convolution</a></p>
<p>Winograd 方法快速计算卷积 <a href="http://shuokay.com/2018/02/21/winograd/" target="_blank" rel="noopener">http://shuokay.com/2018/02/21/winograd/</a></p>
<p><a href="https://blog.csdn.net/antkillerfarm/article/details/78829889" target="_blank" rel="noopener">https://blog.csdn.net/antkillerfarm/article/details/78829889</a></p>
<p><a href="https://blog.csdn.net/xiaoxiaowenqiang/article/details/82050354" target="_blank" rel="noopener">https://blog.csdn.net/xiaoxiaowenqiang/article/details/82050354</a></p>
<p>BLAS 接受, 矩阵乘法优化 <a href="https://www.leiphone.com/news/201704/Puevv3ZWxn0heoEv.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201704/Puevv3ZWxn0heoEv.html</a></p>
<p>im2col 讲解: <a href="https://blog.csdn.net/Mrhiuser/article/details/52672824" target="_blank" rel="noopener">https://blog.csdn.net/Mrhiuser/article/details/52672824</a></p>
<h3 id="实现矩阵乘法并简述其优化方法"><a href="#实现矩阵乘法并简述其优化方法" class="headerlink" title="实现矩阵乘法并简述其优化方法"></a><a href="../深度学习-各种网络层/#简述矩阵乘法的优化方法">实现矩阵乘法并简述其优化方法</a></h3><p><a href="https://jackwish.net/gemm-optimization.html" target="_blank" rel="noopener">https://jackwish.net/gemm-optimization.html</a></p>
<p>矩阵乘法代码实现(三重循环):<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrixMul</span><span class="params">(A, B)</span>:</span></span><br><span class="line">    m = len(A)</span><br><span class="line">    p = len(A[<span class="number">0</span>])</span><br><span class="line">    pp = len(B)</span><br><span class="line">    n = len(B[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> p != pp:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"error input"</span>)</span><br><span class="line">    C = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> range(m)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(p):</span><br><span class="line">                C[i][j] += A[i][p] * B[p][j]</span><br><span class="line">    <span class="keyword">return</span> C</span><br></pre></td></tr></table></figure></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm1.jpg"></div></p>
<p>对矩阵乘法进行优化的方法可分为两类:</p>
<ul>
<li>基于算法分析的方法: 根据矩阵乘法的计算特性, 从数学角度优化, 典型的算法包括 Strassen 算法和 Coppersmith-Winograd 算法</li>
<li>基于计算机系统优化的方法: 根据计算机存储系统的层次结构特性, 选择性的调整计算顺序, 主要有循环拆分向量化, 内存重排等.</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm2.jpg"></div></p>
<h4 id="Strassen-算法"><a href="#Strassen-算法" class="headerlink" title="Strassen 算法"></a>Strassen 算法</h4><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm_strassen.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm_strassen.jpg"></div></p>
<p>完全应用 Strassen 算法的一个局限是其要求矩阵乘的规模为 $2^n$，这在现实情况中不容易满足。一种解决方法是将规模分解为 $2^n X$ 其中 $X$ 无法被 2 整除，那么可以应用 Strassen 算法不断递归拆分计算直到小矩阵规模为 $X$。此时可以用朴素算法计算小矩阵；或者将 $X$ 补零为 $2^n$ 再继续应用 Strassen 算法（亦可直接对大矩阵补零）。最终的性能取决于实现方法和运行的硬件平台。</p>
<p>有时在实际使用 Strassen 算法时，耗时不但没有减少，反而剧烈增多，在 $n=512$ 时计算时间就无法忍受，效果没有朴素矩阵算法好。网上查阅资料，现罗列如下：</p>
<ol>
<li>采用Strassen算法作递归运算，需要创建大量的动态二维数组，其中分配堆内存空间将占用大量计算时间，从而掩盖了Strassen算法的优势</li>
<li>于是对Strassen算法做出改进，设定一个界限。当n&lt;界限时，使用普通法计算矩阵，而不继续分治递归。需要合理设置界限，不同环境（硬件配置）下界限不同</li>
<li>矩阵乘法一般意义上还是选择的是朴素的方法，只有当矩阵变稠密，而且矩阵的阶数很大时，才会考虑使用Strassen算法。</li>
</ol>
<h4 id="Winograd-算法"><a href="#Winograd-算法" class="headerlink" title="Winograd 算法"></a>Winograd 算法</h4><p>Coppersmith–Winograd 算法的思想和 Strassen 算法类似。其证明过程比较复杂，使用的定理太多，这里就不再介绍（实际上是没看懂…）</p>
<p>Winograd 将矩阵乘法的复杂度降到了 $O(n^{2.376})$, 从上图2可以看出, 到目前为止, Winograd 仍然是最优的一类优化算法, 也是在各个深度学习框架中广泛使用的一类算法.</p>
<h4 id="通过划分降低访存次数"><a href="#通过划分降低访存次数" class="headerlink" title="通过划分降低访存次数"></a>通过划分降低访存次数</h4><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm3.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm3.jpg"></div></p>
<p>首先给出朴素矩阵乘法的计算过程(半伪码):<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> range(M):</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">        c[m][n] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(K):</span><br><span class="line">            C[m][n] += A[m][k] * B[k][n]</span><br></pre></td></tr></table></figure></p>
<p>从上面的代码我们可以看出:</p>
<ul>
<li>总共计算操作数为 $2_{mla} MNK$, 其中, $M, N, K$ 分别指代三重循环的执行次数, $2_{mla}$ 指代最内层循环执行的 Multiply-Adds 的次数为 2(一次乘法, 一次加法)</li>
<li>内存访问操作次数为 $4MNK$, 其中 $4 = 2 (C 读取, C 存储) + 1 (A 读取) + 1 (B 读取)$</li>
</ul>
<p>我们以图形化的方式来介绍如何通过划分矩阵来实现降低访存次数的优化.</p>
<p>首先我们对 $N$ 维度进行划分, 也就是说, 我们将矩阵 C 上的输出划分成 $1 \times 4$ 的小块, 这样, 在计算该输出时, 就需要使用矩阵 A 的 <strong>一行</strong>, 和矩阵 B 的 <strong>四列</strong>, 恰如下图所示</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm4.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm4.jpg"></div></p>
<p>要想实现上面的操作, 我们就需要将 $N$ 上的循环分出一部分到最内侧去计算, 伪代码如下所示:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> range(M):</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>, N, <span class="number">4</span>): <span class="comment"># 注意这里的步长变成了 4</span></span><br><span class="line">        c[m][n+<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        c[m][n+<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">        c[m][n+<span class="number">2</span>] = <span class="number">0</span></span><br><span class="line">        c[m][n+<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(K):</span><br><span class="line">            C[m][n+<span class="number">0</span>] += A[m][k] * B[k][n+<span class="number">0</span>]</span><br><span class="line">            C[m][n+<span class="number">1</span>] += A[m][k] * B[k][n+<span class="number">1</span>]</span><br><span class="line">            C[m][n+<span class="number">2</span>] += A[m][k] * B[k][n+<span class="number">2</span>]</span><br><span class="line">            C[m][n+<span class="number">3</span>] += A[m][k] * B[k][n+<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看出, <strong>展开后的计算操作总数仍然是 $2MNK$, 这一点在将降低访存优化方法中一直不变.</strong><br>但是我们通过简单的观察即可发现, 上述伪代码最内层的计算使用的矩阵 A 元素是一直的, <strong>因此我们可以将 A[m][k] 读取到寄存器中(代码中未体现), 从而实现 4 次的数据复用.</strong> 进行这样的优化后, <strong>内存的访问操作数量就从 $4MNK$ 变成了 $(2+1+\frac{1}{4}) MNK$,</strong> 其中, $\frac{1}{4}$ 是对 A 优化的结果.</p>
<p>类似的, 我们可以继续拆分 $M$ 维度, 从而在最内层循环中进行 $4\times 4$ 大小的块计算, 如下图所示:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm5.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm5.jpg"></div></p>
<p>同样的, 我们给出上图的计算伪码, 注意, 这里的<code>[0~3]</code>是对<code>[0], [1], [2], [3]</code>采取的缩写:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>, M, <span class="number">4</span>): <span class="comment"># 注意, 现在 m 的步长也变成了 4</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>, N, <span class="number">4</span>): <span class="comment"># 注意这里的步长变成了 4</span></span><br><span class="line">        c[m+<span class="number">0</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        c[m+<span class="number">1</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        c[m+<span class="number">2</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        c[m+<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(K):</span><br><span class="line">            C[m+<span class="number">0</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">0</span>][k] * B[k][n+<span class="number">0</span>~<span class="number">3</span>]</span><br><span class="line">            C[m+<span class="number">1</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">1</span>][k] * B[k][n+<span class="number">0</span>~<span class="number">3</span>]</span><br><span class="line">            C[m+<span class="number">2</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">2</span>][k] * B[k][n+<span class="number">0</span>~<span class="number">3</span>]</span><br><span class="line">            C[m+<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">3</span>][k] * B[k][n+<span class="number">0</span>~<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到, 我们同样可以将 B 中的元素复用四次, 这样, <strong>我们就通过 $4\times 4$ 的划分, 将输入数据的访存次数缩减到了 $2MNK + \frac{1}{4}MNK + \frac{1}{4}MNK = (2+\frac{1}{2})MNK$. 这相对于最开始的 $4MNK$ 已经得到了 1.6 倍的改进.</strong></p>
<p>接下来, 我们还可以继续对 $K$ 维度进行划分, 也就是令 k 的步长也变成 4, 如下图6所示</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm6.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm6.jpg"></div></p>
<p>对 $K$ 进行划分后, 我们每次最内层循环会计算出 $\frac{4}{K}$ 的部分和, 而不是每次都只计算出 $\frac{1}{K}$ 的和, 在对 $K$ 展开时, 我们可以将部分和累加在寄存器中, 在最内层循环结束时才一次性写到 C 的内存中, 这样就可以在 $K$ 的维度上降低访存次数, 具体的伪码如下所示:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>, M, <span class="number">4</span>): <span class="comment"># 注意, 现在 m 的步长也变成了 4</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>, N, <span class="number">4</span>): <span class="comment"># 注意这里的步长变成了 4</span></span><br><span class="line">        c[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        c[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        c[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        c[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, K, <span class="number">4</span>): <span class="comment"># 可以看到, 现在 k 的步长也变成了 4</span></span><br><span class="line">            C[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">0</span>~<span class="number">3</span>][k+<span class="number">0</span>] * B[k+<span class="number">0</span>][n+<span class="number">0</span>~<span class="number">3</span>]</span><br><span class="line">            C[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">0</span>~<span class="number">3</span>][k+<span class="number">1</span>] * B[k+<span class="number">1</span>][n+<span class="number">0</span>~<span class="number">3</span>]</span><br><span class="line">            C[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">0</span>~<span class="number">3</span>][k+<span class="number">2</span>] * B[k+<span class="number">2</span>][n+<span class="number">0</span>~<span class="number">3</span>]</span><br><span class="line">            C[m+<span class="number">0</span>~<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>] += A[m+<span class="number">0</span>~<span class="number">3</span>][k+<span class="number">3</span>] * B[k+<span class="number">3</span>][n+<span class="number">0</span>~<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>注意上面代码中总的 Multiply-Adds 次数依然是 $2MNK$, 只不过有一部分计算放在了最内层循环中而已. 而我们通过对 $M, N, K$ 三个维度的划分, 成功的将内存访问次数降低到了 $2\times \frac{1}{4}MNK + \frac{1}{4}MNK + \frac{1}{4}MNK = MNK$, 相对于原始实现提升了 4 倍.</p>
<p>上述的优化方法降低了内存访问次数, 但是一条计算指令只能完成一次乘加操作, 效率较低, 因此可以通过向量化优化指令条数, 如下图所示…(这一部分没太看懂, 更详细的讲解请看原文 <a href="https://jackwish.net/gemm-optimization.html" target="_blank" rel="noopener">https://jackwish.net/gemm-optimization.html</a>)</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm7.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm7.jpg"></div></p>
<h4 id="优化内存布局进一步降低访存"><a href="#优化内存布局进一步降低访存" class="headerlink" title="优化内存布局进一步降低访存"></a>优化内存布局进一步降低访存</h4><p>(这一部分没太看懂, 更详细的讲解请看原文 <a href="https://jackwish.net/gemm-optimization.html" target="_blank" rel="noopener">https://jackwish.net/gemm-optimization.html</a>)<br>上一小节列出的是在输入输出原有内存布局上所做的优化。在最后向量化时，每次内存访问都是四个元素。当这些元素为单精度浮点数时，内存大小为 16 字节，这远小于现代处理器高速缓存行大小（Cache line size）——后者一般为 64 字节。在这种情况下，内存布局对计算性能的影响开始显现。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm8.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm8.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm9.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm9.jpg"></div></p>
<p>根据矩阵的尺寸和稀疏程度的不同, 也有不同的优化方法, 对于特定尺寸的卷积核来说, 也存在有特定的优化方法.</p>
<h4 id="神经网络量化中的矩阵乘法优化"><a href="#神经网络量化中的矩阵乘法优化" class="headerlink" title="神经网络量化中的矩阵乘法优化"></a>神经网络量化中的矩阵乘法优化</h4><p>(这一部分没太看懂, 更详细的讲解请看原文 <a href="https://jackwish.net/gemm-optimization.html" target="_blank" rel="noopener">https://jackwish.net/gemm-optimization.html</a>)</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm10.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm10.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm11.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm11.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gemm12.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgemm12.jpg"></div></p>
<h3 id="卷积核的大小如何确定"><a href="#卷积核的大小如何确定" class="headerlink" title="卷积核的大小如何确定"></a>卷积核的大小如何确定</h3><p>卷积核的大小决定了该卷积核在上一层特征图谱上的感受野大小，在确定卷积核的大小时有以下原则（并非通用性原则，实际设计时需要结合具体情况决定）：在网络的起始层，选用较大的卷积核（7×7），这样可以使得卷积核“看到”更多的原图特征；在网络中中间层，可以用两个3×3大小的卷积层来代替一个5×5大小的卷积层，这样做可以在保持感受野大小不变的情况下降低参数个数，减少模型复杂度；通常使用奇数大小的卷积核，原因有二，一是可以更加方便的进行padding，二是奇数核相对于偶数核，具有天然的中心点，并且对边沿、对线条更加敏感，可以更有效的提取边沿信息</p>
<p><span id="池化层"></span></p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><h3 id="池化层的作用是什么"><a href="#池化层的作用是什么" class="headerlink" title="池化层的作用是什么"></a>池化层的作用是什么</h3><ol>
<li><strong>降低优化难度和参数个数:</strong> 池化层可以降低特征图谱的维度，从而降低网络整体的复杂度，不仅可以加速计算，也能起到 <strong>一定的防止过拟合的作用(只保留最大值, 相于不保留非关键信息).</strong></li>
<li><strong>增大感受野:</strong> 当没有池化层时，一个3×3，步长为1的卷积，它输出的一个像素的感受野就是3×3的区域，再加一个stride=1的3×3卷积，则感受野为5×5。当使用pooling后，很明显感受野迅速增大，感受野的增加对于模型的能力的提升是必要的, 当然还有其他更有效的提升感受野的方法, 只不过池化对于感受野的提升也有一定作用.</li>
<li><strong>增加网络平移不变性:</strong> 池化层只会关注核内的值，而不会关注该值的位置，因此，当目标位置发生移动时，池化层也可以得到相同的结果，所以池化层在一定程度上可以增加CNN网络的平移不变性. 同时一定程度上也具有旋转不变性(旋转可以看做是特殊的平移)和尺度不变性(与具体的缩放插值方式有关, 但通常也能保持池化输出值不变)</li>
</ol>
<h3 id="池化层反向传播的梯度时如何求的"><a href="#池化层反向传播的梯度时如何求的" class="headerlink" title="池化层反向传播的梯度时如何求的"></a>池化层反向传播的梯度时如何求的</h3><p>无论是最大池化还是均值池化, 都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算。</p>
<ul>
<li><p>对于 mean pooling，backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了。如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forward: [1 3;2 2] -&gt; [2]</span><br><span class="line">backward: [2] -&gt; [0.5 0.5;0.5 0.5]</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于 max pooling，backward的时候把当前的值放到之前那个最大的位置，其他的三个位置都设置成0。如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forward: [1 3;2 2] -&gt; 3</span><br><span class="line">backward: [3] -&gt; [0 3;0 0]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/pool_1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fpool_1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/pool_2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fpool_2.jpg"></div></p>
<h3 id="最大池化和平均池化有什么异同-分别适用于什么场景"><a href="#最大池化和平均池化有什么异同-分别适用于什么场景" class="headerlink" title="最大池化和平均池化有什么异同, 分别适用于什么场景"></a>最大池化和平均池化有什么异同, 分别适用于什么场景</h3><p>最大池化是取核中的最大值, 平均池化是取核中的最小值.</p>
<p>最大池化多用于提取特征, 因为我们需要提取出物体中最显著的若干特征来帮助我们感知和识别物体, 而最大池化的计算规则将是保留特征相应最大的值, 就像是人眼一样, 我们往往只通过一些很明显的特征就可以判断出一个物体的种类, 最大池化多多少少也有这一层含义. 举一个例子, 比如两处不同的位置进行 mean pooling, 一处的最大值是100, 然后经过mean pooling 之后, 它的输出值变成了 20, 而另一处的最大值是50, 然后经过mean pooling 之后它的输出值也是20, 这样, 对于不同的特征, 我们却得到了重复的结果, 这实际上是一种信息冗余, 也可以认为是一种特征丢失. 在使用中, 由于网络内部的大部分时候都是在进行特征提取, 因此 maxpooling 更常用.</p>
<p>平均池化的作用是可以聚合核内的所有特征信息, 因此通常在整个网络的最后, 我们会使用全局平均池化来整合整体的特征, 此时, 因为特征图谱已经是经过高度提取抽象后的, 所以, 我们不能只关注那些最大的值, 图谱上的每一个值所对应的特征我们都需要综合考虑, 这一点和全连接层本身的计算规则相符合, 因为全连接中的每一个神经元都能够 “看到” 全一层所有的输出, 使得全连接可以通过整合所有的特征信息来决定最终的分类结果, GAP 也能够起到相类似的作用.</p>
<h3 id="全局平均池化层-GAP-的作用"><a href="#全局平均池化层-GAP-的作用" class="headerlink" title="全局平均池化层(GAP)的作用"></a>全局平均池化层(GAP)的作用</h3><ol>
<li>GAP 的第一个作用就是可以替代全连接层, 根据全连接层本身的计算规则可知, 使用全局平均池化, 将池化核的个数设置为全连接的神经元个数, 就可以获得相同维度的计算结果.</li>
<li>全连接层本身用于包含大量的参数, 因此在一定程度上, 全连接层上容易产生过拟合现象, 从而影响整个模型的泛化能力. 而 GAP 本身不包含任何参数, 它直接在 feature map 和样本标签空间内建立了联系, 使得每一个 feature map 本身具有的含义更加清晰, 也就是一个 feature map 对应一个 label. 明确学习目标, 简化学习过程.</li>
<li>全连接层通常需要 dropout 来避免过拟合, 而 GAP 本身就可以看做是一种正则, 因此可以使用模型的泛化性能更好</li>
</ol>
<p><span id="反卷积层"></span></p>
<h2 id="反卷积层"><a href="#反卷积层" class="headerlink" title="反卷积层"></a>反卷积层</h2><p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic</a></p>
<p>又名 Transposed Convolution, Deconvolution, Fractionally-strided convolution.</p>
<h3 id="反卷积和双线性插值的区别-各自的优势"><a href="#反卷积和双线性插值的区别-各自的优势" class="headerlink" title="反卷积和双线性插值的区别, 各自的优势"></a>反卷积和双线性插值的区别, 各自的优势</h3><p>双线性插值, 计算速度快, 实现简单<br>反卷积, 具有学习参数, 可以学习相应特征</p>
<p>Deconvolution是目前争议比较多的方法，主要是名字上的争议，由于实现上采用转置卷积核的方法，所以有人说应该叫(transposed convolution)，但是思想上是为了还原原有特征图，类似消除原有卷积的某种效果，所以叫反卷积(deconvolution).</p>
<h3 id="反卷积计算公式"><a href="#反卷积计算公式" class="headerlink" title="反卷积计算公式"></a>反卷积计算公式</h3><p>大体上相当于是卷积计算公式的逆过程, 不过额外多了 output_padding, 如下所示:</p>
<script type="math/tex; mode=display">H_{out} = (H_{in} - 1)\times stride[0] - 2\times padding[0] + dilation[0]\times(kernel_size[0] - 1) + output_padding[0] + 1</script><script type="math/tex; mode=display">W_{out} = (W_{in} - 1)\times stride[1] - 2\times padding[1] + dilation[1]\times(kernel_size[1] - 1) + output_padding[1] + 1</script><p><span id="空洞卷积"></span></p>
<h2 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h2><p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="noopener">https://github.com/vdumoulin/conv_arithmetic</a></p>
<p>Dilated Convolution</p>
<h3 id="空洞卷积的作用"><a href="#空洞卷积的作用" class="headerlink" title="空洞卷积的作用"></a>空洞卷积的作用</h3><p>可以在不引入额外计算量的前提下, 可以扩大感受野</p>
<p><span id="训练问题"></span></p>
<h2 id="训练问题"><a href="#训练问题" class="headerlink" title="训练问题"></a>训练问题</h2><p><a href="../深度学习-训练问题">训练过程中遇到的问题及解决方案</a></p>
<h3 id="梯度消失和梯度爆炸的产生原因及解决方法"><a href="#梯度消失和梯度爆炸的产生原因及解决方法" class="headerlink" title="梯度消失和梯度爆炸的产生原因及解决方法"></a>梯度消失和梯度爆炸的产生原因及解决方法</h3><p>梯度消失: 中间梯度小于 1, 连乘后趋近于 0<br>产生原因:</p>
<ol>
<li>sigmoid, tanh, 激活函数梯度小于 1</li>
<li>中间层数据分布使得梯度为0(relu 负半区)</li>
<li>网络层数过深</li>
</ol>
<p>解决方法:</p>
<ol>
<li>ReLU</li>
<li>BN</li>
<li>ShortCut</li>
</ol>
<p>梯度爆炸:</p>
<ol>
<li>参数矩阵初始化值过大, 导致梯度值过大</li>
</ol>
<p>解决办法: 预训练初始化, xavier 初始化, msra 初始化</p>
<h3 id="在图像分类任务中-训练数据不足会带来什么问题-如何缓解数据量不足带来的问题"><a href="#在图像分类任务中-训练数据不足会带来什么问题-如何缓解数据量不足带来的问题" class="headerlink" title="在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?"></a><a href="../深度学习-训练问题/#在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?">在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?</a></h3><p>(百面: 1.07.1)<br>带来的问题: <strong>过拟合</strong><br>处理方法</p>
<ul>
<li>基于模型的方法: 采用降低过拟合风险的措施,包括简化模型(如将非线性简化成线性), 添加约束项以缩小假设空间(如L1和L2正则化), 集成学习, Dropout超参数等.</li>
<li>基于数据的方法, 主要通过数据扩充(Data Augmentation), 即根据一些先验知识, 在保持特定信息的前提下, 对原始数据进行适合变换以达到扩充数据集的效果.</li>
</ul>
<p>在图像分类任务中，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换：</p>
<ul>
<li>观察角度：一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等</li>
<li>噪声扰动：椒盐噪声、高斯白噪声</li>
<li>颜色变换：在RGB颜色空间上进行主成分分析</li>
<li>其他：亮度、清晰度、对比度、锐度</li>
</ul>
<p>其他扩充数据方法：特征提取, 在图像的特征空间内进行变换：数据扩充or上采样技术，如SMOTE（Synthetic Minority Over-sampling Technique)。</p>
<p>最后，迁移学习或者用GAN合成一些新样本也可帮助解决数据不足问题。</p>
<h3 id="如何解决数据不均衡问题"><a href="#如何解决数据不均衡问题" class="headerlink" title="如何解决数据不均衡问题?"></a><a href="../深度学习-训练问题/#如何解决数据不均衡问题?">如何解决数据不均衡问题?</a></h3><ul>
<li>在损失函数中使用权重, 对表征性不足的类别使用更高的而权重</li>
<li>过采样: 对样本量少的类进行多次采样</li>
<li>欠采样: 对样本量多的类降低采样频率</li>
<li>数据增广</li>
</ul>
<h3 id="训练不收敛的具体表现是什么-可能的原因是什么-如何解决"><a href="#训练不收敛的具体表现是什么-可能的原因是什么-如何解决" class="headerlink" title="训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?"></a><a href="../深度学习-训练问题/#训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?">训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?</a></h3><p>我们主要通过观察 loss 曲线来判断是否收敛, 根据不同的 loss 曲线, 有以下三种不收敛的情形:</p>
<ol>
<li>从训练开始曲线就一直震荡或者发散<ul>
<li>可能原因: (1) 学习率设置的过大; (2) 向网络中输入的数据是错误数据, 如标签对应错误, 读取图片时将宽高弄反, 图片本身质量极差等;</li>
<li>解决方法: 调节学习率; 检查数据读取代码</li>
</ul>
</li>
<li>在训练过程中曲线突然发散<ul>
<li>可能原因: (1) 学习率设置过大, 或者没有采用衰减策略; (2) 读取到了个别的脏数据, 如标签对应错误, 或者标签为空等</li>
<li>解决方法: 调整学习率及相应的衰减策略; 将 batch size 设为 1, shuffle 置为 false, 检查发散时对应的数据是否正确;</li>
</ul>
</li>
<li>在训练过程中曲线突然震荡<ul>
<li>可能原因: (1) 损失函数中的正则化系数设置有问题, 或者损失函数本身存在 Bug; (2) 数据存在问题</li>
<li>解决方法: 检查损失函数; 检查数据</li>
</ul>
</li>
</ol>
<h3 id="训练过程中出现-Nan-值是什么原因-如何解决"><a href="#训练过程中出现-Nan-值是什么原因-如何解决" class="headerlink" title="训练过程中出现 Nan 值是什么原因? 如何解决?"></a><a href="../深度学习-训练问题/#训练过程中出现 Nan 值是什么原因? 如何解决?">训练过程中出现 Nan 值是什么原因? 如何解决?</a></h3><p>Nan 是 “Not a number” 的缩写, 出现 Nan 的可能情况一般来说有两种:</p>
<ul>
<li>一种是梯度爆炸, 使得某一层计算出来的值超过了浮点数的表示范围</li>
<li>另一种是由于损失函数中 $log$ 项的值出现的负值或者 0 导致的, 因为 $logx$ 只在 $x$ 大于 0 的时候才有意义.</li>
<li>学习率过大, 使得更新的时候值过大, 超出表示范围</li>
<li>batch_size 过大</li>
<li>0 用作了除数</li>
<li>0 或者负数作为自然对数</li>
<li>某些指数计算, 最后计算结果为 INF(无穷)</li>
<li>输入本身就含有 Nan, 有的图片本身就含有 Nan 值</li>
</ul>
<p>解决梯度爆炸的方法通常有:</p>
<ul>
<li>对数据进行归一化(BN, GN 等), 使数据的值不要太大</li>
<li>减小学习率</li>
<li>减小 batch_size, 或者提升数据表示范围, float-&gt;double</li>
<li>加入 gradient clipping</li>
<li>更换参数初始化方法(待商榷).</li>
</ul>
<p>对于 $log$ 项出现负值或者 0 的情况(Softmax 激活函数的取值范围是 [0,1], 因此有可能输出 0), 首先确保网络使用了正确的初始化方法(若参数全为 0, 则输出也为 0), 其次, 检查数据本身是否存在问题, 因为实际业务上的真实数据通常还有大量的脏数据, 有时候数据本身就还有 Nan 值, 因此, 在训练网络之前 先要确保数据是正确的. 可以设计一个简单的小网络, 然后将所有数据跑一遍, 再根据日志信息去除其中的脏数据.</p>
<p>loss 的计算问题, 当我们采用先求取<code>loss</code>的和, 再做归一化除法时, 如果<code>batch_size</code>过大, 那么<code>loss</code>之和可能会超过数据类型的表示上限, 此时, 有两种解决方法, 一种是将数据类型的表示范围提高, 例如将<code>float</code>变成<code>double</code>, 但是这样也不保险, 较好的做法是在计算<code>loss</code>时, 避免添加操作, 或者预先估计loss的大小<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提升 loss 的表示范围, 修改自 ssd.pytorch 代码</span></span><br><span class="line">N = num_pos.data.sum().double()</span><br><span class="line">loss_l = loss_l.double()</span><br><span class="line">loss_c = loss_c.double()</span><br></pre></td></tr></table></figure></p>
<p>如果以上方法都不能解决问题的话, 那应该就是网络结构本身或者损失函数可能存在 Bug, 需要进一步更细致的分析. 如将网络拆解开来, 减少层数, 对不同层进行测试等等.</p>
<h3 id="过拟合是什么-如何处理过拟合"><a href="#过拟合是什么-如何处理过拟合" class="headerlink" title="过拟合是什么? 如何处理过拟合?"></a><a href="../深度学习-训练问题/#过拟合是什么? 如何处理过拟合?">过拟合是什么? 如何处理过拟合?</a></h3><p><strong>过拟合定义:</strong> 当模型在训练数据上拟合的非常好, 但是在训练数据以外的测试集上却不能很好的拟合数据, 此时就说明这个模型出现的过拟合现象.</p>
<p><strong>解决方法:</strong></p>
<ol>
<li>使用正则项(Regularization): <a href="../深度学习-正则化方法深入解析">L1, L2 正则</a></li>
<li>数据增广(Data Augmentation): 水平或垂直翻转图像、裁剪、色彩变换、扩展和旋转等等, 也可利用GAN辅助生成(不常用)</li>
<li>Dropout: Dropout是指在深度网络的训练中, 以一定的概率随机的”临时丢弃”一部分神经元节点.  具体来讲, Dropout作用于每份小批量训练数据, 由于其随机丢弃部分神经元的机制, 相当于每次迭代都在训练不同结构的神经网络, 可以被认为是一种实用的大规模深度神经网络的模型继承算法. 对于包含 $N$ 个神经元节点的网络, 在Dropout的作用下可以看作为 $2^N$ 个模型的集成, 这 $2^N$ 个模型可认为是原始网络的子网络, 它们共享部分权值, 并且拥有相同的网络层数, 而模型整个的参数数目不变, 大大简化了运算. 对于任意神经元来说, 每次训练中都与一组随机挑选的不同的神经元集合共同进行优化, 这个过程会减弱全体神经元之间的联合适应性, 减少过拟合的风险, 增强泛化能力. 工作原理和实现: 应用Dropout包括训练和预测两个阶段, 在训练阶段中, 每个神经元节点需要增加一个概率系数, 在前向传播时, 会以这个概率选择是否丢弃当前的神经元. 在测试阶段的前向传播计算时, 每个神经元的参数都会预先乘以概率系数p, 以恢复在训练中该神经元只有p的概率被用于整个神经网络的前向传播计算</li>
<li>Drop Connect: Drop Connect 是另一种减少算法过拟合的正则化策略，是 Dropout 的一般化。在 Drop Connect 的过程中需要将网络架构权重的一个随机选择子集设置为零，取代了在 Dropout 中对每个层随机选择激活函数的子集设置为零的做法。由于每个单元接收来自过去层单元的随机子集的输入，Drop Connect 和 Dropout 都可以获得有限的泛化性能 [22]。Drop Connect 和 Dropout 相似的地方在于它涉及在模型中引入稀疏性，不同之处在于它引入的是权重的稀疏性而不是层的输出向量的稀疏性。</li>
<li>早停: 早停法可以限制模型最小化代价函数所需的训练迭代次数。早停法通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。早停法通过确定迭代次数解决这个问题，不需要对特定值进行手动设置。</li>
</ol>
<p><strong>Reference:</strong><br><a href="https://www.cnblogs.com/callyblog/p/8094745.html" target="_blank" rel="noopener">https://www.cnblogs.com/callyblog/p/8094745.html</a></p>
<h3 id="欠拟合是什么-如何处理欠拟合"><a href="#欠拟合是什么-如何处理欠拟合" class="headerlink" title="欠拟合是什么? 如何处理欠拟合?"></a><a href="../深度学习-训练问题/#欠拟合是什么? 如何处理欠拟合?">欠拟合是什么? 如何处理欠拟合?</a></h3><p><strong>过拟合定义:</strong> 当模型在训练数据和测试数据上都无法很好的拟合数据时, 说明出现了欠拟合</p>
<p><strong>解决方法:</strong></p>
<ol>
<li>首先看看是否是神经网络本身的拟合能力不足导致的, 具体方法是让神经网络在每次训练时, 只迭代 <strong>同样的数据</strong>, 甚至每一个 batch 里面也是完全相同一模一样的数据, 再来看看 loss 值和 accurancy 值的变化. 如果这时候 loss 开始下降, accurancy 也开始上升了, 并且在训练了一段时间后神经网络能够正确地计算出所训练样本的输出值, 那么这种情况属于神经网络拟合能力不足. 因为对于大量的数据样本, 神经网络由于自身能力的原因无法去拟合全部数据, 只能拟合大量样本的整体特征, 或者少数样本的具体特征. 对于拟合能力不足问题, 通常可以增加网络层数, 增加神经元个数, 增大卷积核通道数等方法.</li>
<li>如果不是拟合能力不足导致的欠拟合, 就需要尝试其他方法, 更改网络初始化方法(Xavier, MSRA), 更改优化器, 降低学习率</li>
</ol>
<h3 id="Dropout-的实现方式在训练阶段和测试阶段有什么不同-如何保持训练和测试阶段的一致性"><a href="#Dropout-的实现方式在训练阶段和测试阶段有什么不同-如何保持训练和测试阶段的一致性" class="headerlink" title="Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?"></a><a href="../深度学习-训练问题/#Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?">Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?</a></h3><p>Dropout 的实现方式有两种:</p>
<ul>
<li>直接 Dropout: 使用较少, AlexNet 使用的是这种Dropout. 该方法在训练阶段会按照保留概率来决定是否将神经元的激活值置为0. <strong>同时, 为了保持训练阶段和测试阶段数值的一致性, 会在测试阶段对所有的计算结果乘以保留概率.</strong></li>
<li>Inverted Dropout: 这是目前常用的方法. 该方法在训练阶段会按照保留概率来决定是否将神经元的激活值置为0, <strong>并且, 在训练阶段会令输出值都会乘以 $\frac{1}{\alpha}$, 这样一来, 在训练阶段可以随时更改 dropout 的参数值, 而对于测试阶段来说, 无需对神经元进行任何额外处理, 所有的神经元都相当于适配了训练过程中 dropout 对参数数值大小带来的影响.</strong></li>
</ul>
<h3 id="Dropout-为什么可以起到防止过拟合的作用"><a href="#Dropout-为什么可以起到防止过拟合的作用" class="headerlink" title="Dropout 为什么可以起到防止过拟合的作用?"></a><a href="../深度学习-训练问题/#Dropout 为什么可以起到防止过拟合的作用?">Dropout 为什么可以起到防止过拟合的作用?</a></h3><ul>
<li><strong>减少神经元之间复杂的共适应关系：</strong> 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）</li>
<li><strong>起到模型集成的作用：</strong> 先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络进行集成。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</li>
</ul>
<h3 id="常用的数据增强方法"><a href="#常用的数据增强方法" class="headerlink" title="常用的数据增强方法"></a>常用的数据增强方法</h3><ul>
<li>水平和垂直旋转或翻转图像</li>
<li>改变图像的亮度和颜色</li>
<li>随机模糊图像</li>
<li>随机从图像裁剪补丁</li>
</ul>
<h3 id="常用的训练-Trick-有哪些-分别介绍"><a href="#常用的训练-Trick-有哪些-分别介绍" class="headerlink" title="常用的训练 Trick 有哪些, 分别介绍"></a>常用的训练 Trick 有哪些, 分别介绍</h3><p>warm up</p>
<p>label smoothing: “软化” 传统的 one-hot 类型标签, 使得在计算损失值时能够有效抑制过拟合现象. $\epsilon$ 是一个小值, 超参数, $K$ 代表类别数</p>
<script type="math/tex; mode=display">P_i = \begin{cases} 1 && i = y \\ 0 && i \neq y \end{cases}  \LongRightarrow P_i = \begin{cases} (1 - \epsilon) && i = y \\ \frac{\epsilon}{K-1} && i \neq y \end{cases}</script><h1 id="网络结构篇"><a href="#网络结构篇" class="headerlink" title="网络结构篇"></a>网络结构篇</h1><p><strong>关于层数的定义</strong>: 值得是网络的深度. 通常只计算卷积层和全连接层, 有一种说法是层数是具有参数的网络层的个数, 不太准确, 因为 BN, SE 等也有参数, 但是通常不计入层数. 另外, 卷积层数不等于卷积个数, 例如 Inception bottleneck 结构, 拥有 9 个卷积层, 但是在计算网络层数时只算两层.<br><strong>参数量, FLOPs</strong>: M = $10^6$, G = $10^9$<br><strong>显存占用, 模型大小</strong>: MB = $2^{20}$, GB = $2^{30}$</p>
<p>InceptionV1:</p>
<ul>
<li>深度 22 (21 Convs + 1 FCs) = 1 Conv + 1 Conv + 1 Conv + 9 Inceptions(2) + 1 FCs</li>
<li>层数 91 (86 FCs + 5 FCs)= 1 Conv + 1 Conv + 1 Conv + 9 Inceptions(9) +  侧枝(1 Conv + 2 FCs) + 侧枝(1 Conv + 2 FCs) + 1 FCs</li>
</ul>
<p>InceptionV3(paper):</p>
<ul>
<li>深度 47 (46 Convs + 1FCs) = 6 Convs + 3 InceptionsA(3) + 5 Inceptions(5) + 2 Inception(3) + 1 FCs</li>
<li>层数: 略</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">层数(深度)</th>
<th style="text-align:center">参数量</th>
<th style="text-align:center">FLOPs</th>
<th style="text-align:center">显存占用 (模型大小, BP 存储)</th>
<th style="text-align:center">img_size</th>
<th style="text-align:center">Acc@1</th>
<th style="text-align:center">Acc@5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center">8 = 5 Convs + 3 FCs</td>
<td style="text-align:center">61.1M</td>
<td style="text-align:center">823.0M +</td>
<td style="text-align:center">242MB (233, 8)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">56.432</td>
<td style="text-align:center">79.194</td>
</tr>
<tr>
<td style="text-align:center">VGGNet16</td>
<td style="text-align:center">16 = 13 Convs + 3 FCs</td>
<td style="text-align:center">138.4M</td>
<td style="text-align:center">17.3G +</td>
<td style="text-align:center">747MB (527, 218)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">71.636</td>
<td style="text-align:center">90.354</td>
</tr>
<tr>
<td style="text-align:center">VGGNet16_BN</td>
<td style="text-align:center">16 = 13 Convs + 3 FCs</td>
<td style="text-align:center">138.4M</td>
<td style="text-align:center">17.3G +</td>
<td style="text-align:center">747MB (527, 218)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">73.518</td>
<td style="text-align:center">91.608</td>
</tr>
<tr>
<td style="text-align:center">VGGNet19</td>
<td style="text-align:center">19 = 16 Convs + 3 FCs</td>
<td style="text-align:center">143.6M</td>
<td style="text-align:center">21.9G</td>
<td style="text-align:center">787MB (548, 238)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">72.080</td>
<td style="text-align:center">90.822</td>
</tr>
<tr>
<td style="text-align:center">VGGNet19_BN</td>
<td style="text-align:center">19 = 16 Convs + 3 FCs</td>
<td style="text-align:center">143.6M</td>
<td style="text-align:center">21.9G</td>
<td style="text-align:center">787MB (548, 238)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">74.266</td>
<td style="text-align:center">92.066</td>
</tr>
<tr>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center">50 = 1 Conv + 48 Convs + 1 FCs</td>
<td style="text-align:center">25.5M</td>
<td style="text-align:center">4.1G</td>
<td style="text-align:center">384MB (97, 286)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">76.002</td>
<td style="text-align:center">92.980</td>
</tr>
<tr>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center">101 = 1 Conv + 99 Convs + 1 FCs</td>
<td style="text-align:center">44.5M</td>
<td style="text-align:center">7.8G</td>
<td style="text-align:center">600MB (169, 429)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">77.438</td>
<td style="text-align:center">93.672</td>
</tr>
<tr>
<td style="text-align:center">ResNet152</td>
<td style="text-align:center">152 = 1 Conv + 150 Convs + 1 FCs</td>
<td style="text-align:center">60.2M</td>
<td style="text-align:center">11.6G</td>
<td style="text-align:center">836MB (229, 606)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">78.438</td>
<td style="text-align:center">94.110</td>
</tr>
<tr>
<td style="text-align:center">InceptionV1 (GoogLeNet)</td>
<td style="text-align:center">22(深度)</td>
<td style="text-align:center">13.0M</td>
<td style="text-align:center">1.5G</td>
<td style="text-align:center">144MB (49, 94)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">69.8</td>
<td style="text-align:center">89.6</td>
</tr>
<tr>
<td style="text-align:center">InceptionV3</td>
<td style="text-align:center">47(深度)</td>
<td style="text-align:center">27.1M</td>
<td style="text-align:center">5.7G</td>
<td style="text-align:center">328MB (103 + 224)</td>
<td style="text-align:center">299</td>
<td style="text-align:center">77.294</td>
<td style="text-align:center">93.454</td>
</tr>
<tr>
<td style="text-align:center">Xception</td>
<td style="text-align:center">29 = 28 Convs + 1 FCs</td>
<td style="text-align:center">22.8M</td>
<td style="text-align:center">8.5G</td>
<td style="text-align:center">887MB (87 + 798)</td>
<td style="text-align:center">229</td>
<td style="text-align:center">78.888</td>
<td style="text-align:center">94.292</td>
</tr>
<tr>
<td style="text-align:center">InceptionV4</td>
<td style="text-align:center">略</td>
<td style="text-align:center">42.6M</td>
<td style="text-align:center">12.3G</td>
<td style="text-align:center">715MB (162 + 551)</td>
<td style="text-align:center">299</td>
<td style="text-align:center">80.062</td>
<td style="text-align:center">94.926</td>
</tr>
<tr>
<td style="text-align:center">InceptionResNetV2</td>
<td style="text-align:center">略</td>
<td style="text-align:center">55.8M</td>
<td style="text-align:center">16.7G</td>
<td style="text-align:center">904MB (213 + 689)</td>
<td style="text-align:center">299</td>
<td style="text-align:center">80.170</td>
<td style="text-align:center">95.234</td>
</tr>
<tr>
<td style="text-align:center">ResNeXt50</td>
<td style="text-align:center">50</td>
<td style="text-align:center">25.0M</td>
<td style="text-align:center">4.2G</td>
<td style="text-align:center">457MB (95 + 361)</td>
<td style="text-align:center">-</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">ResNeXt101_32x8d</td>
<td style="text-align:center">101</td>
<td style="text-align:center">88.8M</td>
<td style="text-align:center">16.5G</td>
<td style="text-align:center">1111MB (338 + 772)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">78.188</td>
<td style="text-align:center">93.886</td>
</tr>
<tr>
<td style="text-align:center">MobileNetV1</td>
<td style="text-align:center">28 = 27 Convs + 1 FC</td>
<td style="text-align:center">4.2M</td>
<td style="text-align:center">569M</td>
<td style="text-align:center"></td>
<td style="text-align:center">-</td>
<td style="text-align:center">70.6</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">MobileNetV2</td>
<td style="text-align:center">(原文)54 Convs = 1 + 51 + 2 <br> (PT实现)53 = 1 + 51 Convs + 1 FC</td>
<td style="text-align:center">3.5M</td>
<td style="text-align:center">334.8M</td>
<td style="text-align:center">166MB (13 + 153)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">71.88</td>
<td style="text-align:center">90.29</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNetV1</td>
<td style="text-align:center">50 = 1 + 48 Convs + 1 FC</td>
<td style="text-align:center">1.9M</td>
<td style="text-align:center">275.8M</td>
<td style="text-align:center">69MB (7 + 62)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNetV2</td>
<td style="text-align:center">51 = 1 + 48 + 1 Convs + 1 FC</td>
<td style="text-align:center">2.3M</td>
<td style="text-align:center">155.2M</td>
<td style="text-align:center">57MB (8 + 48)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">69.36</td>
<td style="text-align:center">88.32</td>
</tr>
<tr>
<td style="text-align:center">DenseNet121</td>
<td style="text-align:center">121 = 1 + 3 + 116 Convs + 1 FC</td>
<td style="text-align:center">7.9M</td>
<td style="text-align:center">2.9G</td>
<td style="text-align:center">325MB (30 + 294)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">74.646</td>
<td style="text-align:center">92.136</td>
</tr>
<tr>
<td style="text-align:center">DenseNet161</td>
<td style="text-align:center">161 = 1 + 3 + 156 Convs + 1 FC</td>
<td style="text-align:center">28.7M</td>
<td style="text-align:center">7.8G</td>
<td style="text-align:center">647MB (109 + 536)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">77.560</td>
<td style="text-align:center">93.798</td>
</tr>
<tr>
<td style="text-align:center">SENet154</td>
<td style="text-align:center">…</td>
<td style="text-align:center">115.1M</td>
<td style="text-align:center">20.8G</td>
<td style="text-align:center">1517MB (439 + 1077)</td>
<td style="text-align:center">-</td>
<td style="text-align:center">81.32</td>
<td style="text-align:center">95.53</td>
</tr>
</tbody>
</table>
</div>
<p><span id="AlexNet"></span></p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/AlexNet.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FAlexNet.jpg"></div></p>
<p>AlexNet 的网络结构相对来说比较简单, 它包括 5 层卷积层, 3 层最大池化层, 以及 3 层全连接层. 池化层被分别放置在 conv1, conv2, 和 conv5 的后面. 虽然 AlexNet 结构简单, 但是由于全连接层的存在, 使得 AlexNet 的参数量较大, 大约有 6000w 个参数.</p>
<p><span id="VGGNet"></span></p>
<h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/VGGNet.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FVGGNet.jpg"></div></p>
<p>VGGNet 的网络结构延续了 AlexNet 的设计思想. 将卷积层分成 5 段, 每一段之间通过池化层分隔开, 后面同样跟了 3 层全连接层, 同时他用多个小卷积核替换了 AlexNet 中的大卷积核, 可以在减少参数量的同时提高感受野的范围, 并且通过统建更深层的网络, 使得提取到的特征图谱的表征能力更强. VGGNet 比较常用的结构有 VGG16 和 VGG19. 二者的区别在于前者每个卷积段的卷积层数量是(2, 2, 3, 3, 3), 后者每个卷积段中的卷积层数量是(2, 2, 4, 4, 4).</p>
<p><span id="InceptionV1"></span></p>
<h2 id="InceptionV1-GoogLeNet"><a href="#InceptionV1-GoogLeNet" class="headerlink" title="InceptionV1 (GoogLeNet)"></a><a href="../计算机视觉-InceptionV1">InceptionV1 (GoogLeNet)</a></h2><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV1_module.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInceptionV1_module.jpg"></div></p>
<p>GoogLeNet 模型的核心思想是 <strong>卷积神经网络中的最优局部稀疏结构可以被现有的组件逼近和覆盖, 因此, 只要找到这个局部最优结构, 并在网络结构中重复使用它, 就可以进一步提升神经网络的拟合能力.</strong> 于是, InceptionV1 跳出了传统卷积神经网络的简单堆叠结构, 首次提出了 Inception 模块. Inception 模块综合了 1x1, 3x3, 5x5 这三种不同尺度的卷积核进行特征提取, 同时, 考虑到池化层的重要作用, 还综合了 3x3 大小的最大池化层. 并且, 还在 3x3 和 5x5 的卷积层之前, 以及池化层之后, 使用了 1x1 的卷积层来降低特征维度, 从而减少计算量.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInceptionV1.jpg"></div></p>
<p>以 Inception 模块为基本单元就可以构建出 IncetionV1 模型, 构建时仍然遵循了 5 个卷积段的段落形式, 段之间通过最大池化层分隔, 具体来说, 前两段使用的是传统的卷积层, 其中第一段是单层的 7x7 大小的卷积层, 第二段是两层较小尺寸的卷积层(1x1, 3x3)(上图没有写出 1x1), 后三段卷积段都是由 Inception 模块组成, 每一段使用的 Inception 模块数量分别为 2, 5, 2. 最后的分类层由全局平均池化层, 全连接层, Softmax 激活层组成. 另外, 由于网络结构较深, 因此, 为了防止梯度消失, <strong>InceptionV1 分别在 4a 和 4d 的 Inception 模块上添加了辅助侧枝分类器, 该分类器由一层平均池化层, 一层 1x1 卷积层, 两层全连接层和 Softmax 激活层组成.</strong></p>
<h3 id="简述一下-GoogLeNet-采用多个卷积核的原因"><a href="#简述一下-GoogLeNet-采用多个卷积核的原因" class="headerlink" title="简述一下 GoogLeNet 采用多个卷积核的原因"></a><a href="../计算机视觉-InceptionV1/#简述一下 GoogLeNet 采用多个卷积核的原因">简述一下 GoogLeNet 采用多个卷积核的原因</a></h3><p>Inception Module这类结构非常看中模型在局部区域的拟合能力。它们认为：一张图像通常具有总体特征和细节特征这两类特征，一般小卷积核能够更好的捕捉一些细节特征，随着深层网络的小卷积不断计算下去，总体特征也会慢慢的被提炼出来(感受野慢慢增大)，但是这样存在一个问题，那就是在如果只采用小卷积，那么网络结构的前段一般只有细节特征，后段才慢慢有一些总体特征(感受野增大)，而我们希望这两方面的特征总是能够一起发挥作用，因此，Inception 模型考虑采用更多不同尺寸的卷积核来提取特征，并把这些特征连接起来，一起送到后面的网络中去计算，使得网络可以获取到更多的特征信息。</p>
<h3 id="Inception-中为什么使用-1×1-卷积层"><a href="#Inception-中为什么使用-1×1-卷积层" class="headerlink" title="Inception 中为什么使用 1×1 卷积层"></a><a href="../计算机视觉-InceptionV1/#Inception 中为什么使用 1×1 卷积层">Inception 中为什么使用 1×1 卷积层</a></h3><p>关于Inception Module，有一种很直接的做法就是将1×1,3×3,5×5卷积和3×3 max pooling直接连接起来，如 Inception module 中的 naive version 所示，但是这样的话就有个问题，那就是计算量增长太快了。</p>
<p>为了解决这个问题，文章在3×3和5×5的卷积之前，3×3max pooling之后使用了1×1卷积，<strong>使其输出的 feature map 的 depth 降低了</strong>，从而达到了降维的效果，抑制的过快增长的计算量。</p>
<h3 id="Inception-中为什么使用全局平均池化层"><a href="#Inception-中为什么使用全局平均池化层" class="headerlink" title="Inception 中为什么使用全局平均池化层"></a><a href="../计算机视觉-InceptionV1/#Inception 中为什么使用全局平均池化层">Inception 中为什么使用全局平均池化层</a></h3><ol>
<li>GAP 的第一个作用就是可以替代全连接层, 根据全连接层本身的计算规则可知, 使用全局平均池化, 将池化核的个数设置为全连接的神经元个数, 就可以获得相同维度的计算结果.</li>
<li>全连接层本身用于包含大量的参数, 因此在一定程度上, 全连接层上容易产生过拟合现象, 从而影响整个模型的泛化能力. 而 GAP 本身不包含任何参数, 它直接在 feature map 和样本标签空间内建立了联系, 使得每一个 feature map 本身具有的含义更加清晰, 也就是一个 feature map 对应一个 label. 明确学习目标, 简化学习过程.</li>
<li>全连接层通常需要 dropout 来避免过拟合, 而 GAP 本身就可以看做是一种正则, 因此可以使用模型的泛化性能更好</li>
</ol>
<h3 id="为什么使用侧枝"><a href="#为什么使用侧枝" class="headerlink" title="为什么使用侧枝"></a><a href="../计算机视觉-InceptionV1/#为什么使用侧枝">为什么使用侧枝</a></h3><p>为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。也就是说在测试的时候，只会用最后的softmax结果作为分类依据。</p>
<p>当时Inception网络还是太深了，不好训练，因此网络中还加了两个侧枝，通过中间层的feature map，来得到预测结果（有了ResNet的shortcut以后，这种侧枝用的比较少了）。</p>
<h3 id="GoogLeNet-在哪些地方使用了全连接层"><a href="#GoogLeNet-在哪些地方使用了全连接层" class="headerlink" title="GoogLeNet 在哪些地方使用了全连接层"></a><a href="../计算机视觉-InceptionV1/#GoogLeNet 在哪些地方使用了全连接层">GoogLeNet 在哪些地方使用了全连接层</a></h3><p>在两个侧枝使用了 Avgpool+conv+FC+FC+SoftmaxActivation 的结构，在最后一层使用了 GAP+FC+SoftmaxActivation的结构。</p>
<p><span id="InceptionV2/3"></span></p>
<h2 id="InceptionV2-3"><a href="#InceptionV2-3" class="headerlink" title="InceptionV2/3"></a><a href="../计算机视觉-InceptionV3">InceptionV2/3</a></h2><h3 id="简述-InceptionV2-相比于-GoogLeNet-有什么区别"><a href="#简述-InceptionV2-相比于-GoogLeNet-有什么区别" class="headerlink" title="简述 InceptionV2 相比于 GoogLeNet 有什么区别"></a>简述 InceptionV2 相比于 GoogLeNet 有什么区别</h3><p>InceptionV2 改进的主要有两点. 一方面加入了 BN 层, 减少了 Internal Covariate Shift 问题(内部网络层的数据分布发生变化), 另一方面参考了 VGGNet 用两个 $3\times 3$ 的卷积核替代了原来 Inception 模块中的 $5\times 5$ 卷积核, 可以在降低参数量的同时加速计算.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV3_1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInceptionV3_1.jpg"></div></p>
<h3 id="简述-InceptionV3-相比于-GoogLeNet-有什么区别"><a href="#简述-InceptionV3-相比于-GoogLeNet-有什么区别" class="headerlink" title="简述 InceptionV3 相比于 GoogLeNet 有什么区别"></a>简述 InceptionV3 相比于 GoogLeNet 有什么区别</h3><p>InceptionV3 最重要的改进是分解(Factorization), 这样做的好处是既可以加速计算(多余的算力可以用来加深网络), 有可以将一个卷积层拆分成多个卷积层, 进一步加深网络深度, 增加神经网络的非线性拟合能力, 还有值得注意的地方是网络输入从 $224\times 224$ 变成了 $299\times 299$, 更加精细设计了 $35\times 35$, $17\times 17$, $8\times 8$ 特征图谱上的 Inception 模块.<br>具体来说, 首先将第一个卷积段的 $7\times 7$ 大小的卷积核分解成了 3 个 $3\times 3$ 大小的卷积核. 在第二个卷积段也由 3 个 $3\times 3$ 大小的卷积核组成. 第三个卷积段使用了 3 个 Inception 模块, 同时将模块中的 $5\times 5$ 卷积分解成了两个 $3\times 3$ 大小的卷积. 在第四个卷积段中, 使用了 5 个分解程度更高的 Inception 模块, 具体来说, 是将 $n\times n$ 大小的卷积核分解成 $1\times n$ 和 $n\times 1$ 大小的卷积核, 在论文中, 对于 $17\times 17$ 大小的特征图谱, 使用了 $n = 7$ 的卷积分解形式. 在第五个卷积段中, 面对 $8\times 8$ 大小的特征图谱, 使用了两个设计更加精细的 Inception 模块. 它将 $3\times 3$ 大小的卷积层分解成 $1\times 3$ 和 $3\times 1$ 的卷积层, 这两个卷积层不是之前的串联关系, 而是并联关系.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV3.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInceptionV3.jpg"></div></p>
<ul>
<li><a href="../计算机视觉-InceptionV3/#Inception 模块的设计和使用原则是什么">Inception 模块的设计和使用原则是什么</a></li>
</ul>
<ol>
<li>在网络的浅层要避免过度的压缩特征信息, 特征图谱的尺寸应该温和的降低;</li>
<li>高维的特征信息更适合在本地进行处理, 在网络中逐渐增加非线性激活层, 这样可以使得网络参数减少, 训练速度更快;</li>
<li>低维信息的空间聚合不会导致网络表达能力的降低, 因此, 当进行大尺寸的卷积之前, 可以先对输入进行进行降维处理, 然后再进行空间聚合操作;</li>
<li>网络的深度和宽度需要反复权衡, 通过平衡网络中每层滤波器的个数和网络的层数使用网络达到最大性能.</li>
</ol>
<p><span id="Xception"></span></p>
<h2 id="Xception"><a href="#Xception" class="headerlink" title="Xception"></a><a href>Xception</a></h2><h3 id="简述-Xception-的特点"><a href="#简述-Xception-的特点" class="headerlink" title="简述 Xception 的特点"></a>简述 Xception 的特点</h3><p>Xception 就是利用 Separable Conv 搭建的网络, Separable Conv 就是将普通的卷积按照通道个数进行分组, 使得每一组内只包含一个通道, 将 spatial learning 和 cross depth learning 解耦.</p>
<p>Inception模块是一大类在ImageNet上取得顶尖结果的模型的基本模块，例如GoogLeNet、Inception V2/V3和Inception-ResNet。有别于VGG等传统的网络通过堆叠简单的3x3卷积实现特征提取，Inception模块通过组合1x1，3x3，5x5和pooling等结构，用更少的参数和更少的计算开销可以学习到更丰富的特征表示。通常，在一组特征图上进行卷积需要三维的卷积核，也即卷积核需要 <strong>同时学习空间上的相关性和通道间的相关性。将这两种相关性显式地分离开来，是Inception模块的思想之一：Inception模块首先使用1*1的卷积核将特征图的各个通道映射到一个新的空间，在这一过程中学习通道间的相关性；再通过常规的3x3或5x5的卷积核进行卷积，以同时学习空间上的相关性和通道间的相关性。</strong><br><strong>但此时，通道间的相关性和空间相关性仍旧没有完全分离，也即3x3或5x5的卷积核仍然是多通道输入的，</strong> 那么是否可以假设它们们可以被完全分离？显然，当所有3x3或5x5的卷积都作用在只有一个通道的特征图上时，通道间的相关性和空间上的相关性即达到了完全分离的效果(解耦)。</p>
<p>若将Inception模块简化，仅保留包含3x3的卷积的分支, 就如下图所示</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/Xception/fig2.png?x-oss-process=style/blog_img" alt="Xception%2Ffig2.png"></div></p>
<p>再将所有1*1的卷积进行拼接, 就如下图3所示, 进一步增多3x3的卷积的分支的数量，使它与1x1的卷积的输出通道数相等, 就如下图4所示了.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/Xception/fig34.png?x-oss-process=style/blog_img" alt="Xception%2Ffig34.png"></div></p>
<p><strong>此时每个3x3的卷积即作用于仅包含一个通道的特征图上</strong>，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。事实上，调节每个3x3的卷积作用的特征图的通道数，即调节3*3的卷积的分支的数量与1x1的卷积的输出通道数的比例，可以实现一系列处于传统Inception模块和“极致的Inception”模块之间的状态。</p>
<p>运用“极致的Inception”模块，作者搭建了Xception网络，它由一系列SeparableConv（即“极致的Inception”）、类似ResNet中的残差连接形式和一些其他常规的操作组成, 如下图所示. Xception</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/Xception/fig5.png?x-oss-process=style/blog_img" alt="Xception%2Ffig5.png"></div></p>
<h3 id="Xception-中使用的-“extreme”-inception-module-的-MobileNet-中使用的传统的-Depthwise-Separable-Conv-有什么区别"><a href="#Xception-中使用的-“extreme”-inception-module-的-MobileNet-中使用的传统的-Depthwise-Separable-Conv-有什么区别" class="headerlink" title="Xception 中使用的 “extreme” inception module 的 MobileNet 中使用的传统的 Depthwise Separable Conv 有什么区别?"></a>Xception 中使用的 “extreme” inception module 的 MobileNet 中使用的传统的 Depthwise Separable Conv 有什么区别?</h3><p>区别主要有两点:</p>
<ol>
<li>在 MobileNet 中的 Separable Conv 中, 是先进行 Depthwise Conv, 再进行 1x1 的 Pointwise Conv; 而 Xception 主要是优化 InceptionV3, 因此它是先进性 1x1 的卷积, 然后再进行 Depthwise Conv; (顺序的区别影响不大, 因为这些操作是堆叠组合的, 相对位置差不多)</li>
<li>在 Xception 中, 作者发现在 1x1 的卷积之后不使用 ReLU 等激活函数时具有较好的效果. 这一点在 MobileNetV2 中被详细分析过, 主要是因为通道在经过 1x1 降维后, 过少的通道数目容易被 ReLU 激活函数破坏其携带的特征信息, 因此在 1x1 降维后, 通常不是激活数.(ShuffleNet 也是如此, 只不过他的通道数是在 depthwise 的时候很小, 所以没有在 depthwise conv 之后使用 relu, 但是疑惑的是为什么第一个 1x1 GConv 的输出通道数也很低, 但是仍然用了 ReLU?)</li>
</ol>
<p><strong>面对通道数较低的特征图谱, 使用 ReLU 会因为低维数据坍塌现象, 即 ReLU 有可能会让某个通道的值全为 0, 从而维度降低, 这些低纬度时会引起信息缺少, 而其他的激活函数又存在梯度消失问题, 所以没有使用激活</strong></p>
<p><span id="InceptionV4"></span></p>
<h2 id="InceptionV4-and-Inception-ResNet"><a href="#InceptionV4-and-Inception-ResNet" class="headerlink" title="InceptionV4 and Inception ResNet"></a><a href="../计算机视觉-InceptionV4-InceptionResNet">InceptionV4 and Inception ResNet</a></h2><p><strong>Inception 系列的缺点:</strong> 模块过于复杂, 人工设计的痕迹太重了.</p>
<h3 id="InceptionV4-做了哪些改进"><a href="#InceptionV4-做了哪些改进" class="headerlink" title="InceptionV4 做了哪些改进"></a>InceptionV4 做了哪些改进</h3><p>InceptionV4 使用了更复杂的结构重新设计了 Inception 模型中的每一个模块. 包括 Stem 模块, 三种不同的 Inception 模块以及两种不同的 Reduction 模块. 每一个模块的具体参数设置均不太一样, 但是整体来说都遵循的 <strong>卷积分解和空间聚合</strong> 的思想.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/InceptionV4.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInceptionV4.jpg"></div></p>
<h3 id="Inception-ResNet-v1-做了哪些改进"><a href="#Inception-ResNet-v1-做了哪些改进" class="headerlink" title="Inception-ResNet-v1 做了哪些改进"></a>Inception-ResNet-v1 做了哪些改进</h3><p>Inception ResNet v1 网络主要被用来与 Inception v3 模型性能进行比较, 因此它所用的 Inception 子网络的计算相对常规模块有所减少, 这是为了保证使得它的整体计算和内存消耗与 Inception v3 近似, 如此才能保证公平性. 具体来说, Inception ResNet v1 网络主要讲 ResNet 中的残差思想用到了 Inception 模块当中, 对于每一种不太的 Inception 模块, 都添加了一个短接连接来发挥残差模型的优势.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/Inception-ResNet-v1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInception-ResNet-v1.jpg"></div></p>
<h3 id="Inception-ResNet-v2-做了哪些改进"><a href="#Inception-ResNet-v2-做了哪些改进" class="headerlink" title="Inception-ResNet-v2 做了哪些改进"></a>Inception-ResNet-v2 做了哪些改进</h3><p>Inception ResNet v2 主要被设计来探索残差模块用于 Inception 网络时所尽可能带来的性能提升. 因此它是论文给出的最终性能最高的网络设计方案, 它和 Inception ResNet v1 的不同主要有两点, 第一是使用了 InceptionV4 中的更复杂的 Stem 结构, 第二是对于每一个 Inception 模块, 其空间聚合的维度都有所提升. 其模型结构如下所示:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/Inception-ResNet-v2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FInception-ResNet-v2.jpg"></div></p>
<p><span id="ResNet"></span></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a><a href="../计算机视觉-ResNet-CVPR2016">ResNet</a></h2><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNet.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FResNet.jpg"></div></p>
<h3 id="简单介绍一下-ResNet"><a href="#简单介绍一下-ResNet" class="headerlink" title="简单介绍一下 ResNet"></a>简单介绍一下 ResNet</h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNet_block.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FResNet_block.jpg"></div></p>
<p>ResNet 的网络结构依然遵循经典的五段式, 其中第一段是一个比较大的卷积层(7x7, 64, s2), 主要用来获取最初的特征图谱, 后四段是由不同数量的残差模块组成的, 残差模块有两种, 分别为基本的 ResNet Block(上图左侧) 和经过 1x1 卷积降维的 Bottleneck(上图右侧). 在 ResNet50 以上的通常都用 Bottleneck, 因为不仅层数更深, 同时参数量也更少, 提取特征的能力更强一些.<br>每一段使用的残差模块的数量都不同一样, 深层残差网络的 <strong>残差模块主要在 conv4 大量堆叠.</strong></p>
<h3 id="简述-ResNet-的提出动机"><a href="#简述-ResNet-的提出动机" class="headerlink" title="简述 ResNet 的提出动机"></a>简述 ResNet 的提出动机</h3><p>首先文章提出了一个假设:<br>有一个L层的深度神经网络, 如果我们在上面加入一层, 直观来讲得到的L+1层深度神经网络的效果应该至少不会比L层的差. 因为可以简单的学习出最后一层为前一层的恒等映射, 并且其它层参数设置不变.(说明是这种更深的网络是存在是的性能不下降的解的)<br>但是, 通过实验发现, 当网络层数加深时, 网络的性能会下降(说明后面几层网络层没有学习到恒等映射这个解), 也就是所谓的”模型退化”问题,</p>
<p>观察这个现象后, 作者认为产生模型退化的根本原因很大程度上也许不在于过拟合, 而在于梯度消失问题. 为了解决模型退化问题, 作者基于以上假设, 提出了<code>深度残差学习框架</code>, 没有直接堆叠网络层来 fit 期望的映射函数, 而是选择让这些网络层来 fit 一个残差映射. 也就是说, 如果我们期望得到的映射函数为 $H(x)$, 那么我们不是通过堆叠网络来直接学习这个映射函数, 而是学习对应的残差函数: $F(x):=H(x)-x$. 那么, 原始的映射函数就可以通过 $F(x)+x$ 得到(如图2所示). 我们认为这个残差映射比原始的映射函数更容易学习和优化. 因为我们可以在原始网络之上直接添加一个恒等连接, 进而加深网络深度, 但是保持网络性能不降低. 这样一来, 在深层网络中, 如果某一层的输出已经较好的拟合了期望结果, 那么它们的梯度就会被直接传送到两层网络之前, 从而减少了深度神经网络中由于连乘问题导致的梯度消失现象, 进而使得网络有可能拟合到更好的结果上.</p>
<h3 id="ResNet-中可以使用哪些短接方式"><a href="#ResNet-中可以使用哪些短接方式" class="headerlink" title="ResNet 中可以使用哪些短接方式"></a><a href="../计算机视觉-ResNet-CVPR2016/#ResNet 中可以使用哪些短接方式">ResNet 中可以使用哪些短接方式</a></h3><p>基本来说, 有三种选项可以选择</p>
<ul>
<li>(A). 使用恒等映射, 如果需要改变输出维度时, 对增加的维度用0来填充, 不会增加任何参数.(这种就是之前讨论的 parameter-free 的恒等短接)</li>
<li>(B). 在输入输出维度一致时使用恒等映射, 不一致时使用矩阵映射以保证维度一致, 增加部分参数.</li>
<li>(C). 对所有的block均使用矩阵映射, 大量增加参数</li>
</ul>
<p>在效果上, 通常 C&gt;B&gt;A, 我们认为这是因为在 A 中的 zero-padded dimensions 实际上并没有进行残差学习. 但是由于 A/B/C 之间的差距比较小, 而线性变换需要引进额外的参数, 因此这是一个可以根据实际问题进行权衡的事情(通常不要用C, 因为增加的参数较多, 且性能提升并不是很明显).<br>对于 bottleneck 结构来说, parameter-free 的恒等短接尤其重要. 如果用矩阵映射替换了 bottleneck 中的恒等短接, 那么因为 shortcuts 需要处理的维度很高, 使得模型的 size 和时间复杂度都会加倍. 因此, 对于 bottlenect 来说, 选择恒等短接可以大大降低模型复杂度.</p>
<h3 id="如何理解所谓的残差-F-x-比原始目标-H-x-更容易优化"><a href="#如何理解所谓的残差-F-x-比原始目标-H-x-更容易优化" class="headerlink" title="如何理解所谓的残差 $F(x)$ 比原始目标 $H(x)$ 更容易优化"></a><a href="../计算机视觉-ResNet-CVPR2016/#如何理解所谓的残差比原始目标更容易优化">如何理解所谓的残差 $F(x)$ 比原始目标 $H(x)$ 更容易优化</a></h3><p>假设我们要学习一种从输入x到输出H(x)的mapping, 最简单的例子, 假设解空间里的函数只有两个，就是在这两个可能的mapping 函数里面选择一个更好的。<br>另一方面, 由于恒等连接的存在, 当我们令学得的 $F(x)=0$ 时, 那么就有 $H(x)=x$, 比如如果我们将残差模块拼接在普通的 vgg 网络之后, 最终的模型性能也不会比 vgg 差, 因为后面几层相当于是一种恒等短接, 也可以认为是为模型的性能做到了一种保底措施.</p>
<p>也可以理解为, 在最差的情况下, 我们可以完全不学习, 这样依然不会损坏精度, 所以有了保底措施, 就更容易优化.</p>
<p><del>如果是非resnet的情况，那么给定 $H(5)＝5.1$ 和 $H(5)＝5.2$ 这两个函数映射, 其对应权重参数分别是 $H(x) = wx = \frac{5.1}{5} x$  和 $H(x) =w  x = \frac{5.2}{5} x$ ，这两个函数的w近似的都近似等于1, 或者说一个 $w$ 是另一个 $w$ 的1.04/1.02＝1.0196倍. 也就是说，如果用sgd来选择参数 $w$ 的话，是容易认为两个 $w$ 很像的(对数据不敏感, 导致训练慢，学错)。</del><br><del>但是resnet就不同了，在resnet下，原输入输出数据相当于变成了 $H(5)=0.1$ 和 $H(5)=0.2$, 这两个对应的潜在函数变成了 $F(x)= wx = \frac{0.1}{5} x$ 和 $H(x) = wx = \frac{0.2}{5} x$ , 两个 $w$ 的关系变成了一个 $w$ 是另一个 $w$ 的0.2／0.1 ＝ 2倍，所以 $w$ 的选取对于数据集非常敏感了。 这是基于这个原因，resnet里面的参数 $w$ 会更加”准确”反映数据的细微变化。(因此也更容易学到不同数据的特征)</del>(感觉不太对, ResNet 的实现上, 只是直接将 out = x, 而其中的权重依然是 H(x), 而不是 F(x), 所以, 上面的说法不太对)</p>
<h3 id="为什么恒等映射x之前的系数是1-而不是其他的值-比如0-5"><a href="#为什么恒等映射x之前的系数是1-而不是其他的值-比如0-5" class="headerlink" title="为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5"></a><a href="../计算机视觉-ResNet-CVPR2016/#为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5">为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5</a></h3><p>关于为什么是 $x$　而不是 $\lambda_i x$,<br>主要是因为如果是 $\lambda_i x$ 的话,梯度里面  就会有一项 $\lambda_i$ 的连乘 $\prod_{i=1}^{L-1}{\lambda_i}$，就是从输出到当前层之间经过的 shortcut上的所有$\lambda_i$相乘，假如$\lambda_i$都大于 1 那经过多层之后就会爆炸，都小于1就会趋向0而引发梯度消失.</p>
<p><del>具体公式分析可见下面关于”用简单缩放来替代恒等连接”的讨论</del></p>
<h3 id="ResNet-为什么好"><a href="#ResNet-为什么好" class="headerlink" title="ResNet 为什么好"></a><a href="../计算机视觉-ResNet-CVPR2016/#ResNet 到底解决了一个什么问题">ResNet 为什么好</a></h3><p>因为 shortcut 可以解决梯度消失问题</p>
<p>补问: 后面都使用 relu 了, 导数恒等于 1, 怎么还会有梯度消失问题?</p>
<p>一方面, relu 正半轴导数为 1, 可以缓解梯度消失问题, 但是负半区导数为 0, 仍然存在梯度消失问题.<br>另一方面, 存在 relu 那一层的导数为 1, 但是其他层的导数并不一定为 1, 神经网络中不只有 relu 这一种变换, 其他的网络层的导数若大量小于 1, 仍然能够引起梯度消失问题.<br>故relu只是相对于其他激活函数可以缓解梯度消失，并不能消除。</p>
<h3 id="ResNet-残差模块中激活层应该如何放置"><a href="#ResNet-残差模块中激活层应该如何放置" class="headerlink" title="ResNet 残差模块中激活层应该如何放置"></a><a href="../计算机视觉-ResNet-CVPR2016/#ResNet 残差模块中激活层应该如何放置">ResNet 残差模块中激活层应该如何放置</a></h3><p>推荐采用预激活的方式来放置激活层: BN+ReLU+Conv</p>
<p><span id="ResNeXt"></span></p>
<h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a><a href="../计算机视觉-ResNeXt-CVPR2017">ResNeXt</a></h2><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNeXt1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FResNeXt1.jpg"></div></p>
<h3 id="ResNeXt-在-ResNet-上做了哪些改进"><a href="#ResNeXt-在-ResNet-上做了哪些改进" class="headerlink" title="ResNeXt 在 ResNet 上做了哪些改进"></a><a href="../计算机视觉-ResNeXt-CVPR2017/#ResNeXt 在 ResNet 上做了哪些改进">ResNeXt 在 ResNet 上做了哪些改进</a></h3><p>ResNeXt 实际上是将 ResNet Block 当中的输入数据的通道划分到了不同的组, 每个组的计算过程相对独立, 最终将所有组的计算结果进行空间聚合, 作为最终的输出. ResNeXt 可以在不增加参数量的情况下进一步提高 ResNet 的特征提出能力, 从而表现出更好的网络性能. ResNeXt 的卷积方式实际上可以看做是通道分组卷积. 在实现上, 只需要将普通的卷积替换成 Group Conv 即可, 下图是 Group=32 的示例.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNeXt2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FResNeXt2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/ResNeXt3.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FResNeXt3.jpg"></div></p>
<p><span id="DenseNet"></span></p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a><a href>DenseNet</a></h2><h3 id="简述-DenseNet-的原理"><a href="#简述-DenseNet-的原理" class="headerlink" title="简述 DenseNet 的原理"></a><a href="../计算机视觉-DenseNet-CVPR2017/#简述 DenseNet 的原理">简述 DenseNet 的原理</a></h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/DenseNet1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FDenseNet1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/net_arch/DenseNet2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fnet_arch%2FDenseNet2.jpg"></div></p>
<p>在训练特别深层的网络时, 随着深度的增加, 梯度消失的问题会越来越明显, 对此, ResNet 给出了一个很好的解决方案, 那就是在接近输入层的网络中添加一个短接路径到靠近输出层的网络. DenseNet 也延续了这个思路, 与 ResNet 不同的是, 他采用了一个更暴力的方式, 就是将所有网络层都连接起来, 具体来说, 就是每一层的输入会来自于 <strong>前面所有层的输出</strong>(这些层的特征图谱大小是相同的, 因此可以在 Channel 维度上进行叠加). 如果假设一个 DenseBlock 的层数是 $L$, 那么 DenseNet 就会有 $\frac{L(L+1)}{2}$ 个短接路径. 在传统的卷积神经网络中, 通常会利用 Pooling 层或者卷积层来降低特征图谱的大小, 而 DenseNet 的密集连接需要特征图大小保持一致, 因此, 为了解决这个问题,  DenseNet 将网络分成了若干个 DenseBlock + Transition 的结构. 在 DenseBlock 内部, 特征图谱大小相同, 可以直接连接, 在具有不同大小特征图谱的 DenseBlock 之间, 使用 Transition Layer 进行过渡.</p>
<h3 id="DenseBlock-每一层输出的特征图谱是怎么构成的-如何确定输出通道数"><a href="#DenseBlock-每一层输出的特征图谱是怎么构成的-如何确定输出通道数" class="headerlink" title="DenseBlock 每一层输出的特征图谱是怎么构成的, 如何确定输出通道数"></a>DenseBlock 每一层输出的特征图谱是怎么构成的, 如何确定输出通道数</h3><p>DenseBlock 内部的网络层都具有相同大小的特征图谱, 因此可以直接使用 dense connect 的方式进行连接. 在 DenseBlock 内部, 每一层网络输出的特征图谱的通道数是通过一个 <strong>超参数增长率 $k$</strong> 来决定的, 这个 $k$ 可以设定的比较小, 比如32, 在 DenseBlock 中, 所有的网络层都输出 $k$ 维通道, 虽然每一个网络层的输出图谱通道数较低, 但是每一层的输入是综合前面所有层的输出的, 因此随着网络层的增加, 其最终的输出通道数依然是增加的(会叠加前面所有的输出), 只不过每一层只有 $k$ 层通道数自己独自输出的.</p>
<h3 id="Transition-Layer-怎么构成的-有什么作用"><a href="#Transition-Layer-怎么构成的-有什么作用" class="headerlink" title="Transition Layer 怎么构成的, 有什么作用"></a>Transition Layer 怎么构成的, 有什么作用</h3><p>对于不同的 DenseBLock, 其具有的尺寸是不同的, 因此无法直接使用 dense 链接, 为此, 作者利用 1x1 卷积层和 2x2 池化层定义了一个 transition layer, 其主要作用有两个, 一个是降低通道维数, 另一个是对特征图谱进行下采样. Transition Layer 的结构为: <strong>BN + ReLU + 1x1 Conv + 2x2 AvgPooling</strong>.<br>另外，Transition层可以起到压缩模型的作用。假定Transition的上接DenseBlock得到的特征图channels数为 $m$，Transition层可以产生 $\theta m$ 个特征（通过卷积层），其中 $theta$ 是压缩系数（compression rate）。当 $\theta = 1$ 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用 $\theta = 0.5$ 。</p>
<h3 id="深层的网络层输入的特征图谱很大-怎么解决"><a href="#深层的网络层输入的特征图谱很大-怎么解决" class="headerlink" title="深层的网络层输入的特征图谱很大, 怎么解决"></a>深层的网络层输入的特征图谱很大, 怎么解决</h3><p>另外, 由于深层的网络层输入非常大, 因此 DenseBlock 内部会采用 bottleneck 来减少计算量, 主要是在原来的 $3 \times 3$ 卷积层之前添加 $1\times 1$ 的卷积层, 变成 <strong>BN + ReLU + 1x1 Conv + BN + ReLU + 3x3 Conv</strong> 的结构(DenseNet-B), $1\times 1$ 卷积会将 $l\times k$ 的通道数降低成 $4\times k$ 的通道数, 从而提升计算效率.</p>
<p>对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构我们称为DenseNet-BC。</p>
<p><span id="SqueezeNet"></span></p>
<h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a><a href>SqueezeNet</a></h2><h3 id="简述-SqueezeNet-的原理"><a href="#简述-SqueezeNet-的原理" class="headerlink" title="简述 SqueezeNet 的原理"></a><a href="../计算机视觉-SqueezeNet/#简述 SqueezeNet 的原理">简述 SqueezeNet 的原理</a></h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SqueezeNet/fig1.jpg?x-oss-process=style/blog_img" alt="SqueezeNet%2Ffig1.jpg"></div></p>
<p>SqueezeNet 定义了 Fire Modules 作为其基本的组成部件, Fire Modules 由 squeeze layer 和 expand layer 组成, 其中前者是通过 1x1 的卷积层实现的, 后者是通过 1x1 和 3x3 的卷积层实现的(这两个卷积层的输入都是 sequeeze layer, 输出会将二者在通道维度叠加). SqueezeNet 的第一层是传统的 7x7 卷积层, 之后由 8 个 Fire Modules 组成,  最后一层 conv10 是传统的 1x1 卷积层, 最后是由 GAP 和 Softmax 组成的分类层. SqueezeNet 会在 conv1(自身也是下采样), fire4, fire8, conv10 之后添加 max-pooling 层来进行下采样(总步长为 32). 同时, SqueezeNet 受到 ResNet 的启发, 可以在 Fire Modules 3, 5, 7, 9 的输入和输出之间添加 bypass 连接, 令这些模型学习输入输出之间的残差(可以获得更高的精度). 对于通道数不同的其他 Modules, 可以通过 1x1 卷积层来建立 complex bypass).</p>
<h3 id="SqueezeNet-的网络结构"><a href="#SqueezeNet-的网络结构" class="headerlink" title="SqueezeNet 的网络结构"></a>SqueezeNet 的网络结构</h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SqueezeNet/fig2.jpg?x-oss-process=style/blog_img" alt="SqueezeNet%2Ffig2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SqueezeNet/tab1.jpg?x-oss-process=style/blog_img" alt="SqueezeNet%2Ftab1.jpg"></div></p>
<p><span id="MobileNet"></span></p>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a><a href="../计算机视觉-MobileNet">MobileNet</a></h2><h3 id="简述-MobileNet"><a href="#简述-MobileNet" class="headerlink" title="简述 MobileNet"></a><a href="../计算机视觉-MobileNet/#简述 MobileNet 的原理">简述 MobileNet</a></h3><p>MobileNet 的设计原则是要构建出模型 size 小, 并且执行速度快的卷积网络. 它的整体结构框架和 AlexNet 以及 VGGNet 类似, 都是通过不断堆叠卷积层的方式来构建深层的卷积神经网络. 但与传统卷积网络不同的是, MobileNet 除了在第一层使用了标准的卷积层之外, 其余的卷积层都是基于深度可分离卷积构建的. 具体来说, 标准的卷积层在进行操作时, 包括 filter 和 combining 两步, 而深度可分离卷积将这两步分成两个网络层执行, 分别为 Depthwise Convolution 和 Pointwise Convolution</p>
<ul>
<li>Depthwise Conv(用 Group Conv 实现, group_num = in_channels): 对每个 input channel(input depth) 使用一个单独的 filter. 因此输出通道数保持不变, 依然为 $M$</li>
<li>Pointwise Conv(用 1x1 Conv 实现, 每个卷积核深度为 $M$): 利用 1x1 卷积跨通道的融合 depthwise conv 的输出, 并输出指定的通道数 $N$, 从而建立新的特征图谱.</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNets/tab1.jpg?x-oss-process=style/blog_img" alt="MobileNets%2Ftab1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNets/fig2.jpg?x-oss-process=style/blog_img" alt="MobileNets%2Ffig2.jpg"></div></p>
<h3 id="推导-Separable-Conv-的参数量和计算量"><a href="#推导-Separable-Conv-的参数量和计算量" class="headerlink" title="推导 Separable Conv 的参数量和计算量"></a>推导 Separable Conv 的参数量和计算量</h3><p>输入特征图谱为: $D_F\times D_F \times M$<br>输出特征图谱为: $D_F\times D_F \times N = D_G\times D_G \times N$<br>卷积核大小为: $D_K \times D_K$,</p>
<p><strong>参数量:</strong></p>
<ul>
<li>标准卷积: $D_K\times D_K \times M \times N$,</li>
<li>Separable: $D_K\times D_K \times M + M\times N$<ul>
<li>Depthwise: $D_K\times D_K \times M$</li>
<li>Pointwise: $M\times N$</li>
</ul>
</li>
</ul>
<p>于是乎, 在参数量上, 可以节省:</p>
<script type="math/tex; mode=display">\frac{D_K\times D_K \times M + M\times N}{D_K\times D_K \times M \times N} = \frac{1}{N} + \frac{1}{D_K^2}</script><p><strong>卷积计算量(Multiply-Adds):</strong><br>标准卷积: $D_K\times D_K \times M \times N \times D_G \times D_G$</p>
<ul>
<li>Separable: $D_K\times D_K\times M \times D_F \times D_F + M\times N \times D_G \times D_G$<ul>
<li>Depthwise: $D_K\times D_K\times M \times D_G \times D_G$</li>
<li>Pointwise: $M\times N \times D_G \times D_G$</li>
</ul>
</li>
</ul>
<p>于是乎, 在计算量上, 可以节省:</p>
<script type="math/tex; mode=display">\frac{D_K\times D_K\times M \times D_G \times D_G + M\times N \times D_G \times D_G}{D_K\times D_K \times M \times N \times D_G \times D_G} = \frac{1}{N} + \frac{1}{D_K^2}</script><p><strong>关于分组卷积:</strong> 分组卷积的计算量实际上为 $frac{HWMNK^2}{G}$, 其中, H, W 分别为输入图片的尺寸, M 为输入通道, N 为输出通道, K 为卷积核尺寸, G 为分组个数. 因此分组卷积的计算量为普通卷积的 $\frac{1}{G}$. MobileNet 的 Depthwise Conv 实际上就是分组卷积的一种特例, 即 $M=N=G$ 的分组卷积, 所以其计算量为普通卷积的 $frac{1}{G} = \frac{1}{N}$.</p>
<h3 id="MobileNets-为什么快"><a href="#MobileNets-为什么快" class="headerlink" title="MobileNets 为什么快"></a>MobileNets 为什么快</h3><p>第一个原因, MobileNet 将普通卷积替换为 Separable Conv 以后, 其参数量和计算量都大约缩小了 $O(\frac{1}{D_K\times D_K})$ 倍, 如果是 3x3 的卷积, 就是 8~9 倍左右. 虽然计算量小了 8~9 倍, 但实际中并不能加速 8~9 倍, 还要同时考虑带宽等其他因素.</p>
<p>第二个原因, 在 MobileNet 中, 参数量上, Conv 1x1 占比约 75%, FC 约 24%. 而在 FLOPs 上, Conv 1x1 占比约 95%, Conv DW 3x3 约 3%, FC 不到 1%.<br>当卷积层使用 im2col+ GEMM 的方式实现时, $1\times 1$ 的卷积层不需要在内存中重新排序, 因此它的实际执行速度很快.</p>
<p>最后, 原文还给出了两个超参数来进一步压缩模型大小, 分别 width multiplier $\alpha$ 和 resolution multiplier $rho$, 前者用于控制特征图谱的通道数, 后者用于控制输入图片的尺寸.</p>
<p><span id="MobileNetV2"></span></p>
<h2 id="MobileNetV2"><a href="#MobileNetV2" class="headerlink" title="MobileNetV2"></a><a href>MobileNetV2</a></h2><h3 id="MobileNetV2-做了哪些改进"><a href="#MobileNetV2-做了哪些改进" class="headerlink" title="MobileNetV2 做了哪些改进"></a><a href="../计算机视觉-MobileNetV2/#MobileNetV2 做了哪些改进">MobileNetV2 做了哪些改进</a></h3><p>相比于 MobileNetV1, MobileNetV2 的改进主要有两点: Linear Bottleneck 和 Inverted Residual.</p>
<ul>
<li>Linear Bottleneck(用 1x1 conv2d 实现): 在 MobileNetV1 中, Separable Conv 的结构是: 3x3 Conv + BN + ReLU + 1x1 Conv + BN + ReLU. 在 MobileNetV2 中, 先将原来的结果换成了 Bottleneck (conv 1x1 + conv 3x3 + conv 1x1), 同时去掉了最后一个 conv 1x1 的 ReLU, 这是因为 ReLU 会造成低纬度数据的坍塌, 在 channel 少的 feature map 后如果使用 ReLU, 就会破坏 feature map, 造成信息缺失.</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/fig2.jpg?x-oss-process=style/blog_img" alt="MobileNetV2%2Ffig2.jpg"></div></p>
<ul>
<li>Inverted Residual: 添加了 Inverted Residual 加强特征复用和梯度扩多层传播的能力, 将 shortcut 连接位置迁移到 channel 数量较少的层. ResNet Bottleneck 是先降维, 后升维, MobileNetV2 刚好与之相反, 因此称为 Inverted Residual. 这样做的原因其一是因为在内存的使用上更加高效(论述过程比较繁琐, 没细看), 其二是因为在实际实验中效果也比较好.</li>
</ul>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/Inverted.png?x-oss-process=style/blog_img" alt="MobileNetV2%2FInverted.png"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/fig3.jpg?x-oss-process=style/blog_img" alt="MobileNetV2%2Ffig3.jpg"></div></p>
<p>expansion ratio: the ratio between the size of the input bottleneck and the inner size</p>
<p>某一层的 Bottleneck, 其中层与之类似, 可以看出, 先利用 1x1 conv 升维, 再进行 Depthwise Conv, 再利用 1x1 conv 降维, 降维后由于 channel 过少, 因此不使用 ReLU<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">InvertedResidual(</span><br><span class="line">      (conv): Sequential(</span><br><span class="line">        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (2): ReLU6(inplace)</span><br><span class="line">        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)</span><br><span class="line">        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">        (5): ReLU6(inplace)</span><br><span class="line">        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)</span><br><span class="line">        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p>
<h3 id="MobileNetV1-2-中为什么使用-ReLU6-哪些层后面不用-ReLU-激活-为什么"><a href="#MobileNetV1-2-中为什么使用-ReLU6-哪些层后面不用-ReLU-激活-为什么" class="headerlink" title="MobileNetV1/2 中为什么使用 ReLU6? 哪些层后面不用 ReLU 激活? 为什么?"></a>MobileNetV1/2 中为什么使用 ReLU6? 哪些层后面不用 ReLU 激活? 为什么?</h3><p>ReLU6 就是将普通的 ReLU 的最大输出值限制为 6(对输出值做 clip). 这是因为在移动端设备上, 由于资源限制, 使用的数据类型通常为 float16, 如果不对 ReLU 的输出值限制范围, 则其输出值为 0 到正无穷, 而低精度的 float 16 无法很好的精确描述如此大范围的数值, 因此会带来精度损失.</p>
<p>在 Separable Block 中对于 channel 数量较少的 1x1 conv, 使用 Linear Bottleneck 来替换 ReLU. 这么做的原因个人理解是对于 channel 较少的层, 其本身某一维度的张量容易被 ReLU 置为全 0, 张量维度的减小意味着特征描述容量的下降, 从而造成当前层的特征信息损耗.</p>
<h3 id="MobileNetV2-的网络结构及与其他轻量级网络的区别"><a href="#MobileNetV2-的网络结构及与其他轻量级网络的区别" class="headerlink" title="MobileNetV2 的网络结构及与其他轻量级网络的区别"></a>MobileNetV2 的网络结构及与其他轻量级网络的区别</h3><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/tab2.jpg?x-oss-process=style/blog_img" alt="MobileNetV2%2Ftab2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MobileNetV2/fig4.jpg?x-oss-process=style/blog_img" alt="MobileNetV2%2Ffig4.jpg"></div></p>
<p><span id="ShuffleNet"></span></p>
<h2 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a><a href="../计算机视觉-ShuffleNet">ShuffleNet</a></h2><h3 id="简述-ShuffleNet"><a href="#简述-ShuffleNet" class="headerlink" title="简述 ShuffleNet"></a><a href="../计算机视觉-ShuffleNet/#简述 ShuffleNet 的原理">简述 ShuffleNet</a></h3><p>分组卷积具有降低计算量和跨通道信息解耦的优势, 但不论是 Xception 还是 ResNeXt 还是 MobileNet, 他们在进行分组卷积的时候, 都只是对 3x3 的卷积核进行分组卷积. 而对于这些网络结构来说, 占计算量 90% 甚至 95% 的 1x1 卷积却没有进行分组卷积. 其主要原因就是这些网络需要 1x1 卷积来学习跨通道的信息.<br>ShuffleNet 就是在这一点上提出了改进方案, 利用一个简单 Shuffle Channel 操作来实现不同组之间的通道交互, 进而使得可以对 1x1 卷积也是用分组卷积. 如下图所示.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNet/fig1.jpg?x-oss-process=style/blog_img" alt="ShuffleNet%2Ffig1.jpg"></div></p>
<p>Shuffle Channel 的实现十分简单, 只需要一个<code>reshape</code>, <code>transpose</code>和<code>flatten</code>操作即可, 如下所示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">                         111                  123</span><br><span class="line">111222333  -(reshape)-&gt;  222  -(transpose)-&gt;  123  -(flatten)-&gt;  123123123</span><br><span class="line">                         333                  123</span><br></pre></td></tr></table></figure></p>
<p>利用 Shuffle Channel 操作, 作者基于 ResNet 中的 bottleneck 进行改进(所以 ShuffleNet Bottleneck 也是中间小两边大, 和 MobileNetV2 相反). 主要改进方法是先将 bottlenect 中的 3x3 卷积替换成 Xception/MobileNet 中的 Depthwise Conv. 然后将两个 1x1 卷积替换成 Group Conv, 接着对第一个 1x1 卷积的输出图谱执行 Shuffle Channel 操作即可. 第二个 Group Conv(Pointwise) 的目的是恢复通道数以匹配 shortcut 连接, 为了简单起见, 我们没有这里使用额外的 Channel Shuffle 操作, 因为这已经可以产生不错的效果了.<br><strong>注意, 在 Depthwise Conv 之后, 没有使用 ReLU 等激活层, 其原因是因为 Depthwise Conv 输出的通道数较低, 使用 ReLU 会因为低维数据坍塌现象, 即 ReLU 有可能会让某个通道的值全为 0, 从而维度降低, 这些低纬度时会引起信息缺少, 而其他的激活函数又存在梯度消失问题, 所以没有使用激活</strong><br><strong>疑问: 为什么第一层的 1x1 GConv 的输出通道数也很低, 但是却用了 ReLU?</strong> 猜想: ShuffleNet 并没有对 ReLU 激活函数进行大量验证, 它只是采用了 Xception 中的设定, 这一块有待实验验证.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNet/fig2.jpg?x-oss-process=style/blog_img" alt="ShuffleNet%2Ffig2.jpg"></div></p>
<p>上图为 ShuffleUnit 结构. 其中, (b) 为基本的 Shuffle Unit, Add 表示 element-wise add, (c) 为进行下采样时的 Shuffle Unit(步长为2, 图片缩小一半), concat 表示在 depth 维度上叠加, 用于加深深度.</p>
<h3 id="ShuffleNet-的网络结构"><a href="#ShuffleNet-的网络结构" class="headerlink" title="ShuffleNet 的网络结构"></a>ShuffleNet 的网络结构</h3><p>ShuffleNet 的网络结构最开始由 3x3 的卷积层和最大池化层组成, 二者的步长均为2, 也就说这里的 Downsample 总步长为 4. 然后是三个不同的 Stage(3,7,3), 每个 Stage 最开始第一个 building block 的步长为2(stride=2), 用于降低特征图谱的尺寸, 同时会借助旁路的 avg pool 使得 channels 数量翻倍. 最后是由 GAP+FC+Softmax 组成的分类层.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNet/tab1tab2.jpg?x-oss-process=style/blog_img" alt="ShuffleNet%2Ftab1tab2.jpg"></div></p>
<h3 id="ShuffleNet-的计算量推导"><a href="#ShuffleNet-的计算量推导" class="headerlink" title="ShuffleNet 的计算量推导"></a>ShuffleNet 的计算量推导</h3><p>ShuffleNet 最主要的特点是可以在较少的计算资源限制下达到更高的精度和计算效率. 举例来说</p>
<p>Input feature map: $c\times h\times w$<br>bottlenect 3x3 卷积的通道数: $m$</p>
<p>计算量分别为(单位: FLOPs, 文中单位是 FLOPs, 但实际上应该是 Mult-Adds, 也即 MAC, 注意二者区别):</p>
<ul>
<li>ResNet: $hwcm + 9m^2 + hwmc$</li>
<li>ResNeXt: $hwcm + 9m^2/g + hwmc$, $g$ 代表分组数量</li>
<li>ShuffleNet: $hwcm/g + 9m + hwmc/g$, $g$ 代表 1x1 卷积的分组数量, 3x3 是 Depthwise Conv, 分组数等于通道数</li>
</ul>
<p><strong>注意 Xception 和 MobileNet 没有采用 bottleneck 结构, 而是对普通的 Conv 替换成了 Separable Conv(DP Conv), 所以这里不与之比较</strong><br><strong>而 MobileNetV2 采用的 bottleneck 是两边小中间大, 与上面相反, 所以也不做比较</strong></p>
<p>, 当给定输入尺寸为 $c\times h\times w$, 而 bottleneck 的通道数为 $m$ 时, ResNet unit 需要 $hw(2cm + 9m^2)$ FLOPs, ResNeXt 需要 $hw(2cm + 9m^2/g)$ FLOPs(ResNeXt 的 $g\neq m$), 而 ShuffleNet 只需要 $hw(2cm/g + 9m)$ FLOPs(Depthwise 的分组是每个图谱单独一组). 也就说, 当给定计算资源限制后, ShuffleNet 可以使用更大的特征图谱, 这对于小型网络来说非常重要, 因为小型网络通畅没有足够的通道来处理信息.</p>
<p><span id="ShuffleNetV2"></span></p>
<h2 id="ShuffleNetV2"><a href="#ShuffleNetV2" class="headerlink" title="ShuffleNetV2"></a><a href="../计算机视觉-ShuffleNetV2-ECCV2018">ShuffleNetV2</a></h2><h3 id="ShuffleNetV2-做了哪些改进"><a href="#ShuffleNetV2-做了哪些改进" class="headerlink" title="ShuffleNetV2 做了哪些改进"></a>ShuffleNetV2 做了哪些改进</h3><p>作者通过分析影响网络执行速度的原因, 给出了四条十分实用的建议, 分别为:</p>
<ol>
<li><strong>在 Pointwise Conv 中, 当输入输出通道维度相等时，内存访问成本(MAC)最小.</strong> 理论证明如下:<br>假设 Pointwise Conv 的输入通道为 $c_1$, 输出通道为 $c_2$。设 $h$ 和 $w$ 为 feature map 的空间大小，则 1×1 卷积的 FLOPs 为 $B = hwc_1c_2$<br>为了简单起见，本文假设计算设备中的缓存足够大，可以存储整个特征图谱和参数。因此，内存访问成本(MAC)或内存访问操作的数量是 $MAC = hwc_1+ hwc_2+c_1c_2$。这三项分别为: 访问输入图谱, 访问输出图谱, 每次访问输出图谱的一个点时, 需要 c_1 个输入图谱的像素, 总共访问 c_2 个输出图谱的像素.<br>均值不等式，对于非负实数有 $a + b \geq 2\sqrt {ab}$, 并且仅当 $a=b$ 时才取得最小值, 由此本文得到:<script type="math/tex; mode=display">MAC \geq 2\sqrt{hwB} + \frac{B}{hw}</script>推理过程如下:<script type="math/tex; mode=display">MAC = hw(c_1+c_2)+c_1c_2 = hwc_1 + hwc_2 + c_1 c_2 \geq 2\sqrt{hwc_1 hwc_2} + c_1 c_2 = 2\sqrt{hwB} + \frac{B}{hw}</script><strong>因此，MAC有一个由FLOPs给出的下界。当输入通道和输出通道数目相等时，到达下界。</strong> 上面结论只是理论性的。实际上，许多设备上的缓存不够大。但是根据实验表明, 这种理论上的 MAC 同样具有指导意义, 也就是说 <strong>当 $c_1 = c_2$ 时, MAC 最小, 网络推演速度最快.</strong></li>
<li><strong>Group Conv 虽然有助于提高精度, 但是过多的分组会增加 MAC</strong><br>Group Conv 可以降低 FLOPs, 降低幅度为 $frac{1}{g}$, $g$ 为分组数量, 但是, 过多的分组会增加内存访问成本, 以 1x1 Group Conv 的 MAC 和 FLOPs 之间的关系证明:<script type="math/tex; mode=display">MAC = hw(c_1 + c_2) + \frac{c_1 c_2}{g} = hwc_1 + \frac{Bg}{c_1} + \frac{B}{hw}</script>其中 $g$ 为组数，$B = hwc_1c_2 / g$ 为 FLOPs。可以看出，<strong>当给定固定的输入形状 $c_1\times h\times w$ 和计算量 B(根据 $g$ 的值改变自身的 $c_2$ 值, 以保持 B 不变), MAC 会随着 $g$ 的增大而增大。</strong> 实验证明也是如此, 当 group 为 1 时, 网络速度最快.</li>
<li><strong>Building Block 中过多的子模块会降低并行程度.</strong><br>在 Inception 系列和 NAS 结构中, 一个 build block 里面通常会包含需要种不同的卷积模块(例如, NASNET-A 有 13 种卷积模块, 而 ResNet 只有 2,3个). 这种将多个卷积模块组合作为一个 building block 的方式通常可以提高精度, 因为特征表达能力明显增强, 但是它对 GPU 并行设备非常不友好, 即使是并行排列的子模块, 也会大幅降低实际并行速度, 因为它们需要引入额外的开销, 比如 kernel launching 和信息同步(synchronization), 实验证明如下图所示<br><div style="width: 600px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/Afig1.jpg?x-oss-process=style/blog_img" alt="ShuffleNetV2%2FAfig1.jpg"><br>表3的结果显示，多段结构(即使是多段并行)在GPU上显著降低了速度，如4段结构比1段慢3倍。在ARM上，减速相对较小, 因为主要影响的是并行程度, 而 CPU 上主要进行串行计算.<br><div style="width: 600px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/tab3.jpg?x-oss-process=style/blog_img" alt="ShuffleNetV2%2Ftab3.jpg"></div></div></li>
<li><strong>Element-wise 操作也会影响速度</strong><br>在轻量模型 ShuffleNetV1 和 MobileNetV2 中，element-wise 操作占用了相当多的时间(15%, 23%)，尤其是在GPU上。这里的元素操作符主要包括 ReLU、Shortcut(Tensor add)、Add Bias等。<strong>它们的 FLOPs 较小，但 MAC 相对较高</strong>, 也就是说他们具有较高的 MAC/FLOPs ratio。特别地，本文还考虑了深度卷积(depthwise convolution)作为一个 element-wise 运算符的情况，因为它也具有较高的MAC/FLOPs比。</li>
</ol>
<h3 id="ShuffleNetV2-的网络结构"><a href="#ShuffleNetV2-的网络结构" class="headerlink" title="ShuffleNetV2 的网络结构"></a>ShuffleNetV2 的网络结构</h3><p>根据上面四条 Guide, ShuffleNetV2 的设计目标是在保持较大通道数的情况下, 减少密集卷积和分组数量, 为此, 它使用 Channel Split 方法对 ShuffleNetV1 进行了如下改进:</p>
<p><div style="width: 600px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/fig3.jpg?x-oss-process=style/blog_img" alt="ShuffleNetV2%2Ffig3.jpg"></div></p>
<p>上图中 (a), (b) 是 ShuffleNetV1 中的两个核心 building block. 易知, 它同时违反了 G1, G2, G4. 为此, ShuffleNetV2 改进如下:</p>
<ul>
<li>将输入图谱的 Channel 分成两部分(分配比例默认 1:1), 一部分直接通过 Shortcut 传递, 另一部分有 bottleneck 传递;</li>
<li>为了遵循 G1, G2, bottleneck 的三个卷积层的输入输出通道均相等, 同时两端的 1x1 卷积使用普通卷积, 而不是 Group Conv; (Channel Split 本身也具有一定的分组作用)</li>
<li>对于下采样模块, 没有使用 Channel Split, 这样输出通道数刚好可以翻倍, 另外将原先的 3x3 AVG Pool 替换成了 3x3 Depth-wise Conv + 1x1 Conv.</li>
</ul>
<p>将这两个 building-block 反复堆叠，进而构建整个网络, 总体网络结构与ShuffleNet v1相似，如表5所示。只有一个区别: <strong>在全局平均池之前添加一个传统的1×1卷积层来混合特性</strong>，而在ShuffleNet v1中没有。与类似，将每个块中的信道数进行缩放，生成不同复杂度的网络，标记为0.5×、1×等。</p>
<p><div style="width: 600px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/tab5.jpg?x-oss-process=style/blog_img" alt="ShuffleNetV2%2Ftab5.jpg"></div></p>
<p><strong>补充1:</strong></p>
<ul>
<li>concat, channel shuffle, 以及下一层的 channel split 可以 merge 到同一个 element-wise 中去, 减少 element-wise 的冗余;</li>
<li>疑问: ShuffleNetV2 Unit 中的 Depthwise Conv 后面依然没有用 RelU ? 但是个人认为, 当通道数已经相同时, 可以使用 ReLU.</li>
</ul>
<p><strong>补充2:</strong> ShuffleNet 可以和 Residual, SE 联合使用, 如下所示:</p>
<p><div style="width: 600px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/ShuffleNetV2/Afig2.jpg?x-oss-process=style/blog_img" alt="ShuffleNetV2%2FAfig2.jpg"></div></p>
<h3 id="现有的轻量级网络存在哪些问题"><a href="#现有的轻量级网络存在哪些问题" class="headerlink" title="现有的轻量级网络存在哪些问题"></a>现有的轻量级网络存在哪些问题</h3><ul>
<li>ShuffleNetV1:<ul>
<li>使用基于 ResNet bottleneck 的基本结构, point-wise conv 的通道比不为 1, 违反 G1;</li>
<li>使用大量的 Group Conv, 分组过多, 违反 G2;</li>
<li>Shortcut, ReLU, 存在大量的 Element-wise 操作(15%), 违反 G4</li>
</ul>
</li>
<li>MobileNetV1:<ul>
<li>使用 Point-Wise 改变通道维度, 违反 G1;</li>
<li>使用 Gourp Conv, 分组过多, 违反 G2;</li>
</ul>
</li>
<li>MobileNetV2:<ul>
<li>使用 Inverted Residual bottleneck, 通道比不为 1, 违反 G1;</li>
<li>使用大量的 Group Conv, 范围 G2;</li>
<li>在一个很 “厚” 的特征图谱上使用 ReLU, 以及大量的 Shortcut, 使用 Element-wise 操作较多(23%), 违反 G4;</li>
</ul>
</li>
<li>NAS 网络:<ul>
<li>building block 内部存在过多路径, 碎片化程度严重, 违反 G3;</li>
</ul>
</li>
</ul>
<h3 id="简述-ShuffleNetV1-2-和-MobileNetV1-2-的区别"><a href="#简述-ShuffleNetV1-2-和-MobileNetV1-2-的区别" class="headerlink" title="简述 ShuffleNetV1/2 和 MobileNetV1/2 的区别"></a><a href="../计算机视觉-ShuffleNet/简述 ShuffleNet 和 MobileNet 的区别">简述 ShuffleNetV1/2 和 MobileNetV1/2 的区别</a></h3><ul>
<li>Xception: 基于 InceptionV3, 先进行 1x1 Pointwise Conv, 在进行 Group Conv. (由于 building block 是堆叠使用的, 因此相对位置的变化影响并不大), 在 1x1 卷积降维后不使用 ReLU.</li>
<li>MobileNetV1: 将 3x3 Conv 替换成 Depthwise Conv + Pointwise Conv, 正常使用 ReLU6.<br>MobileNetV2: Inverted Residual + Linear Bottleneck, 两边小中间大, 最后的 1x1 卷积不使用 ReLU6.</li>
<li>ShuffleNetV1: 基于 ResNet Bottleneck, 在 Bottleneck 两端的 1x1 卷积上也使用 Group Conv, 为了促进组间通信, 在第一个 1x1 卷积后使用 Channel Shuffle. 两边大中间小, 在中间的 DWConv 不使用 ReLU. 疑问: 为什么输出通道少的第一个 1x1 GConv 依然使用 ReLU?</li>
<li>ShuffleNetV2: 基于 4 条 Guide, 利用 Channel Split 改良 ShuffleNetV1 Unit, 减少 MAC 通信和 FLOPs, 提升模型运行效率</li>
</ul>
<p><span id="SENet"></span></p>
<h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a><a href="../计算机视觉-SENet-CVPR2017">SENet</a></h2><h3 id="简述-SENet-的原理"><a href="#简述-SENet-的原理" class="headerlink" title="简述 SENet 的原理"></a><a href="../计算机视觉-SENet-CVPR2017">简述 SENet 的原理</a></h3><p>SENet 提出了 SEBlock, 该模块可以加入到任意的特征图谱上, 进而可以重新校准各个通道上的特征信息. 该模块分两个步骤执行:</p>
<ol>
<li>Squeeze: 在特征图谱上进行全局平均池化, 得到 channel embedding, 这是最简单的 Squeeze 策略.</li>
<li>Excitation: 利用 FC(r) + ReLU(r) + FC + Sigmoid 的组合普通各个通道之间的依赖关系, 为了能够同时关注多个通道, 使用 Sigmoid 激活(而不是 One-Hot 激活), 这样, 就有可能有多个通道的权重都很高. 上面的 r 代表通道个数的衰减系数, 前两个网络层的通道数为 $\frac{C}{r}$, 后两个为 $C$. 使用两个 FC 的原因主要是为了构成 bottleneck 结构, 减少参数量和计算量.</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig1.jpg?x-oss-process=style/blog_img" alt="SENet%2Ffig1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig2.jpg?x-oss-process=style/blog_img" alt="SENet%2Ffig2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig3.jpg?x-oss-process=style/blog_img" alt="SENet%2Ffig3.jpg"></div></p>
<h3 id="SE-Block-放置的位置是否会严重影响性能"><a href="#SE-Block-放置的位置是否会严重影响性能" class="headerlink" title="SE-Block 放置的位置是否会严重影响性能?"></a>SE-Block 放置的位置是否会严重影响性能?</h3><p>不会, 通常网络是堆叠放置的, 因此改变 SE Block 的绝对位置并不会对其相对位置产生太大的影响, 因此, SE Block 对于各种位置都具有较好的性能, 如下实验所示.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/fig5.jpg?x-oss-process=style/blog_img" alt="SENet%2Ffig5.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/tab14.jpg?x-oss-process=style/blog_img" alt="SENet%2Ftab14.jpg"></div></p>
<h3 id="SENet-的网络结构"><a href="#SENet-的网络结构" class="headerlink" title="SENet 的网络结构"></a>SENet 的网络结构</h3><p>SE Block 可以使用在任意的网络中, 下表是在 ResNet-50 和 ResNeXt-50 中使用的情况, 可以看出, SE BLock 会在每个 bottleneck 中使用</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SENet/tab1.jpg?x-oss-process=style/blog_img" alt="SENet%2Ftab1.jpg"></div></p>
<h3 id="除了-SE-Block-还有哪些其他的-Attention-机制"><a href="#除了-SE-Block-还有哪些其他的-Attention-机制" class="headerlink" title="除了 SE-Block, 还有哪些其他的 Attention 机制"></a>除了 SE-Block, 还有哪些其他的 Attention 机制</h3><h4 id="Convolutional-Block-Attention-Module-CBAM"><a href="#Convolutional-Block-Attention-Module-CBAM" class="headerlink" title="Convolutional Block Attention Module (CBAM)"></a>Convolutional Block Attention Module (CBAM)</h4><p>除了 Channel Attention 之外, 还添加了 Spatial Attention, Spatial Attention 可以告诉神经网络特征图谱上 “where to focus”</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/fig1.jpg?x-oss-process=style/blog_img" alt="CBAM%2Ffig1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/fig2.jpg?x-oss-process=style/blog_img" alt="CBAM%2Ffig2.jpg"></div></p>
<p>$M_c(F)$ 代表 Channel Attention, $M_s(F)$ 代表 Spatial Attention</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/form1.jpg?x-oss-process=style/blog_img" alt="CBAM%2Fform1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/form2.jpg?x-oss-process=style/blog_img" alt="CBAM%2Fform2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/form3.jpg?x-oss-process=style/blog_img" alt="CBAM%2Fform3.jpg"></div></p>
<p>作者建议 CBAM 插入到 backbone 网络的 basic block 部分, 如下图 3 所示.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CBAM/fig3.jpg?x-oss-process=style/blog_img" alt="CBAM%2Ffig3.jpg"></div></p>
<h4 id="Block-Attention-Module-BAM"><a href="#Block-Attention-Module-BAM" class="headerlink" title="Block Attention Module (BAM)"></a>Block Attention Module (BAM)</h4><p>和 CBAM 结构类似, 只不过插入位置不同</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/BAM/fig1.jpg?x-oss-process=style/blog_img" alt="BAM%2Ffig1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/BAM/fig2.jpg?x-oss-process=style/blog_img" alt="BAM%2Ffig2.jpg"></div></p>
<h1 id="目标检测篇"><a href="#目标检测篇" class="headerlink" title="目标检测篇"></a>目标检测篇</h1><p><span id="IoU"></span></p>
<h2 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h2><p>在目标检测中, IoU 是评价生成的预测框好坏的一个很重要的数值, 计算两个边框 IoU 的公式如下所示:</p>
<script type="math/tex; mode=display">x1 = \max (box1_{x1}, box2_{x1}), y1 = \max (box1_{y1}, box2_{y1})</script><script type="math/tex; mode=display">x2 = \min (box1_{x2}, box2_{x2}), y2 = \min (box1_{y2}, box2_{y2})</script><script type="math/tex; mode=display">intersection = (x2 - x1 + 1) \times (y2 - y1 + 1)</script><script type="math/tex; mode=display">IoU = \frac{intersection}{area1+area2-intersection}</script><p>根据上面的公式, 我们可以写出计算预测框和 GT 框之间任意两个框的 IoU 代码(返回一个二维数组,  <code>matrix[i][j]</code>表示第<code>i</code>个预测框和第<code>j</code>个真实框的 IoU)</p>
<p>Numpy 版本<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_inter_matrix</span><span class="params">(boxes_pd, boxes_gt)</span>:</span></span><br><span class="line">    A = boxes_pd.shape[<span class="number">0</span>]</span><br><span class="line">    B = boxes_gt.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    lt_xy = np.maximum( <span class="comment"># left top</span></span><br><span class="line">            np.repeat(np.expand_dims(boxes_pd[:, :<span class="number">2</span>], axis=<span class="number">1</span>), B, axis=<span class="number">1</span>),</span><br><span class="line">            np.repeat(np.expand_dims(boxes_gt[:, :<span class="number">2</span>], axis=<span class="number">0</span>), A, axis=<span class="number">0</span>)</span><br><span class="line">            )</span><br><span class="line">    rb_xy = np.minimum( <span class="comment"># right bottom</span></span><br><span class="line">            np.repeat(np.expand_dims(boxes_pd[:, <span class="number">2</span>:], axis=<span class="number">1</span>), B, axis=<span class="number">1</span>),</span><br><span class="line">            np.repeat(np.expand_dims(boxes_gt[:, <span class="number">2</span>:], axis=<span class="number">0</span>), A, axis=<span class="number">0</span>)</span><br><span class="line">            )</span><br><span class="line">    inter = np.clip(rb_xy - lt_xy, a_min=<span class="number">0</span>, a_max=<span class="keyword">None</span>) <span class="comment"># (A, B, 2): (h, w)</span></span><br><span class="line">    <span class="keyword">return</span> inter[:, :, <span class="number">0</span>] * inter[:, :, <span class="number">1</span>] <span class="comment"># return (A, B), inter_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_iou_matrix</span><span class="params">(boxes_pd, boxes_gt)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        box_pd: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]</span></span><br><span class="line"><span class="string">        box_gt: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A = boxes_pd.shape[<span class="number">0</span>]</span><br><span class="line">    B = boxes_gt.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    area_pd = (boxes_pd[:, <span class="number">3</span>]-boxes_pd[:, <span class="number">1</span>]) * (boxes_pd[:, <span class="number">2</span>]-boxes_pd[:, <span class="number">0</span>]) <span class="comment"># h * w</span></span><br><span class="line">    area_gt = (boxes_gt[:, <span class="number">3</span>]-boxes_gt[:, <span class="number">1</span>]) * (boxes_gt[:, <span class="number">2</span>]-boxes_gt[:, <span class="number">0</span>]) <span class="comment"># h * w</span></span><br><span class="line"></span><br><span class="line">    inter_matrix = get_inter_matrix(boxes_pd, boxes_gt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inter_matrix / (area_gt + area_pd - inter_matrix) <span class="comment"># (A, B), iou_matrix</span></span><br></pre></td></tr></table></figure></p>
<p>PyTorch 版本<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span><span class="params">(box_a, box_b)</span>:</span></span><br><span class="line">    <span class="string">""" We resize both tensors to [A,B,2] without new malloc:</span></span><br><span class="line"><span class="string">    [A,2] -&gt; [A,1,2] -&gt; [A,B,2]</span></span><br><span class="line"><span class="string">    [B,2] -&gt; [1,B,2] -&gt; [A,B,2]</span></span><br><span class="line"><span class="string">    Then we compute the area of intersect between box_a and box_b.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      box_a: (tensor) bounding boxes, Shape: [A,4].</span></span><br><span class="line"><span class="string">      box_b: (tensor) bounding boxes, Shape: [B,4].</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">      (tensor) intersection area, Shape: [A,B].</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A = box_a.size(<span class="number">0</span>)</span><br><span class="line">    B = box_b.size(<span class="number">0</span>)</span><br><span class="line">    max_xy = torch.min(box_a[:, <span class="number">2</span>:].unsqueeze(<span class="number">1</span>).expand(A, B, <span class="number">2</span>),</span><br><span class="line">                       box_b[:, <span class="number">2</span>:].unsqueeze(<span class="number">0</span>).expand(A, B, <span class="number">2</span>))</span><br><span class="line">    print(box_a[:, :<span class="number">2</span>])</span><br><span class="line">    print(box_a[:, :<span class="number">2</span>].unsqueeze(<span class="number">1</span>))</span><br><span class="line">    print(box_a[:, :<span class="number">2</span>].unsqueeze(<span class="number">1</span>).expand(A, B, <span class="number">2</span>))</span><br><span class="line">    min_xy = torch.max(box_a[:, :<span class="number">2</span>].unsqueeze(<span class="number">1</span>).expand(A, B, <span class="number">2</span>),</span><br><span class="line">                       box_b[:, :<span class="number">2</span>].unsqueeze(<span class="number">0</span>).expand(A, B, <span class="number">2</span>))</span><br><span class="line">    inter = torch.clamp((max_xy - min_xy), min=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> inter[:, :, <span class="number">0</span>] * inter[:, :, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">jaccard</span><span class="params">(box_a, box_b)</span>:</span></span><br><span class="line">    <span class="string">"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap</span></span><br><span class="line"><span class="string">    is simply the intersection over union of two boxes.  Here we operate on</span></span><br><span class="line"><span class="string">    ground truth boxes and default boxes.</span></span><br><span class="line"><span class="string">    E.g.:</span></span><br><span class="line"><span class="string">        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]</span></span><br><span class="line"><span class="string">        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    inter = intersect(box_a, box_b)</span><br><span class="line">    area_a = ((box_a[:, <span class="number">2</span>]-box_a[:, <span class="number">0</span>]) *</span><br><span class="line">              (box_a[:, <span class="number">3</span>]-box_a[:, <span class="number">1</span>])).unsqueeze(<span class="number">1</span>).expand_as(inter)  <span class="comment"># [A,B]</span></span><br><span class="line">    area_b = ((box_b[:, <span class="number">2</span>]-box_b[:, <span class="number">0</span>]) *</span><br><span class="line">              (box_b[:, <span class="number">3</span>]-box_b[:, <span class="number">1</span>])).unsqueeze(<span class="number">0</span>).expand_as(inter)  <span class="comment"># [A,B]</span></span><br><span class="line">    union = area_a + area_b - inter</span><br><span class="line">    <span class="keyword">return</span> inter / union  <span class="comment"># [A,B]</span></span><br><span class="line"></span><br><span class="line">box_a = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br><span class="line">box_b = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br><span class="line">iou = jaccard(box_a, box_b)</span><br><span class="line">print(iou)</span><br></pre></td></tr></table></figure></p>
<p><span id="mAP"></span></p>
<h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><p>TP: $IoU &gt; 0.5$ 的检测框数量(同一个 GT 只计算一次)<br>FP: $IoU \leq 0.5$ 的检测框数量, 或者检测到同一个 GT 的多余检测框的数量<br>FN: 没有被检测到的 GT 的数量</p>
<p>准确率(Acc):</p>
<script type="math/tex; mode=display">Acc = \frac{TP + TN}{FP + FN + TP + TN}</script><p>精确度(Pre):</p>
<script type="math/tex; mode=display">Acc = \frac{TP}{FP + TP}</script><p>在目标检测中, 由于 <strong>不存在</strong> 预测正确的负样本这种说法, 因此, 我们在目标检测中有时候也默认准确率就是精确度.</p>
<p>召回率(Recall):</p>
<script type="math/tex; mode=display">Recall = \frac{TP}{TP+FN}</script><p><strong>对于 Pre 和 Recall 来说, 一般都是针对某一类的, 在不说明类的情况下, 则认为是二分类问题, 即只有正负样本.</strong></p>
<p><strong>AP: Average Precision</strong></p>
<p>以 Recall 为横轴, Precision 为纵轴, 可以画出一条 PR 曲线, PR 曲线下的面积就定义为 AP. 由于积分计算相对困难, 因此通常用插值法计算.</p>
<p><strong>AP 计算方式:</strong> $\sum_{k=1}^{N}\max_{k’ \geq k} P(k’) \Delta r(k)$</p>
<ul>
<li>VOC2010 以前: 只需要选取当Recall &gt;= 0, 0.1, 0.2, …, 1共 11 个点时的 Precision 最大值，然后 AP 就是这 11 个 Precision 的平均值</li>
<li>VOC2010 以后: 针对样本产生的每一个不同的 Recall 值（包括0和1），选取其大于等于这些 Recall 值时对应的 Precision 最大值，然后计算 PR 曲线下面积作为 AP 值。</li>
</ul>
<p><strong>mAP: mean Average Precision</strong> 指的是分别求每个类别的 AP, 然后取其平均值.</p>
<h3 id="AP-计算示例"><a href="#AP-计算示例" class="headerlink" title="AP 计算示例"></a>AP 计算示例</h3><p>假设, 对于 Aeroplane 类别, 我们网络有以下输出 (BB 表示 BBox 序号, IoU &gt; 0.5 时, GT = 1)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">BB  | confidence | GT</span><br><span class="line">----------------------</span><br><span class="line">BB1 |  0.9       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB2 |  0.9       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB1 |  0.8       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB3 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB4 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB5 |  0.7       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB6 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB7 |  0.7       | 0</span><br><span class="line">----------------------</span><br><span class="line">BB8 |  0.7       | 1</span><br><span class="line">----------------------</span><br><span class="line">BB9 |  0.7       | 1</span><br><span class="line">----------------------</span><br></pre></td></tr></table></figure>
<p>因此，我们有 TP=5 (BB1, BB2, BB5, BB8, BB9), FP=5 (重复检测到的BB1也算FP)。除了表里检测到的5个GT以外，我们还有2个GT没被检测到，因此: FN = 2. 这时我们就可以按照Confidence的顺序给出各处的PR值，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">rank=1  precision=1.00 and recall=0.14</span><br><span class="line">----------</span><br><span class="line">rank=2  precision=1.00 and recall=0.29</span><br><span class="line">----------</span><br><span class="line">rank=3  precision=0.66 and recall=0.29</span><br><span class="line">----------</span><br><span class="line">rank=4  precision=0.50 and recall=0.29</span><br><span class="line">----------</span><br><span class="line">rank=5  precision=0.40 and recall=0.29</span><br><span class="line">----------</span><br><span class="line">rank=6  precision=0.50 and recall=0.43</span><br><span class="line">----------</span><br><span class="line">rank=7  precision=0.43 and recall=0.43</span><br><span class="line">----------</span><br><span class="line">rank=8  precision=0.38 and recall=0.43</span><br><span class="line">----------</span><br><span class="line">rank=9  precision=0.44 and recall=0.57</span><br><span class="line">----------</span><br><span class="line">rank=10 precision=0.50 and recall=0.71</span><br><span class="line">----------</span><br></pre></td></tr></table></figure></p>
<p>因此, 根据不同的计算方法, 其对应的 AP 分别为:</p>
<ul>
<li>VOC2010以前, 选取Recall &gt;= 0, 0.1, …, 1的11处Percision的最大值: $(1 + 1 + 1 + 0.5 + 0.5 + 0.5 + 0.5 + 0.5 + 0 + 0 + 0) / 11 = 5.5 / 11 = 0.5$</li>
<li>VOC2010以后, $(0.14-0)\cdot 1 + (0.29-0.14)\cdot 1 + (0.43 - 0.29) \cdot 0.5 + (0.57 - 0.43) \cdot 0.5 + (0.71 - 0.57) \cdot 0.5 + (1 - 0.71) \cdot 0 = 0.5$</li>
</ul>
<h3 id="mAP-计算代码实现"><a href="#mAP-计算代码实现" class="headerlink" title="mAP 计算代码实现"></a>mAP 计算代码实现</h3><p>下面是 AP 的代码实现, mAP 只需分别对不同类别执行下述计算即可</p>
<p>固定 Recall 区间 (0, 0.1, …, 1 总共 11 个 点)<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision</span><span class="params">(precision, recall)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> pre <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> rec <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> np.nan</span><br><span class="line">    psum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> np.arange(<span class="number">0.</span>, <span class="number">1.1</span>, <span class="number">0.1</span>):</span><br><span class="line">        <span class="keyword">if</span> np.sum(recall &gt;= t) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span> <span class="comment"># 如果后面已经不存在大于阈值的 recall 了, 则可跳出</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 找到大于阈值的所有对应精度中, 最大的那个</span></span><br><span class="line">            psum += np.max(np.nan_to_num(precision)[recall &gt;= t])</span><br><span class="line">    <span class="keyword">return</span> psum / <span class="number">11.0</span> <span class="comment"># 除以采样点个数(11个)</span></span><br></pre></td></tr></table></figure></p>
<p>样本 Recall 区间 (对于每一个不同的 recall 都进行计算)<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average_precision_rec</span><span class="params">(pre, rec)</span>:</span></span><br><span class="line">    pre = np.nan_to_num(pre)</span><br><span class="line">    pre = np.concatenate([[<span class="number">0</span>], pre, [<span class="number">0</span>]]) <span class="comment"># 添加哨兵元素</span></span><br><span class="line">    rec = np.concatenate([[<span class="number">0</span>], rec, [<span class="number">1</span>]]) <span class="comment"># 添加哨兵元素</span></span><br><span class="line"></span><br><span class="line">    n = len(pre)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>): <span class="comment"># 将pre置为后面最大的</span></span><br><span class="line">        pre[i<span class="number">-1</span>] = max(pre[i<span class="number">-1</span>], pre[i])</span><br><span class="line"></span><br><span class="line">    i = np.where(rec[<span class="number">1</span>:] != rec[:<span class="number">-1</span>])[<span class="number">0</span>] <span class="comment"># i = 0 ~ n-2, 找到 rec 变化的点</span></span><br><span class="line">    print((rec[i+<span class="number">1</span>] - rec[i]), pre[i+<span class="number">1</span>]) <span class="comment"># 输出相乘元素, 方便检查</span></span><br><span class="line">    <span class="keyword">return</span> np.sum((rec[i+<span class="number">1</span>] - rec[i]) * pre[i+<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p>
<p>调用上方代码<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pre = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0.66</span>, <span class="number">0.5</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.43</span>, <span class="number">0.38</span>, <span class="number">0.44</span>, <span class="number">0.5</span>])</span><br><span class="line">rec = np.array([<span class="number">0.14</span>, <span class="number">0.29</span>, <span class="number">0.29</span>, <span class="number">0.29</span>, <span class="number">0.29</span>, <span class="number">0.43</span>, <span class="number">0.43</span>, <span class="number">0.43</span>, <span class="number">0.57</span>, <span class="number">0.71</span>])</span><br><span class="line"></span><br><span class="line">print(average_precision(pre, rec))</span><br><span class="line">print(average_precision_rec(pre, rec))</span><br></pre></td></tr></table></figure></p>
<p><span id="mmAP"></span></p>
<h2 id="mmAP"><a href="#mmAP" class="headerlink" title="mmAP"></a>mmAP</h2><p>给定一组IOU阈值，在每个IOU阈值下面，求所有类别的AP，并将其平均起来，作为这个IOU阈值下的检测性能，称为mAP(比如mAP@0.5就表示IOU阈值为0.5时的mAP)；最后，将所有IOU阈值下的mAP进行平均，就得到了最终的性能评价指标：mmAP。</p>
<p>实际计算时，mmAP并不会把所有检测结果都考虑进来，因为那样总是可以达到Recall=100%，为了更有效地评价检测结果，mmAP只会考虑每张图片的前100个结果。这个数字应该随着数据集变化而改变，比如如果是密集场景数据集，这个数字应该提高。</p>
<p><span id="NMS"></span></p>
<h2 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h2><h3 id="简述-NMS-的原理"><a href="#简述-NMS-的原理" class="headerlink" title="简述 NMS 的原理"></a><a href="../计算机视觉-NMS-Implementation/#简述 NMS 的原理">简述 NMS 的原理</a></h3><p>非极大值抑制(Non-Maximum Suppression, NMS), 顾名思义就是抑制那些不是极大值的元素, 可以理解为局部最大值搜索. 对于目标检测来说, 非极大值抑制的含义就是对于重叠度较高的一部分同类候选框来说, 去掉那些置信度较低的框, 只保留置信度最大的那一个进行后面的流程, 这里的重叠度高低与否是通过 IoU 的大小来判断的.</p>
<p><span id="NMS 算法源码实现"></span></p>
<h3 id="NMS-算法源码实现"><a href="#NMS-算法源码实现" class="headerlink" title="NMS 算法源码实现"></a><a href="../计算机视觉-NMS-Implementation/#NMS 算法源码实现">NMS 算法源码实现</a></h3><p><strong>算法逻辑:</strong><br>输入: $n$ 行 $4$ 列的候选框数组, 以及对应的 $n$ 行 $1$ 列的置信度数组.<br>输出: $m$ 行 $4$ 列的候选框数组, 以及对应的 $m$ 行 $1$ 列的置信度数组, $m$ 对应的是去重后的候选框数量<br>算法流程:</p>
<ol>
<li>计算 $n$ 个候选框的面积大小</li>
<li>对置信度进行排序, 获取排序后的下标序号, 即采用<code>argsort</code></li>
<li>将当前置信度最大的框加入返回值列表中</li>
<li>获取当前置信度最大的候选框与其他任意候选框的相交面积</li>
<li>利用相交的面积和两个框自身的面积计算框的交并比, 将交并比大于阈值的框删除.</li>
<li>对剩余的框重复以上过程</li>
</ol>
<p><strong>Python 实现:</strong><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span><span class="params">(bounding_boxes, confidence_score, threshold)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(bounding_boxes) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [], []</span><br><span class="line">    bboxes = np.array(bounding_boxes)</span><br><span class="line">    score = np.array(confidence_score)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 n 个候选框的面积大小</span></span><br><span class="line">    x1 = bboxes[:, <span class="number">0</span>]</span><br><span class="line">    y1 = bboxes[:, <span class="number">1</span>]</span><br><span class="line">    x2 = bboxes[:, <span class="number">2</span>]</span><br><span class="line">    y2 = bboxes[:, <span class="number">3</span>]</span><br><span class="line">    areas =(x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对置信度进行排序, 获取排序后的下标序号, argsort 默认从小到大排序</span></span><br><span class="line">    order = np.argsort(score)</span><br><span class="line"></span><br><span class="line">    picked_boxes = [] <span class="comment"># 返回值</span></span><br><span class="line">    picked_score = [] <span class="comment"># 返回值</span></span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 将当前置信度最大的框加入返回值列表中</span></span><br><span class="line">        index = order[<span class="number">-1</span>]</span><br><span class="line">        picked_boxes.append(bounding_boxes[index])</span><br><span class="line">        picked_score.append(confidence_score[index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取当前置信度最大的候选框与其他任意候选框的相交面积</span></span><br><span class="line">        x11 = np.maximum(x1[index], x1[order[:<span class="number">-1</span>]])</span><br><span class="line">        y11 = np.maximum(y1[index], y1[order[:<span class="number">-1</span>]])</span><br><span class="line">        x22 = np.minimum(x2[index], x2[order[:<span class="number">-1</span>]])</span><br><span class="line">        y22 = np.minimum(y2[index], y2[order[:<span class="number">-1</span>]])</span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, x22 - x11 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, y22 - y11 + <span class="number">1</span>)</span><br><span class="line">        intersection = w * h</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用相交的面积和两个框自身的面积计算框的交并比, 将交并比大于阈值的框删除</span></span><br><span class="line">        ratio = intersection / (areas[index] + areas[order[:<span class="number">-1</span>]] - intersection)</span><br><span class="line">        left = np.where(ratio &lt; threshold)</span><br><span class="line">        order = order[left]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> picked_boxes, picked_score</span><br></pre></td></tr></table></figure></p>
<p><strong>C++ 实现</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Bbox</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> x1;</span><br><span class="line">    <span class="keyword">int</span> y1;</span><br><span class="line">    <span class="keyword">int</span> x2;</span><br><span class="line">    <span class="keyword">int</span> y2;</span><br><span class="line">    <span class="keyword">float</span> score;</span><br><span class="line">    Bbox(<span class="keyword">int</span> x1_, <span class="keyword">int</span> y1_, <span class="keyword">int</span> x2_, <span class="keyword">int</span> y2_, <span class="keyword">float</span> s):</span><br><span class="line">	x1(x1_), y1(y1_), x2(x2_), y2(y2_), score(s) &#123;&#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">iou</span><span class="params">(Bbox box1, Bbox box2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> area1 = (box1.x2 - box1.x1 + <span class="number">1</span>) * (box1.y2 - box1.y1 + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">float</span> area2 = (box2.x2 - box2.x1 + <span class="number">1</span>) * (box2.y2 - box2.y1 + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> x11 = <span class="built_in">std</span>::max(box1.x1, box2.x1);</span><br><span class="line">    <span class="keyword">int</span> y11 = <span class="built_in">std</span>::max(box1.y1, box2.y1);</span><br><span class="line">    <span class="keyword">int</span> x22 = <span class="built_in">std</span>::min(box1.x2, box2.x2);</span><br><span class="line">    <span class="keyword">int</span> y22 = <span class="built_in">std</span>::min(box1.y2, box2.y2);</span><br><span class="line">    <span class="keyword">float</span> intersection = (x22 - x11 + <span class="number">1</span>) * (y22 - y11 + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> intersection / (area1 + area2 - intersection);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Bbox&gt; nms(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Bbox&gt; &amp;vecBbox, <span class="keyword">float</span> threshold) &#123;</span><br><span class="line">    <span class="keyword">auto</span> cmpScore = [](Bbox box1, Bbox box2) &#123;</span><br><span class="line">	<span class="keyword">return</span> box1.score &lt; box2.score; <span class="comment">// 升序排列, 令score最大的box在vector末端</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">std</span>::sort(vecBbox.begin(), vecBbox.end(), cmpScore);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Bbox&gt; pickedBbox;</span><br><span class="line">    <span class="keyword">while</span> (vecBbox.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        pickedBbox.emplace_back(vecBbox.back());</span><br><span class="line">        vecBbox.pop_back();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; vecBbox.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (iou(pickedBbox.back(), vecBbox[i]) &gt;= threshold) &#123;</span><br><span class="line">                vecBbox.erase(vecBbox.begin() + i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pickedBbox;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>CUDA 实现:</strong><br>待补充</p>
<h3 id="Soft-NMS-简介"><a href="#Soft-NMS-简介" class="headerlink" title="Soft-NMS 简介"></a><a href="../计算机视觉-NMS-Implementation/#简述 Soft-NMS 的原理及算法实现">Soft-NMS 简介</a></h3><p>在 Soft-NMS 中, 对于那些重叠度大于一定阈值的 box, 我们并不将其删除, 而仅仅只是根据重叠程度来降低那些 box 的 socre, 这样一来, 这些 box 仍旧处于 box 列表中, 只是 socre 的值变低了. 具体来说, 如果 box 的重叠程度高, 那么 score 的值就会变得很低, 如果重叠程度小, 那么 box 的 score 值就只会降低一点, Soft-NMS 算法伪代码如下图所示:</p>
<h3 id="Soft-NMS-算法源码实现"><a href="#Soft-NMS-算法源码实现" class="headerlink" title="Soft-NMS 算法源码实现"></a><a href="../计算机视觉-NMS-Implementation/#Soft-NMS 算法源码实现">Soft-NMS 算法源码实现</a></h3><p><strong>算法逻辑:</strong><br>输入:</p>
<ul>
<li>bboxes: 坐标矩阵, 每个边框表示为 [x1, y1, x2, y2]</li>
<li>scores: 每个 box 对应的分数, 在 Soft-NMS 中, scores 会发生变化(<strong>对外部变量也有影响</strong>)</li>
<li>iou_thresh:   交并比的最低阈值</li>
<li>sigma2: 使用 gaussian 函数的方差, sigma2 代表 $\sigma^2$</li>
<li>score_thresh: 最终分数的最低阈值</li>
<li>method: 使用的惩罚方法, 1 代表线性惩罚, 2 代表高斯惩罚, 其他情况代表默认的 NMS</li>
</ul>
<p>返回值: 最终留下的 boxes 的 index, 同时, scores 值也已经被改变.<br>算法流程:</p>
<ol>
<li>在 bboxes 之后添加对于的下标[0, 1, 2…], 最终 bboxes 的 shape 为 [n, 5], 前四个为坐标, 后一个为下标</li>
<li>计算每个 box 自身的面积</li>
<li><strong>对于每一个下标 $i$</strong>, 找出 i 后面的最大 score 及其下标, 如果当前 i 的得分小于后面的最大 score, 则与之交换, 确保 i 上的 score 最大.</li>
<li>计算 IoU</li>
<li>根据用户选定的方法更新 scores 的值</li>
<li>以上过程循环 $N$ 次后($N$ 为总边框的数量), 将最终得分大于最低阈值的下标返回, 根据下标获取最终存留的 Boxes, <strong>注意, 此时, 外部 scores 的值已经完成更新, 无需借助下标来获取.</strong></li>
</ol>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soft_nms</span><span class="params">(bboxes, scores, iou_thresh=<span class="number">0.3</span>, sigma2=<span class="number">0.5</span>, score_thresh=<span class="number">0.001</span>, method=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 在 bboxes 之后添加对于的下标[0, 1, 2...], 最终 bboxes 的 shape 为 [n, 5], 前四个为坐标, 后一个为下标</span></span><br><span class="line">    N = bboxes.shape[<span class="number">0</span>] <span class="comment"># 总的 box 的数量</span></span><br><span class="line">    indexes = np.array([np.arange(N)])  <span class="comment"># 下标: 0, 1, 2, ..., n-1</span></span><br><span class="line">    bboxes = np.concatenate((bboxes, indexes.T), axis=<span class="number">1</span>) <span class="comment"># concatenate 之后, bboxes 的操作不会对外部变量产生影响</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每个 box 的面积</span></span><br><span class="line">    x1 = bboxes[:, <span class="number">0</span>]</span><br><span class="line">    y1 = bboxes[:, <span class="number">1</span>]</span><br><span class="line">    x2 = bboxes[:, <span class="number">2</span>]</span><br><span class="line">    y2 = bboxes[:, <span class="number">3</span>]</span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="comment"># 找出 i 后面的最大 score 及其下标</span></span><br><span class="line">        pos = i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i != N<span class="number">-1</span>:</span><br><span class="line">            maxscore = np.max(scores[pos:], axis=<span class="number">0</span>)</span><br><span class="line">            maxpos = np.argmax(scores[pos:], axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            maxscore = scores[<span class="number">-1</span>]</span><br><span class="line">            maxpos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果当前 i 的得分小于后面的最大 score, 则与之交换, 确保 i 上的 score 最大</span></span><br><span class="line">        <span class="keyword">if</span> scores[i] &lt; maxscore:</span><br><span class="line">            bboxes[[i, maxpos + i + <span class="number">1</span>]] = bboxes[[maxpos + i + <span class="number">1</span>, i]]</span><br><span class="line">            scores[[i, maxpos + i + <span class="number">1</span>]] = scores[[maxpos + i + <span class="number">1</span>, i]]</span><br><span class="line">            areas[[i, maxpos + i + <span class="number">1</span>]] = areas[[maxpos + i + <span class="number">1</span>, i]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># IoU calculate</span></span><br><span class="line">        xx1 = np.maximum(bboxes[i, <span class="number">0</span>], bboxes[pos:, <span class="number">0</span>])</span><br><span class="line">        yy1 = np.maximum(bboxes[i, <span class="number">1</span>], bboxes[pos:, <span class="number">1</span>])</span><br><span class="line">        xx2 = np.minimum(bboxes[i, <span class="number">2</span>], bboxes[pos:, <span class="number">2</span>])</span><br><span class="line">        yy2 = np.minimum(bboxes[i, <span class="number">3</span>], bboxes[pos:, <span class="number">3</span>])</span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line">        intersection = w * h</span><br><span class="line">        iou = intersection / (areas[i] + areas[pos:] - intersection)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Three methods: 1.linear 2.gaussian 3.original NMS</span></span><br><span class="line">        <span class="keyword">if</span> method == <span class="number">1</span>:  <span class="comment"># linear</span></span><br><span class="line">            weight = np.ones(iou.shape)</span><br><span class="line">            weight[iou &gt; iou_thresh] = weight[iou &gt; iou_thresh] - iou[iou &gt; iou_thresh]</span><br><span class="line">        <span class="keyword">elif</span> method == <span class="number">2</span>:  <span class="comment"># gaussian</span></span><br><span class="line">            weight = np.exp(-(iou * iou) / sigma2)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># original NMS</span></span><br><span class="line">            weight = np.ones(iou.shape)</span><br><span class="line">            weight[iou &gt; iou_thresh] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        scores[pos:] = weight * scores[pos:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># select the boxes and keep the corresponding indexes</span></span><br><span class="line">    inds = bboxes[:, <span class="number">4</span>][scores &gt; score_thresh]</span><br><span class="line">    keep = inds.astype(int)</span><br><span class="line">    <span class="keyword">return</span> keep</span><br><span class="line"></span><br><span class="line"><span class="comment"># boxes and scores</span></span><br><span class="line">boxes = np.array([[<span class="number">200</span>, <span class="number">200</span>, <span class="number">400</span>, <span class="number">400</span>], [<span class="number">220</span>, <span class="number">220</span>, <span class="number">420</span>, <span class="number">420</span>],</span><br><span class="line">                  [<span class="number">240</span>, <span class="number">200</span>, <span class="number">440</span>, <span class="number">400</span>], [<span class="number">200</span>, <span class="number">240</span>, <span class="number">400</span>, <span class="number">440</span>],</span><br><span class="line">                  [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]], dtype=np.float32)</span><br><span class="line">boxscores = np.array([<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.5</span>], dtype=np.float32)</span><br><span class="line">index = soft_nms(boxes, boxscores, method=<span class="number">2</span>)</span><br><span class="line">print(index) <span class="comment"># 按照 scores 的排序指明了对应的 box 的下标</span></span><br><span class="line">print(boxes[index])</span><br><span class="line">print(boxscores) <span class="comment"># 注意, scores 不需要用 index 获取, scores 已经是更新过的排序 scores</span></span><br></pre></td></tr></table></figure>
<h3 id="其他的-NMS-算法"><a href="#其他的-NMS-算法" class="headerlink" title="其他的 NMS 算法"></a><a href="../计算机视觉-NMS-Implementation/#介绍一下其他的 NMS 算法">其他的 NMS 算法</a></h3><p>待补充</p>
<p><span id="R-CNN"></span></p>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><a href="../计算机视觉-R-CNN-CVPR2014">R-CNN</a></h2><h3 id="R-CNN-简介"><a href="#R-CNN-简介" class="headerlink" title="R-CNN 简介"></a>R-CNN 简介</h3><ol>
<li>Selective Search 提取出候选区域框;</li>
<li>根据候选区域框与真实框的交并比决定正负样本标签(此时不关心框内物体的类别);</li>
<li>送入到 AlexNet 中提取 CNN 特征</li>
<li>将提取到的特征送入到 SVM 分类器中进行分类, 每一个类别都单独训练了一个 SVM 分类器;</li>
<li>对每一个框进行边框回归, 学习特征图谱候选区域框到真实框的转换, 调整框的位置.</li>
</ol>
<p><strong>Feature Extraction</strong>: AlexNet (5层卷积, 2层FC, 最终特征向量的维度为 4096).</p>
<p><strong>输入图片大小</strong>: $227\times 227$.</p>
<p><strong>正负样本划分</strong>: 与 gt-box 的 IoU 大于 0.5 的认为是正样本, 反之认为是负样本. 训练时, mini-batch 中正样本采样数为32(over all classes), 负样本的采样数为 96. 负样本数量多是因为在真实情况下, 背景的区域数量远大于物体数量.</p>
<p><strong>分类器</strong>: 为每个类别训练了一个 SVM.</p>
<h3 id="Selective-Search-简介"><a href="#Selective-Search-简介" class="headerlink" title="Selective Search 简介"></a>Selective Search 简介</h3><p>首先, 首先利用分割算法(Graph-Based Image Segmentation, 2004, IJCV, 贪心)得到一些初始化的区域, 然后计算每个相邻区域的相似性, 相似性的计算依赖于颜色相似性和纹理相似性, 同时给较小的区域赋予更多的权重, 也就是优先合并小区域(否则大区域有可能会不断吞并周围区域, 使得多尺度之应用了在局部区域, 而不是在每个位置都具有多尺度), 接着找出相似性最大的区域, 将它们合并, 并计算新合并的区域与其他相邻区域的相似性, 重复这个过程, 直到所有的区域被合并完为止.</p>
<h3 id="为什么要-R-CNN-使用-SVM-而不用更加方便的-Softmax-分类器"><a href="#为什么要-R-CNN-使用-SVM-而不用更加方便的-Softmax-分类器" class="headerlink" title="为什么要 R-CNN 使用 SVM 而不用更加方便的 Softmax 分类器"></a>为什么要 R-CNN 使用 SVM 而不用更加方便的 Softmax 分类器</h3><ul>
<li>作者尝试过但是 mAP 从 54.2% 降到了 50.9%</li>
<li>下降的原因是多因素造成的, 比如对正负样本的定义, 再比如在训练 Softmax 时使用的负样本是随机采样的, 而训练 SVM 时的负样本更像是 “hard negatives” 的子集, 导致训练精度更高等等.</li>
<li>后续的 Fast RCNN 使用 Softmax 也达到了和 SVM 差不多的准确率, 训练过程更加简单.</li>
</ul>
<p><span id="Bounding Box 的回归方式简介"></span></p>
<h3 id="Bounding-Box-的回归方式简介"><a href="#Bounding-Box-的回归方式简介" class="headerlink" title="Bounding Box 的回归方式简介"></a>Bounding Box 的回归方式简介</h3><p>在 R-CNN 的边框回归中, 我们不是直接学习真实框的坐标, 而是学习从 Proposals 到 真实框的一个偏移变换函数, 具体来说, 对于中心点, 需要学习的是 proposal 和 真实框相对位移, 这个位移会用 proposal 的宽和高进行归一化, 对于宽和高, 需要学习的是真实框相对于 proposal 的 log 缩放度.</p>
<script type="math/tex; mode=display">t_x = (G_x - P_x) / P_w</script><script type="math/tex; mode=display">t_y = (G_y - P_y) / P_h</script><script type="math/tex; mode=display">t_w = log(G_w / P_w)</script><script type="math/tex; mode=display">t_h = log(G_h / P_h)</script><p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/bbox_regression1.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fbbox_regression1.jpg"></div></p>
<p><span id="Bounding box 回归的时候, 为什么不直接对坐标回归, 而是采用偏移量和缩放度"></span></p>
<h3 id="Bounding-box-回归的时候-为什么不直接对坐标回归-而是采用偏移量和缩放度"><a href="#Bounding-box-回归的时候-为什么不直接对坐标回归-而是采用偏移量和缩放度" class="headerlink" title="Bounding box 回归的时候, 为什么不直接对坐标回归, 而是采用偏移量和缩放度"></a>Bounding box 回归的时候, 为什么不直接对坐标回归, 而是采用偏移量和缩放度</h3><p>为了获得对物体回归过程的尺度不变性</p>
<ul>
<li>尺度不变性: 对于不同尺度下的同一个物体, 如果不使用归一化, 那么对于固定的偏移量(像素值), 大物体只会挪动一点, 小物体会挪动很多. 但是由于这是同一物体, 我们得到的特征应该是相似的, 因此这样不合理. 如果使用归一化的偏移量, 那么其偏移程度就与物体尺寸无关, 故而具有尺寸不变性.</li>
</ul>
<p>平移不变性: 直接对坐标回归, 则回归过程不具有平移不变性, 即对于相同的 GT 和 Prior, 改变位置时, 回归的值应该是不变的, 因此需要对相对偏移量进行回归, 而不是对绝对坐标回归.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/bbox_regression2.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fbbox_regression2.jpg"></div></p>
<p>边框回归(BoundingBoxRegression)详解<br><a href="http://caffecn.cn/?/question/160" target="_blank" rel="noopener">http://caffecn.cn/?/question/160</a><br><a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="noopener">https://blog.csdn.net/zijin0802034/article/details/77685438</a></p>
<h3 id="为什么当-Region-Proposals-和-Ground-Truth-较接近时-即-IoU-较大时-可以认为是边框回归函数是线性变换"><a href="#为什么当-Region-Proposals-和-Ground-Truth-较接近时-即-IoU-较大时-可以认为是边框回归函数是线性变换" class="headerlink" title="为什么当 Region Proposals 和 Ground Truth 较接近时, 即 IoU 较大时, 可以认为是边框回归函数是线性变换?"></a>为什么当 Region Proposals 和 Ground Truth 较接近时, 即 IoU 较大时, 可以认为是边框回归函数是线性变换?</h3><p>当输入的 Proposal 与 Ground Truth 相差较小时(RCNN 设置的是 IoU&gt;0.6)， 可以认为这种变换是一种线性变换， 那么我们就可以用线性回归来建模对窗口进行微调， 否则会导致训练的回归模型不 work (当 Proposal跟 GT 离得较远，就是复杂的非线性问题了，此时用线性回归建模显然不合理). 对于这一段的话解释如下:</p>
<p>首先, Log 函数肯定不满足线性函数的定义, 但是根据极限的相关定义, 我们如下面的等式成立:</p>
<script type="math/tex; mode=display">lim_{x\rightarrow0}log(1+x) = x</script><p>根据上面的公式, 我们可以对公式 $t_w$ 作如下推导:</p>
<script type="math/tex; mode=display">t_w = log(G_w / P_w) = log(\frac{G_w + P_w - P_w}{P_w}) = log(1 + \frac{G_w - P_w}{P_w})</script><p>从上式我们可以看出, 当 $G_w - P_w = 0$ 的时候, 回归函数 $t_w$ 可以看做是线性函数.</p>
<p>这里还有一点疑问: 从公式来说, $t_x$ 和 $t_y$ 本身就已经是线性函数, 而 $t_w$ 和 $t_h$ 只需要 Proposals 和 Ground Truth 的宽高相似即可满足线性回归条件. 那么为什么必须要 IoU 较大才可以? 不是只要宽高相似就可以吗?</p>
<p><a href="http://caffecn.cn/?/question/160" target="_blank" rel="noopener">http://caffecn.cn/?/question/160</a><br><a href="https://blog.csdn.net/zijin0802034/article/details/77685438" target="_blank" rel="noopener">边框回归(BoundingBoxRegression)详解</a></p>
<h3 id="R-CNN-缺点"><a href="#R-CNN-缺点" class="headerlink" title="R-CNN 缺点"></a>R-CNN 缺点</h3><ul>
<li>训练过程是分阶段的(Training is a multi-stage pipeline)</li>
<li>耗费资源(Training is expensive in space and time)</li>
<li>目标检测速度太慢(Object detection is slow)</li>
</ul>
<p><span id="SPPNet"></span></p>
<h2 id="SPPNet"><a href="#SPPNet" class="headerlink" title="SPPNet"></a><a href="../计算机视觉-SPPNet-ECCV2014">SPPNet</a></h2><h3 id="SPPNet-简介"><a href="#SPPNet-简介" class="headerlink" title="SPPNet 简介"></a>SPPNet 简介</h3><p><strong>(1) 提出了一种新的池化方法—-空间金字塔池化SPP</strong>:</p>
<ul>
<li>可以接受任意尺寸的输入图片,并生成固定长度的表征向量</li>
<li>可以进行多尺度的联合训练, 提升模型精度</li>
<li>这种池化方法是比较general的, 可以提升不同模型架构的性能(分类任务)</li>
</ul>
<p>SPPNet实现原理如下图所示:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SPPNet/fig3.jpg?x-oss-process=style/blog_img" alt="SPPNet%2Ffig3.jpg"></div></p>
<p>首先, 设定好固定的网格划分方法, 以便得到spatial bins, 如上图, 有三种不同spatial bins, 网格划分粒度分别为 4×4, 2×2 和 1×1, 因此, spatial bins的数量为:$4\times 4+2\times 2+ 1\times 1 = 21 = M$, 图中的256代表卷积层输出的特征图谱的通道数量, 也就是特征图谱的 depth = $k$. 因此, SPP层的输出为 $kM$ 维的一维向量.</p>
<p>注意: 这里最粗粒度的spatial bins 是对整张特征图谱上进行pooling, 这实际上是为了获得一些全局信息.(之前也很很多work集成了这种全局pooling方法, 貌似有助于提升精度, 同时由于是全局信息, 所以相当general, 可以一定程度上起到降低过拟合的作用)</p>
<p><strong>(2) 将SPP用于目标检测, 并且提出了先求卷积特征图谱, 然后在特征图谱上取区域的的策略:</strong></p>
<ul>
<li>大大提升了模型训练和预测的速度(在预测阶段, 比RCNN快24~102倍, 同时取得了更好的精度).</li>
</ul>
<p><strong>注1: 在特征图谱上使用检测方法不是首次提出</strong>, 而SPP的贡献可以结合了deep CNN结构强大的特征提取能力和SPP的灵活性, 使得精度和速度同时提高.<br>注2: 相比于RCNN, SPPNet使用了EdgeBoxes( $0.2s/img$ )的方法来进行候选区域推荐, 而不是Selective Search( $1\sim 2s/img$ )<br>注3: SPPNet在ILSVRC2014的目标检测任务上取得第二名, 在图片分类任务上取得第三名</p>
<h3 id="SPPNet-缺点"><a href="#SPPNet-缺点" class="headerlink" title="SPPNet 缺点"></a>SPPNet 缺点</h3><ul>
<li>训练过程是分阶段的(Training is a multi-stage pipeline)</li>
<li>无法 Fine-Tuning 金字塔池化层之前的卷积层<br><span id="Fast R-CNN"></span></li>
</ul>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><a href="../计算机视觉-FastR-CNN-ICCV2015">Fast R-CNN</a></h2><h3 id="Fast-R-CNN-有哪些改进"><a href="#Fast-R-CNN-有哪些改进" class="headerlink" title="Fast R-CNN 有哪些改进"></a>Fast R-CNN 有哪些改进</h3><ul>
<li>直接采用在特征图谱上的候选框组成 mini-batch, 共享卷积计算结果, 大大加速训练和推演速度.</li>
<li>提出了 RoI Pooling, 从而可以在特征图谱上截取任意尺寸的候选框</li>
<li>采用 Smooth L1 损失, 使得训练过程中对于回归值的离异点鲁棒性更好, 同时分类损失从用 Softmax 交叉熵, 将二者进行联合训练, 训练过程更加统一.</li>
<li>对于计算量较大的全连接层, 使用奇异值分解加速计算</li>
</ul>
<h3 id="RoI-Pooling-简介"><a href="#RoI-Pooling-简介" class="headerlink" title="RoI Pooling 简介"></a>RoI Pooling 简介</h3><p>RoI Pooling Layer 使用 max pooling 来将 <strong>任意尺寸</strong> 的有效感兴趣区域中的特征转换成一个具有 <strong>固定尺寸</strong> $H\times W (e.g., 7\times 7)$的较小的 feature map, 这里的 $H$ 和 $W$ 是超参数. 在本文中, 一个 RoI(感兴趣区域)就是 feature map 上面的一个矩形窗口. 每一个 RoI 都通过四元组 $(r,c,h,w)$ 来表示(top-left corner, and its height and width).</p>
<p>RoI Pooling的前向传播过程如下:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FastR-CNN/roi_pooling1.jpg?x-oss-process=style/blog_img" alt="FastR-CNN%2Froi_pooling1.jpg"></div></p>
<p>对于任意给定尺寸为 $h\times w$ 的feature map的 RoI 窗口, 将其划分成 $W\times H$ 的网格大小(上图中的示例为 $W\times H= 3\times 3$ ), 这样, 每一个网格 cell 中的尺寸大约为 $h/H \times w/W$, 然后我们在网格 cell 中执行max pooling操作. 和标准的 max pooling 相同, RoI pooling 在卷积图谱上的各个通道之间是独立计算的. 这样, 对于任意size的输入, 都可以获得固定长度的输出. 可以看出, <strong>RoI layer 实际上是 spatial pyramid pooling layer 中的一个特例, 即只有一个 pyramid level.</strong> (但是相比于金字塔池化, RoI 池化只确定了唯一大小的 pooling 窗口, 这使得我们可以利用反向传播更新池化层之前的网络层参数, 进而提高准确率)</p>
<h3 id="RoI-Pooling-如何进行反向传播"><a href="#RoI-Pooling-如何进行反向传播" class="headerlink" title="RoI Pooling 如何进行反向传播"></a>RoI Pooling 如何进行反向传播</h3><p>RoI Pooling 的反向传播过程和 Max Pooling 类似, 不同的是, 对于每个 mini-batch 的 RoI $r$ 和每个 pooling 单元 $j$ 及其输出 $y_{rj}$ ，偏导数 $\partial L / \partial y_{rj}$ 是所有 RoI 反向传播回来的累加和. 具体如下图所示, 当不同的 RoI 区域出现重叠时, 恰好这两个区域都选取了 $x_{2, 3}$ 作为激活点, 那么对这个点的反向传播值 $\frac{\partial L}{\partial x_{2, 3}}$ 就应该等于 $\frac{\partial L}{\partial y_{0,2}} + \frac{\partial L}{\partial y_{1, 0}}$.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FastR-CNN/roi_pooling3.jpg?x-oss-process=style/blog_img" alt="FastR-CNN%2Froi_pooling3.jpg"></div></p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial x_i} = \sum_r \sum_j [i = i^\ast(r,j)]\frac{\partial L}{\partial y_{rj}}</script><h3 id="为什么-RoI-Pooling-比-SPP-效果好"><a href="#为什么-RoI-Pooling-比-SPP-效果好" class="headerlink" title="为什么 RoI Pooling 比 SPP 效果好"></a>为什么 RoI Pooling 比 SPP 效果好</h3><p>SPP的Pooling方式是组合不同划分粒度下feature map的max pooling. 它也具有和 RoI Pooling 类似的效果, 可以接受任意尺度的特征图谱, 并将其提取成固定长度的特征向量, <strong>但是 SPPNet 的池化方式使得无法用恰当的方式来进行反向传播, 因此 SPPNet 没有对浅层的网络进行 fine-tuning, 而是直接在最后两个全连接层上进行fine-tune,</strong> 虽然最后也取得了不错的成果, 但是Roos认为, 虽然离输入层较近的前几层卷积层是比较generic和task independent的, 但是靠近输出层的卷积层还是很有必要进行fine-tune的, 他也通过实验证实了这种必要性, 于是他简化了SPP的Pooling策略, 用一种更简单粗暴的Pooling方式来获得固定长度的输出向量, 同时也设计了相应的RoI Pooling的反向传播规则, 并对前面的基层卷积层进行了fine-tune, 最终取得了不错的效果.</p>
<h3 id="Fast-R-CNN-的-Multi-task-Loss"><a href="#Fast-R-CNN-的-Multi-task-Loss" class="headerlink" title="Fast R-CNN 的 Multi-task Loss"></a>Fast R-CNN 的 Multi-task Loss</h3><p>Multi-task loss: 下式中, $L_{cls}(p,u) = - log p_u$, 即对于真实类别 $u$ 的 log 损失.</p>
<script type="math/tex; mode=display">L(p, u, t_u, v) = L_{cls}(p,u) + \lambda [u \geq 1] L_{loc}(t^u, v) \tag 1</script><script type="math/tex; mode=display">L_{loc}(t^u, v) = \sum_{i\in {x,y,w,h}} smooth_{L_1}(t_i^u - v_i) \tag 2</script><script type="math/tex; mode=display">smooth_{L_1}(x) = \begin{cases} 0.5x^2 && |x|<1 \\ |x| - 0.5 && otherwise \end{cases} \tag 3</script><h3 id="Smooth-L1-相比于-L2-损失-在回归时有什么优势"><a href="#Smooth-L1-相比于-L2-损失-在回归时有什么优势" class="headerlink" title="Smooth L1 相比于 L2 损失, 在回归时有什么优势"></a>Smooth L1 相比于 L2 损失, 在回归时有什么优势</h3><p>smooth L1 损失是一种鲁棒性较强的 L1 损失, 相比于 R-CNN 和 SPPNet 中使用的 L2损失, 它对离异点的敏感度更低. 当回归目标趋于无限时, L2 损失需要很小心的处理学习率的设置以避免发生梯度爆炸, 而 smooth L1 损失则会消除这种敏感情况.</p>
<p>相比于 $L_2$ 损失, $L_1$ 损失对于离异值更加鲁棒, 当预测值与目标值相差很大时, 梯度很容易爆炸, 因为梯度里面包含了 $(t_i^u - v_i)$ 这一项, 而smooth L1 在值相差很大时, 其梯度为 $\pm 1$ ( $L_1$ 在 $x$ 绝对值较大时, 是线性的, 而 $L_2$ 是指数的, 很容易爆炸).</p>
<p>公式(1)中的超参数 $\lambda$ 用于平衡两种损失之间的影响力. 默认情况下 $\lambda = 1$.</p>
<h3 id="SVG-奇异值分解简介"><a href="#SVG-奇异值分解简介" class="headerlink" title="SVG 奇异值分解简介"></a>SVG 奇异值分解简介</h3><p>对于一个权重矩阵为 $u\times v$ 的全连接层来说, 该矩阵可以被近似的因式分解为:</p>
<script type="math/tex; mode=display">W \approx U \Sigma_t V^T</script><p>式中, $U$ 是一个 $u\times t$ 的矩阵, $\Sigma_t$ 是一个 $t\times t$ 的对角矩阵, 包含着矩阵 $W$ 的值最大的 $t$ 个奇异值, $V$ 是一个 $v\times t$ 的矩阵. 可以看到, 奇异值分解将矩阵 $W$ 的参数量从 $uv$ 降低到了 $ut+tv$, 这个 $t$ 就是奇异矩阵中的奇异值数量, 奇异值有一个非常重要的性质, 就是它的下降速度很快, 在很多情况下, 前 10% 甚至 1% 的奇异值的和就站了全部奇异值之和的 99% 以上的比例. 也就是说, 我们可以用最大的 $k$ 个奇异值来近似描述矩阵. 由于 $k$ 远远小于 $\min(u,v)$, 因此可以大大节省参数量. 在实现上, 将单个的全连接网络层的权重矩阵 $W$ 用两层全连接层所替代, 注意在这两层全连接层中间没有非线性激活函数. 第一个全连接层使用的权重矩阵为 $\Sigma_t V^T$ (没有偏置项), 第二个权重矩阵为 $U$ (带有原始矩阵 $W$ 的偏置项).<br>关于奇异值分解更详细的介绍可以看 <a href="../深度学习-奇异值分解">奇异值分解解析</a></p>
<p><span id="Faster R-CNN"></span></p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><a href="../计算机视觉-FasterR-CNN-NIPS2015">Faster R-CNN</a></h2><p><strong>注意, RPN 网络和 Fast R-CNN 网络各自可以进行端到端的训练, 但是 Faster R-CNN 网络并非是端到端的, 它需要结合 RPN 和 Fast R-CNN 网络才可以工作</strong></p>
<h3 id="Faster-R-CNN-简介"><a href="#Faster-R-CNN-简介" class="headerlink" title="Faster R-CNN 简介"></a>Faster R-CNN 简介</h3><p>Faster R-CNN 主要由两部分组成. 其一是用于生成候选区域框的深度全卷积网络, 其二是 Fast R-CNN 检测模型. 二者在训练的时候会进行参数共享.</p>
<p>RPN 主要是通过在 BackBone 网络输出的特征图谱上设置一组固定大小的 anchor boxes 实现. 对于图谱上的每一个像素点, 都会枚举 k 个具有预设尺寸的 anchor boxes. 对于一个 $W\times H$ 大小的特征图谱, 总共会产生 $WHk$ 个 anchor boxes. 对于每一个 anchor box, 我们需要预测 (4+2) 个值, 分别代表 location 的偏移量和是否包含物体的二分类预测.</p>
<p>在实现上, RPN 通常会先用一个 3x3 的卷积层融合 BackBone 传来的特征图谱, 并且保持尺寸不变和通道数不变(通道数也可以变小, 节省计算量), 依然为 $W\times H\times C$. 然后是两个并行的 1x1 卷积构成的 branch, 分别进行回归预测和分类预测, 其输出分别为 $W\times H\times 4k$ 和 $W\times H \times 2k$(这里的 two-class 是为了实现方便, 因此使用了 softmax, 对于二分类来说, 也可以使用 Sigmoid, 这样就只需要输出 $k$ 个 scores).</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FasterR-CNN/anchor.jpg?x-oss-process=style/blog_img" alt="FasterR-CNN%2Fanchor.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FasterR-CNN/fig3.jpg?x-oss-process=style/blog_img" alt="FasterR-CNN%2Ffig3.jpg"></div></p>
<h3 id="正负样本标定策略"><a href="#正负样本标定策略" class="headerlink" title="正负样本标定策略"></a>正负样本标定策略</h3><p>我们会根据 RPN 网络的输出结果, 选取 128 个 positive anchor boxes 和 128 个 negative anchor boxes 参与 Fast R-CNN 网络的训练, <strong>注意, 在 Fast R-CNN 网络中, 然后会有一次 location regression 操作, 因此, Faster R-CNN 网络实际上进行了两次 location regression.(同理也进行了两次 NMS)</strong></p>
<p>我们通常利用下面的方式确定 anchor 属于正样本还是负样本</p>
<ul>
<li>正样本:<ul>
<li>和某个 GT box 具有 <strong>最大(不一定大于0.7)</strong> 的 IoU (防止有些 GT boxes 没有匹配的 anchor)</li>
<li>和某个 GT box 的 IoU 大于0.7</li>
</ul>
</li>
<li>负样本:<ul>
<li>和所有的 GT boxes 的 IoU 都小于 0.3</li>
</ul>
</li>
</ul>
<p><strong>注意1:</strong> 对于剩下的既不是正样本也不是负样本的 anchor boxes, 不参与训练过程<br><strong>注意2:</strong> 一个 GT box 可能会与 <strong>多个</strong> anchor boxes 相匹配<br><strong>注意3:</strong> 一个 anchor box 只能与 <strong>一个</strong> GT box 相匹配</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><script type="math/tex; mode=display">L(\{p_i\}, \{t_i\}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^\ast ) + \lambda \frac{1}{N_{reg}} \sum_i p_i^\ast L_{reg}(t_i, t_i^\ast )</script><p>上式中, $i$ 代表 mini-batch 中 anchor 的下标, $p_i$ 代表预测 anchor $i$ 是一个物体的可能性大小. 如果 anchor 是正样本, 则真实标签 $p^\ast_i$ 为1, 反之, 如果是负样本, 则为0. $t_i$ 是一个 vector, 用来表示参数化以后的边框坐标, $t^\ast_i$ 是正样本 anchor 对应的真实框的参数化坐标. 分类损失函数 <strong>$L_{cls}$ 是一个二分类 log 损失</strong>. 对于回归损失, 我们使用 $L_{reg}(t_i, t^\ast_i) = R(t_i - t^\ast_i)$, 这里 $R$ 代表 robust loss function(smooth L1). $p^\ast_i L_{reg}$ 代表回归损失仅仅会被正样本的 anchor 所激活.<br>$N_{cls}$ 和 $N_{reg}$ 是两个归一项, 同时 $\lambda$ 会调节每一项的权重. 在本文的代码中, $N_{cls}$ 设定为 mini-batch 的大小(256), $N_{reg}$ 设定为 anchor locations 的大小(约2400). 同时 $\lambda$ 默认为10, 这样, $cls$ 和 $reg$ 所占的比重大致相等. 我们在表9中给出了 $\lambda$ 值在很大范围内对结果的影响并不敏感(这是比较好的, 说明我们不需要过度调整该超参的值). 同时, 我们发现归一化也不是必须的, 可以被简化.</p>
<h3 id="共享参数训练方法"><a href="#共享参数训练方法" class="headerlink" title="共享参数训练方法"></a>共享参数训练方法</h3><ul>
<li>4-Step Alternating training(四步式交叉训练, 实验默认训练方法):<ol>
<li>用 ImageNet 初始化 RPN, 训练 region proposals 任务直至收敛(得到一个不错的边界框生成器)</li>
<li>使用第一步生成的 proposals 来训练一个单独的 Fast R-CNN 网络. 这个检测网络同样也是用 ImageNet 预训练的模型进行初始化. <strong>到这里为止, 这两个网络还没有共享卷积层.</strong></li>
<li>第三步, 使用检测网络的参数对 RPN 网络进行初始化, 但是此时 <strong>我们固定住共享的卷积层(backbone), 仅仅 fine-tuning 属于 RPN 独有的那些网络层</strong>.</li>
<li>利用新的 RPN 产生的 proposals, 训练 Fast R-CNN, <strong>同样固定住共享的卷积层, 仅仅 fine-tuning 属于 Fast R-CNN 独有的网络层</strong>.</li>
</ol>
</li>
<li>Approximate joint training(近似联合训练, 包含在官方代码中)<br>将 RPN 和 Fast R-CNN 在训练期间合并到一个网络中, 在每个 SGD 迭代过程, 前向计算会先由 RPN 生成 proposals (修正后的), 然后这些 proposals 会作为输入送到 Fast R-CNN 中, 在反向传播的时候, 对于共享层的参数更新, 会同时考虑来自 RPN 损失和 Fast R-CNN 损失传递过来的信号. <strong>但是这种策略忽略了相对于 proposals boxes 坐标的导数</strong>, 因此这只是一种粗略的联合训练方式. 在实验中, 近似联合训练可以大幅减少训练时间(25~50%), 精度会有所损失, 但是性能依然客观.</li>
<li>Non-approximate joint training(非近似联合训练)<br>正如上面讨论的, RPN 预测的 bounding boxes 的坐标同样与输入数据之间存在联系. 在 Fast R-CNN 的 RoI pooling Layer 中会接受卷积特征, 同时也会将预测的 bounding boxes 作为输入, 因此理论上来说, 一个有效的优化器(backpropagation solver)应该包含相对于 box coordinate 的梯度. 因此, 我们需要 RoI pooling layer 相对于 box coordinates 是可导的.</li>
</ul>
<p>Ross 原话:</p>
<blockquote>
<blockquote>
<blockquote>
<p>it is actually very easy to train RPN + Fast R-CNN approximately* jointly. I’m planning on adding prototxt files and training scripts for how to do this soon. The mAP ends up about the same, but the training time is reduced by about 50%.</p>
<ul>
<li>“approximately” because the RoI pooling layer is not differentiable w.r.t. the RoI coordinates and therefore gradients of the objective w.r.t. the RoI coordinates cannot be passed back through the network. This is likely a minor issue.</li>
</ul>
</blockquote>
</blockquote>
</blockquote>
<h3 id="如果两个物体重合度很高-Faster-R-CNN-会怎么样"><a href="#如果两个物体重合度很高-Faster-R-CNN-会怎么样" class="headerlink" title="如果两个物体重合度很高, Faster R-CNN 会怎么样?"></a>如果两个物体重合度很高, Faster R-CNN 会怎么样?</h3><p>在 RPN 阶段, 优势 proposals 是类别不可知的(class-agnostic)<br>因此重叠度非常高的两个框会被 NMS 过滤掉. 因此对于重合度非常高的两个物体, 很有可能会发生漏检</p>
<p>注, 但是在 Detection 阶段, 每个 anchor boxes 都与一个 GT 对应, 并且会预测特定的类别, 在进行 NMS 的时候, 是对每个类别分别进行 NMS, 所以, 也有一定的概率能够检测成功, 不过可能置信度较低, 同时框的 IoU 水平也较低(因为较高的 IoU 已经在 RPN 阶段被 NMS 掉了)</p>
<p>Faster R-CNN 由两部分组成, 分别为 class-agnostic 的 RPN, 以及 class-specific 的 detection</p>
<p><span id="Mask R-CNN"></span></p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a><a href="../计算机视觉-MaskR-CNN-ICCV2017">Mask R-CNN</a></h2><h3 id="Mask-R-CNN-简介"><a href="#Mask-R-CNN-简介" class="headerlink" title="Mask R-CNN 简介"></a>Mask R-CNN 简介</h3><p>Mask R-CNN 的大体框架还是 Faster R-CNN,  它在 Faster R-CNN 模型中添加了一个与分类和回归分支平行的掩膜预测分支. 掩膜分支(mask branch) 是一个小型的 FCN 网络, 它会作用在每一个 RoI 上, 以像素到像素的方式来预测一个分割掩膜. Mask R-CNN 的掩膜预测分支对于每一个 RoI 的输出维度为 $Km^2$, 也就是每一个类别都会单独输出一个 $m\times m$ 大小的掩膜. 在预测掩膜时非常关键的一点就是要对分类任务和分割任务解耦, 否则对于多分类任务会引起类别之间的竞争, 因此, Mask R-CNN 使用了基于单像素的 sigmoid 二值交叉熵来替换基于单像素的 Softmax 多项式交叉熵. 另外, 在 Faster R-CNN 中使用的 RoI pooling 需要经过两次量化取整(图像坐标到特征图谱坐标, 特征图谱划分固定网格)才能获得固定长度的特征向量. 这种粗糙的量化操作会造成 RoI 和特征图谱之间的不对齐, 这对精度要求较高的的分割任务来说有较大影响. 为了克服这个问题, Mask R-CNN 提出了 RoI Align 层来替代 RoI Pooling, RoI Align 的核心思想就是避免在 RoI 边界上或者 bins 中执行任何量化计算. 它在处理每一个 RoI 的时候, 会保持其浮点边界的值而不进行量化操作, 然后将 RoI 划分成的 $k\times k$ 大小的网格, 对于每一个网络, 都会固定四个采样点, 并利用双线性插值法来计算每个采样点的数值, 最后根据这些数值进行池化操作. 除了这些比较重要的点之外, Mask R-CNN 也有一些其他的优化, 比如说更多的 anchor, 更大的 batch size, 更强的 backbone 网络(ResNeXt+FPN)等等.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/fig4.jpg?x-oss-process=style/blog_img" alt="MaskR-CNN%2Ffig4.jpg"></div></p>
<h3 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h3><p>由于掩膜预测分支是像素级别的二分类网络, 因此它对于 proposals 的精度要求更多, 而原先的 RoI Pooling 存在两次量化操作, 使得位置映射关系较为粗糙, 为此, MaskRCNN 利用双线性插值法消除量化, 提出了 RoI Align 进行改进.</p>
<p>RoI Pooling存在两次量化过程:</p>
<ul>
<li>将 RoI 坐标量化为特征图谱上的整数坐标值(使用 $[x/16]$ 计算 RoI 在特征图谱上的坐标, 16 是总步长, $[]$ 代表四舍五入)</li>
<li>将量化后的 RoI 分割成 $k\times k$ 个bins, 并对每一个 bin 的边界量化, 使其与特征图谱的整数坐标对应.</li>
</ul>
<p>要将下图中的 RoI 均等分成 4 块是做不到的, 因此, RoI Pooling 会就近取整. 这是第二次量化, 第一次量化是将 RoI 与 feature map 上的网格对齐</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/roi_pooling.jpg?x-oss-process=style/blog_img" alt="MaskR-CNN%2Froi_pooling.jpg"></div></p>
<p>可以看出, 上面的量化操作是一种很粗糙的 Pooling 方式, 由于 feature map 可以看做是对原图特征的高度概括信息, 所以 feature map 上的细微差别映射回原图时, 往往会导致产生很大的像素位移差. 故此, 提出了RoI Align的解决思路: 取消量化操作(即, 我们使用 $x/16$, 而不是 $[x/16]$), 使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值, 从而将整个特征聚集过程转换为一个连续的操作. 其具体流程如下(假设 RoI 使用 2x2 bins):</p>
<ol>
<li>计算 RoI 在特征图谱上的浮点坐标 $x/16$</li>
<li>在每一个 bin 当中, 均匀的选取若干个(实验效果显示 4 个较好)采样点</li>
<li>对于每一个采样点, 利用离它最近的四个 feature map 点计算当前采样点的值</li>
<li>对每个 bin 中的所有采样点, 分别执行 max / average 操作.</li>
<li>最终输出 $k \times k$ 的固定尺寸特征图谱 (如 2x2).</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/fig3.jpg?x-oss-process=style/blog_img" alt="MaskR-CNN%2Ffig3.jpg"></div></p>
<h3 id="Mask-分支怎么实现的"><a href="#Mask-分支怎么实现的" class="headerlink" title="Mask 分支怎么实现的"></a>Mask 分支怎么实现的</h3><p>可以用多层感知机(若干全连接层)或者全卷积网络(若干卷积层)实现, 如下图中(e)所示, 卷积层由于具有空间编码的优势, 因此效果更好.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/MaskR-CNN/tab2.jpg?x-oss-process=style/blog_img" alt="MaskR-CNN%2Ftab2.jpg"></div></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>基于 Faster R-CNN 的基本结构, 替换 backbone 为 ResNet-50/101-C4 和 ResNet-50/101-FPN, ResNeXt-101-FPN.</li>
<li>除了边框回归和目标分类两个分支外, 新添加了一个全卷积的 Mask Prediction 网络, 该分支在每一个 RoI 上的输出维度为 $K\times m^2$, 代表 $K$ 个类别的二值掩膜.</li>
<li>在计算 mask 的损失时, 使用的是 sigmoid 的二分类损失, 而不是多分类的 softmax 损失, 这减少了类别之前的竞争, 使得可以生成更好的分割结果.</li>
<li>RoIAlign 消除了 RoI Pooling 中的两次量化操作, 使得最终提取到的特征可以 RoI 尽可能的对齐. 从而大幅提升在实例分割任务上的性能表现.</li>
</ul>
<p><div style="width: 550px; margin: auto"><img src="https://wx2.sinaimg.cn/large/d7b90c85ly1fx1toyw0xkj20kc0a5wkw.jpg" alt="图1"></div></p>
<p><span id="FPN"></span></p>
<h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a><a href="../计算机视觉-FPN-CVPR2017">FPN</a></h2><h3 id="FPN-简介"><a href="#FPN-简介" class="headerlink" title="FPN 简介"></a>FPN 简介</h3><p>将最后一层特征图谱进行不断尽快上采样, 并与每一个金字塔阶级的特征图谱进行加法合并操作, 得到新的表征能力更强的不同金字塔层次的特征图谱, 然后将RoI按照尺寸分别映射到这些特征图谱上, 再在每个特征图谱上进行类别和位置预测. 在横向连接 $\{C_2, C_3, C_4, C_5\}$ 与 $\{P_2, P_3, P_4, P_5\}$ 时, 我们使用 element-wise add 操作. 然后我们使用一个 3x3 的卷积对每一个融合后的图谱进行操作以生成最终的金字塔图谱, 这是为了消除上采样的混叠效应(aliasing effect of unsampling??), 由于金字塔的每一层都使用了共享的分类器和回归器(就像传统的 featurized image pyramid 一样), 因此我们固定了其输出深度 $d$(numbers of channels), 本文中, 我们令 $d=256$, 也就是说所有 <strong>额外添加的卷积层</strong> 的输出通道数都是 256.</p>
<p>可以直观感受到, 这种多尺度的特征图谱在面对不同尺寸的物体时, 具有更好的鲁棒性, 尤其是在面对小型物体时. 同时, 这种特征金字塔结构是一种通用的特征提取结构, 可以应用到不同的网络框架中, 显著提高(5~8%)模型的召回率(因为提出了更多不同尺度, 不同特征信息的anchor box), 并且可以广泛提高(2~3%)模型的mAP.</p>
<p><strong>思想: 浅层特征负责感知和检测小物体, 但是欠缺足够深度的高级语义信息, 因此将具备深层语义信息的特征层通过反卷积的方式扩大 feature map 的 size, 然后结合浅层和深层的特征图谱来进行预测.</strong></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FPN/fig3.jpg?x-oss-process=style/blog_img" alt="FPN%2Ffig3.jpg"></div></p>
<h3 id="FPN-的使用-RPN-Fast-R-CNN"><a href="#FPN-的使用-RPN-Fast-R-CNN" class="headerlink" title="FPN 的使用(RPN, Fast R-CNN)"></a>FPN 的使用(RPN, Fast R-CNN)</h3><ul>
<li>FPN for RPN:<br>对于特征图谱 $\{P_2, P_3, P_4, P_5, P_6\}$, 为其分别分配不同大小的 anchors ${32^2, 64^2, 128^2, 256^2, 512^2}$, 然后利用一个 <strong>共享的</strong> rpn heads 进行训练和预测</li>
<li>FPN for Fast R-CNN:<br>将FPN用于FastRCNN时, 我们需要在不同的金字塔层次上赋予不同尺度的 RoI 大小(因为 RoI pooling 是根据特征图谱和原图的尺寸关系决定的), 较小的 RoI, 会被分配到较浅的网络层, 因为太深的图谱可能已经不包含小物体信息了. 注意, <strong>detect heads 在所有层级上的权重都是共享的</strong></li>
</ul>
<p><span id="FCN"></span></p>
<h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a><a href="../计算机视觉-FCN-CVPR2015">FCN</a></h2><h3 id="FCN-简介"><a href="#FCN-简介" class="headerlink" title="FCN 简介"></a>FCN 简介</h3><p>FCN 是分割模型, 这里是为介绍 R-FCN 做的铺垫</p>
<p>FCN 将网络中的全连接层全都换成了卷积层(卷积核大小为 $1\times 1$, 通道数为 FC 神经元个数), 这么做有两个好处:</p>
<ol>
<li>可以接受任意尺寸的图片输入</li>
<li>对于较大的图片输入, 最终会产生多个卷积块, 共用一套权重, 减少重复计算, 从而可以加快模型的计算速度.</li>
</ol>
<p>在进行分割任务时, FCN 会利用 <strong>反卷积</strong> 和 <strong>跳跃连接</strong> 进行像素预测. 也就是说它会将浅层图谱的预测结果和 upsample 后的图谱预测结果融合(采用 max fusion), 这样预测出来的结果会更加精细, 在 FCN 中, 分别在步长为 16 和 步长为 8 的特征图谱进行了分割预测, 结果也显示越精细的特征图谱, 分割的结果也越好. 只不过随着步长的缩短, 获得的提升也慢慢变小了.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCN/fig2.jpg?x-oss-process=style/blog_img" alt="FCN%2Ffig2.jpg"></div></p>
<p>在 pool4 上添加一个 1x1 卷积来生成额外的 class predictions, 然后在 conv7 (stride 32) 上使用 2x upsampling 来获得 2x upsampled prediction, 最后将二者结合, 如下所示</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCN/fig3.jpg?x-oss-process=style/blog_img" alt="FCN%2Ffig3.jpg"></div></p>
<p>反卷积层相对于双线性插值来说, 具有可学习的参数, 可以通过网络的输出 loss 动态的调整自身的上采样过程</p>
<p><span id="R-FCN"></span></p>
<h2 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a><a href="../计算机视觉-R-FCN-NIPS2016">R-FCN</a></h2><h3 id="R-FCN-简介"><a href="#R-FCN-简介" class="headerlink" title="R-FCN 简介"></a>R-FCN 简介</h3><p>R-FCN 的主要贡献在于解决了“分类网络的位置不敏感性（translation-invariance in image classification）”与“检测网络的位置敏感性（translation-variance in object detection）”之间的矛盾，在提升精度的同时利用“位置敏感得分图（position-sensitive score maps）”提升了检测速度。<br>对于 two-stage 检测网络来说, 在 RoI Pooling 之前的 backbone 网络层的计算时被所有 RoI 所共享的, 而 RoI Pooling 之后的计算时单独对每个 RoI 进行分类和回归, 因此虽然各个 RoIs 的计算存在大量的重复, 依然无法共享这部分计算. 而对于前面的 backbone 网络来说, 它本身不会考虑坐标信息, 因此具有 “位置不敏感性(translation-invariance)”, 而 RoI Pooling 之后的网络, 它会考虑目标的位置信息, 因此具有 “位置敏感性(translation-variance)”. 为了让目标检测网络具有 “位置敏感性”, Faster R-CNN 采取了一种不太自然的做法, 那就是将 RoI Pooling 插在了 backbone 网络的卷积层之间, 对于 ResNet 来说, 它将 RoI Pooling 插在了 C4 卷积段的后面, 这样带来的问题就是 RoI Pooling Layer 之后的 C5 卷积段是无法在 RoIs 上共享卷积计算的, 牺牲了速度来换取精度. 如下表1中所示.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/R-FCN/fig1_tab1.jpg?x-oss-process=style/blog_img" alt="R-FCN%2Ffig1_tab1.jpg"></div></p>
<p>为了解决上述问题, R-FCN (Region-based Fully Convolutional Network) 使用 position-sensitive score map 来编码每个物体的 RoI 信息, 它将 RoI 划分成 $k\times k$ 子区域, 每个子区域代表了 RoI 的某一特定部位. <strong>R-FCN 首先会共享 backbone 的所有卷积层</strong>, 然后在共享的卷积层的最后接上一层卷积, 这个卷积层的输出图谱就是 position-sensitive score map, 它的 height, width 保持不变, 其 channels = $k^2(C+1)$, $C+1$ 代表物体类别, 也就是说每个类别都有 $k^2$ 个 score maps, 每个 score map 都对应了物体某一特定部位的响应值.<br>然后利用 Position-sensitive RoI Pooling 对这个尺度为 $W\times H \times k^2 (C+1)$ 的特征图谱进行池化, 池化时对于每个 RoI 的第 $i$ 个子区域, 我们就在第 $i$ 个 score map 上的对应区域采用池化来得到 RoI 第 $i$ 个子区域的值, 对于每一个类别都是如此, 最终, PsRoI Pooling 的输出就是一个 $k^2\times (C+1)$ 大小的特征图谱(注意 RoI Pooling 的输出尺寸是用划分的网格数决定的).<br>最后, 我们对得到的 $k^2\times (C+1)$ 大小的特征图谱通过取 <strong>平均值</strong> 的方式来投票, 得到 $(C+1)$ 维的向量, 然后利用 softmax 得到每个类别的预测概率.<br>对于回归操作, 其方法类似, 通过在回归分支上添加一个卷积层实现, 卷积层的输出尺寸是 $W\times H\times 4k^2$, 然后经过 Ps-RoI Pooling, 对每个 RoI 都输出 $k^2 \times $ 尺寸的特征图谱, 再经过取平均投票的方式, 得到一个 $4-d$ 的向量, 该向量就代表了这个 RoI 所对应的偏移量 $t = (t_x, t_y, t_w, t_h)$</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/R-FCN/fig2.jpg?x-oss-process=style/blog_img" alt="R-FCN%2Ffig2.jpg"></div></p>
<p><span id="CoupleNet"></span></p>
<h2 id="CoupleNet"><a href="#CoupleNet" class="headerlink" title="CoupleNet"></a><a href="../计算机视觉-CoupleNet-ICCV2017">CoupleNet</a></h2><h3 id="CoupleNet-简介"><a href="#CoupleNet-简介" class="headerlink" title="CoupleNet 简介"></a>CoupleNet 简介</h3><p>R-FCN 利用 PSRoI Pooling, 利用物体的局部特征, 将位置敏感行引入到了检测网络中, 但是同时, PsRoI Pooling 的设计在一定程度上忽略了物体的整体结构信息以及它的 Global Feature. 因此 CoupleNet 提出利用 RoI Pooling 捕捉物体的 Global Feature, 并将其与 PsRoI Pooling 提取到的 local feature 相结合, 共同决定最终的预测结果. 其网络整体结构如下图所示. CoupleNet 在 C4 阶段利用 RPN 网络提取 Proposals, 然后在 C5 阶段之后, 分成了两个分支:</p>
<ol>
<li>Local FCN: 由 PSRoI Pooling 和相应的预测网络组成, 用于提取 local feature, 就像 R-FCN 中的那样</li>
<li>Global FCN: 由 RoI Pooling 和相应的预测网络组成, 其中, RoI Pooling 会分别在原始的 RoI 和 <strong>扩大两倍</strong> 后的 RoI 上提取特征, 然后将结果在通道维度上连接. 这样做不仅可以提取到该物体的全局特征, 同时也可以提取到物体的 context 信息, 这对于解决遮挡类问题有很大帮助.</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CounpleNet/fig2.jpg?x-oss-process=style/blog_img" alt="CounpleNet%2Ffig2.jpg"></div></p>
<h3 id="CoupleNet-是怎么结合-Local-Feature-和-Global-Feature-的"><a href="#CoupleNet-是怎么结合-Local-Feature-和-Global-Feature-的" class="headerlink" title="CoupleNet 是怎么结合 Local Feature 和 Global Feature 的"></a>CoupleNet 是怎么结合 Local Feature 和 Global Feature 的</h3><p>Normalization:</p>
<ol>
<li>L2 归一化: 会损伤性能</li>
<li>1x1 卷积自动学习合适的输出: 输入输出 Tensor 形状不变, 每个值都根据学习到的结果进行变化. 效果好, 提升 0.6 个百分点.</li>
</ol>
<p>Coupling Structure(结合 local 和 global 分支):</p>
<ol>
<li>element-wise sum: 在任何情况下有最优, 即使在不使用 Normalization 的情况下, 也是最优选择</li>
<li>element-wise product: 很差</li>
<li>element-wise maximum: 不如 sum</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CounpleNet/tab1.jpg?x-oss-process=style/blog_img" alt="CounpleNet%2Ftab1.jpg"></div></p>
<h3 id="CoupleNet-为什么可以解决遮挡类问题"><a href="#CoupleNet-为什么可以解决遮挡类问题" class="headerlink" title="CoupleNet 为什么可以解决遮挡类问题"></a>CoupleNet 为什么可以解决遮挡类问题</h3><p>以下图为例, 沙发坐了了两个人, 当我们对检测沙发时, PSRoI Pooling 会给出非常低的置信度, 因他目标框内的各个部位都不是沙发的特定部位, 而是其他类别的物体, 因此, R-FCN 对于遮挡问题表现很差. 对于 RoI Pooling 来说, 由于它在一定程度上考虑了 RoI 的整体特征, 因此, 可以给出一定的置信度, 但是也只有 0.45.<br>而 CoupleNet 一方面使用 PSRoI Pooling 解决了分类网络位置不敏感和检测网络位置敏感性之间的矛盾, 同时利用 global 信息和 context 信息弥补了 PSRoI Pooling 只关注 local feature 的缺点, 因此在结合两个分支的输出结果以后, 可以很好的检测出被局部遮挡的物体.(人坐在沙发, 椅子, 桌子前等等)</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CounpleNet/fig1.jpg?x-oss-process=style/blog_img" alt="CounpleNet%2Ffig1.jpg"></div></p>
<p><span id="Deformable ConvNets V1"></span></p>
<h2 id="Deformable-ConvNets-V1"><a href="#Deformable-ConvNets-V1" class="headerlink" title="Deformable ConvNets V1"></a><a href="../计算机视觉-DCN-ICCV2017">Deformable ConvNets V1</a></h2><h3 id="Deformable-ConvNets-V1-简介"><a href="#Deformable-ConvNets-V1-简介" class="headerlink" title="Deformable ConvNets V1 简介"></a>Deformable ConvNets V1 简介</h3><p>Deformable ConvNet 从目标检测任务中物体的几何形变角度发出, 在神经网络中引入了具有学习空间几何形变能力的可形变卷积网络(convolutional neutral networks). 该网络主要由两个模块组成, 分别是 deformable convolution 和 deformable RoI. 对于可形变卷积来说, 通过在每个卷积核的采样点上加一个偏移量来达到更好的采样效果. 对于可形变 RoI pooling 来说, 通过对传统的 RoI bins 添加一个偏移量还使得 RoI pooling 中的窗口具有能够适应几何形变的效果.<br>Deformable ConvNet 一个比较好的性质就是它不会对原有检测模型的整体结构进行更改, 也不会增加过多的计算量, 因此可以相对容易的添加到现有的检测模型当中, 同时还可以和其他多种提升精度的 trick 叠加使用.</p>
<h3 id="Deformable-模块具体是怎么实现的"><a href="#Deformable-模块具体是怎么实现的" class="headerlink" title="Deformable 模块具体是怎么实现的"></a>Deformable 模块具体是怎么实现的</h3><p><a href="https://blog.csdn.net/u011974639/article/details/79996353" target="_blank" rel="noopener">csdn</a><br><a href="https://github.com/msracver/Deformable-ConvNets/tree/master/DCNv2_op" target="_blank" rel="noopener">github official implementation</a></p>
<p>Deformable Convolution: <strong>在原始特征图谱平面的每一个 location 上都生成一组与卷积核采样点个数相关的偏移量</strong></p>
<ol>
<li>对于一个形状为 $W\times H \times C$ 的特征图谱, 我们将其作为输入, 添加一个新的卷积层, 这个卷积层的输出形状为 $W\times H \times 2N$, 即为 offset filed, 其中 $N = W_k \times H_k$ 代表卷积核平面上的采样点个数.</li>
<li>在计算某一个的输出值时, 会根据 offset filed 的偏移量来决定新的采样点 position</li>
<li>利用双线性插值重新计算每个 position 的值</li>
<li>得到所有 position 的值后, 就利用卷积计算得到该点的结果.</li>
<li>对所有的卷积计算都执行类似的过程, 直到结算结束.</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCN/fig2.jpg?x-oss-process=style/blog_img" alt="DCN%2Ffig2.jpg"></div></p>
<p>Deformable RoI Pooling:</p>
<ol>
<li>先利用普通的 RoI Pooling 得到池化后 $k\times k$ 大小的特征图谱</li>
<li>利用一个 FC 层生成归一化的 offsets</li>
<li>将 offsets 应用到对应的 bins 中得到新的像素值.</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCN/fig3.jpg?x-oss-process=style/blog_img" alt="DCN%2Ffig3.jpg"></div></p>
<p>Deformable Ps RoI Pooling:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCN/fig4.jpg?x-oss-process=style/blog_img" alt="DCN%2Ffig4.jpg"></div></p>
<p><span id="Deformable ConvNets V2"></span></p>
<h2 id="Deformable-ConvNets-V2"><a href="#Deformable-ConvNets-V2" class="headerlink" title="Deformable ConvNets V2"></a><a href="../计算机视觉-DCNv2-Arxiv2018">Deformable ConvNets V2</a></h2><h3 id="Deformable-ConvNets-V1-存在哪些问题"><a href="#Deformable-ConvNets-V1-存在哪些问题" class="headerlink" title="Deformable ConvNets V1 存在哪些问题"></a>Deformable ConvNets V1 存在哪些问题</h3><p>DCNv1 虽然可以适应不同几何形变的物体形状, 但是它存在的问题就是学习到的 offsets 不可控, 进而导致引入了过多的 context, 而这些 context 对于网络来说属于干扰信息, 是有害的</p>
<h3 id="Deformable-ConvNets-V2-做了哪些改进"><a href="#Deformable-ConvNets-V2-做了哪些改进" class="headerlink" title="Deformable ConvNets V2 做了哪些改进"></a>Deformable ConvNets V2 做了哪些改进</h3><ol>
<li>增加了更多的 Deformable Convolution<br>DCNv1 中只有 ResNet C5 中有 Deformable Conv(共 3 个), 而在 DCNv2 中把 C3~C5 的 3x3 conv 都换成了 Deformable Conv(共 12 个)</li>
<li>让 Deformable Conv 不仅能够学习 offsets, 还能够学习每个采样点的权重, 就是文中的 modulation 机制<br>在 DCNv1 中, Deformable Conv 只学习 offset:<script type="math/tex; mode=display">y(p) = \sum_{k=1}^{K} w_k \cdot x(p + p_k + \Delta p_k)</script>而在 DCNv2 中, 还加入了对每个采样点的权重 $\Delta m_k$:<script type="math/tex; mode=display">y(p) = \sum_{k=1}^{K} w_k \cdot x(p + p_k + \Delta p_k) \cdot \Delta m_k</script>这样做的好处是对采样点值的选取起到了更灵活的规范作用, 对于一些不想要的采样点, 只需令 $\Delta m_k$ 学成 0 即可.</li>
<li>利用知识蒸馏让 DCNv2 来模拟 R-CNN 的 feature.<br>有论文指出将 R-CNN 和 Faster R-CNN 的 Classification score 结合起来可以提升 performance, 说明 R-CNN 学到的 focus 在物体上的 feature 可以解决 redundant context 的问题. 但是增加的额外计算会使得 inference 速度慢很多, 因此 DCNv2 的解决方法就是让 R-CNN 当做 teacher network, 让 DCNv2 的 RoI Pooling 之后的 feature 去模拟 R-CNN 的feature.</li>
</ol>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/DCNv2/fig3.jpg?x-oss-process=style/blog_img" alt="DCNv2%2Ffig3.jpg"></div></p>
<p><span id="Cascade R-CNN"></span></p>
<h2 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a><a href="../计算机视觉-CascadeR-CNN-CVPR2018">Cascade R-CNN</a></h2><h3 id="Cascade-R-CNN-简介"><a href="#Cascade-R-CNN-简介" class="headerlink" title="Cascade R-CNN 简介"></a>Cascade R-CNN 简介</h3><p><strong>本文针对检测问题中正负样本区分的 IoU 阈值选择问题提出了一种新的目标检测框架, Cascade R-CNN</strong><br>在 two-stage 的目标检测模型当中, 需要设置 IoU 阈值来区分正样本和负样本, 通常, 阈值选的越高, 正样本的框就与真实框越接近, 但是这样就会使得正样本的数量大大降低, 训练时容易产生过拟合问题, 而如果阈值选的较低, 就会产生大量的 FP 样本. 根据经验和实验证明可知, <strong>当输入的 proposals 和真实框的 IoU 的值, 与训练器训练时采用的 IoU 的阈值比较接近的时候, 训练器的性能会比较好</strong>, 为此, 作者提出了一种级联式的阈值训练方法, 先在较低的阈值上训练检测器, 得到具有更高 IoU 的候选框输出, 然后在此基础上进行训练, 不断提升 IoU 的阈值, 这样一来, 最终生成的候选框质量会变得更高 (与真实框的 IoU 更大). <strong>作者提出这种框架的启发来自于图1(c), 整体来说, 输入的 proposals 的 IoU 在经过检测器边框回归以后, 其输出的边框与真实框会有更大的 IoU, 因此可以将这个具有更大 IoU 的框作为下一个检测器的输入, 同时调高训练时的 IoU, 进而得到质量更高的框</strong></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CascadeR-CNN/fig1.jpg?x-oss-process=style/blog_img" alt="CascadeR-CNN%2Ffig1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/CascadeR-CNN/fig3.jpg?x-oss-process=style/blog_img" alt="CascadeR-CNN%2Ffig3.jpg"></div></p>
<p>由上图可知, 在 Cascade R-CNN 中:</p>
<ul>
<li>每一个 stage 的 detector 都可以有足够多满足阈值的样本进行训练, 不会发生过拟合问题</li>
<li>更深层的 detector 可以在精度更高的 proposals 上进行预测, 生成的结果效果更好</li>
<li>每个 stage 的阈值是逐级升高的(对比 Iterative BBox 使用的是相同的阈值), 使之可以逐渐提高 proposals 的 IoU</li>
</ul>
<p><span id="SNIP"></span></p>
<h2 id="SNIP"><a href="#SNIP" class="headerlink" title="SNIP"></a><a href="../计算机视觉-SNIP">SNIP</a></h2><h3 id="SNIP-简介"><a href="#SNIP-简介" class="headerlink" title="SNIP 简介"></a>SNIP 简介</h3><p>SNIP: Scale Normalization for Image Pyramids<br>MST: Multi-Scale Training</p>
<p>SNIP 可以看做是 MST 的改进版, 只有当某个 RoI 的尺度处于预定的尺度范围内时, 该 RoI 才会参与训练, 否则, BP 的时候会将其忽略(梯度置为 0). 这里有一个假设前提, 即: 对于一个物体, 因为多尺度训练, 它总有机会落在一个合理的尺度范围内. 只有这部分合理尺度的物体参与了训练, 剩余部分在BP的时候被忽略了.</p>
<p>对于一个特定的分辨率 $i$, 如果 RoI 的面积 $ar(r)$ 落在了合理区间 $[s_i^c, e_i^c]$ 内, 我们就将其标记为有效样本, 否则将其标记为无效样本. 与 invalid gt box 的交并比大于 0.3 的 anchors 都会在训练中剔除(即, 他们的梯度设置为 0).</p>
<p>在 Inference 阶段, 我们在每个分辨率下都使用 RPN 来生成 proposals, 并在每个分辨率下进行单独分类, 如图 6 所示. 和训练阶段相同, 我们在每个分辨率下没有选择落在特定范围外的检测结果(是 detectin, 不是 proposals).</p>
<p><strong>池化后的 RoI 的分辨率和预训练的网络相匹配, 因此在 fine-tuning 期间网络更容易学习.</strong></p>
<p>如果某个分辨率下的图像在 valid range 内没有 gt boxes, 那么这个 image-resolution pair 在训练时会被忽略.</p>
<p><div style="width: 550px; margin: auto"><img src="https://wx1.sinaimg.cn/large/d7b90c85ly1g1nf97t0s9j21jf0oyhat.jpg" alt="图6"></div></p>
<p><span id="SNIPER"></span></p>
<h2 id="SNIPER"><a href="#SNIPER" class="headerlink" title="SNIPER"></a><a href="../计算机视觉-SNIPER">SNIPER</a></h2><h3 id="SNIPER-简介"><a href="#SNIPER-简介" class="headerlink" title="SNIPER 简介"></a>SNIPER 简介</h3><p>缩放图像后挑处于某一尺度下的gt,用包含挑选gt的chips来做正样本,然后用一个RPN网络来生成proposal后挑选负样本.</p>
<p>这样不用对原始图像进行训练, 而且chip尺寸不大,可以用大batch, 显著加速了训练3x.</p>
<p>chips是某个图片的某个scale上的一系列固定大小的（比如KxK个像素）的以恒定间隔（比如d个像素）排布的窗口（window） ，每个window都可能包含一个或几个objects。</p>
<p>一张图的每个scale，会生成 <strong>若干个</strong> Positive Chips 和 <strong>若干个</strong> Negative Chips 。每个Positive Chip都包含若干个ground truth boxes，所有的Positive Chips则覆盖全部有效的ground truth boxes。每个 Negative Chips 则包含若干个 false positive cases。</p>
<p>positive chip:<br>对于每个scale，都会有个 area range $R^i=[r^i_{min},r^i_{max}]，i\in [1,n]$ ，这个范围决定了这个scale上哪些ground truth box是用来训练的。所有落在 $R^i$ 范围内的 ground truth box 称为有效的（对某个scale来说），其余为无效的，有效的 gt box 集合表示为 G_i 。从所有chips中选取包含（完全包含）有效 gt box最多的 top K 个 chips，作为Positive Chips，其集合称为 $C^i_{pos}$.<br>每一个 gt box 都会有 chip 包含它。因为 $R^i$ 的区间会有重叠，所以一个gt box可能会被不同scale的多个chip包含，也有可能被同一个scale的多个chip包含。被割裂的gt box（也就是部分包含）则保持残留的部分。</p>
<p>Negative Chip Selection</p>
<p>如果只用正chips作为样本进行训练，模型的假正利率往往很高（不包含任何物体的chip判断为包含包含物体的chip）。具体原因是因为参与模型训练的数据都是包围着Ground Truth的chips，而在测试的时候（第2节详细介绍SNIPER的测试部分），输入的是整张图像的图像金字塔，这时候必然包含不包围任何Ground Truth的背景区域. 为此, SNIPER 会生成相应的 Negative Chip 来弥补.</p>
<p>生成方式:<br>论文中给出的策略是首先只使用正 chip 训练一个只有几个 Epoch 的弱 RPN。在这里我们对 RPN 的精度并没有特别高的要求，因为它只是我们用来选择 chip 的一个工具，对最终的结果影响十分微弱。尽管这个 RPN 检测能力很弱，但是其并不是随机初始化的一个模型，它得到的检测框还是有一定的置信度的。所以策略的第二步是根据弱 RPN 的检测结果选择那些 “假正” 的样本。详细的说，首先去掉 $C^i_{pos}$ 中的正 chips，然后根据弱 RPN 的检测结果，从每个尺度选择至少包含 M 个候选区域的chips组成负 chips 池，最后在训练的时候从中随机选择固定数量的负样本进行训练.</p>
<h3 id="SNIPER-小结"><a href="#SNIPER-小结" class="headerlink" title="SNIPER 小结"></a>SNIPER 小结</h3><p>小物体检测一直是困扰物体检测领域的一个重要难题，传统的图像金字塔式解决该问题的一个常见的传统策略，但是速度太慢，SNIPER的提出便是动机便是解决图像金字塔的速度问题。</p>
<p>需要注意SNIPER并不是一个检测算法，而是对输入图像的一个采样策略，其采样的结果（chips）将作为输入输入到物体检测算法中。</p>
<p>算法虽然使用了RPN，但是并不是离开了RPN就无法工作了，RPN提供了一个提取假正利率的功能，这个可以通过Selective Search或者Edge Box近似替代。</p>
<p>另外，SNIPER仅仅是对训练速度的提升，往往更重要的检测速度并没有提升，反而是模型必须依赖图像金字塔，这反而降低了模型的通用性。</p>
<p>最后，作者开源的源码和论文出入较大，读起来比较费劲，等之后有时间的话再详细学习这份源码。</p>
<p><span id="SSD"></span></p>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a><a href="../计算机视觉-SSD-ECCV2016">SSD</a></h2><h3 id="SSD-简介"><a href="#SSD-简介" class="headerlink" title="SSD 简介"></a>SSD 简介</h3><p>SSD 是一种 one-stage 检测模型, 它最主要的特点就是使用了多尺度的特征图谱进行 one-stage 的目标检测预测, 具体来说, SSD 在 VGGNet 之后又添加了五个卷积段, 每个卷积段都是用 $1\times 1$ 和 $3\times 3$ 大小的卷积核组成的, 然后在加上 VGGNet 的 conv4_3 卷积层, 总共可以得到六种不同尺度的特征图谱. 然后对于每一个特征图谱上的每一个 location, 都会有 $k$ 个 default boxes 作为初始的候选框, 不同尺度的特征图谱对应的 $k$ 的大小也不相同(4, 6, 6, 6, 4, 4). 对于一个尺度为 $m \times n$ 的特征图谱来说, 它具有的 default box 的个数就是 $m\times n\times k$, 又因为 one-stage 模型会在回归的同时进行分类, 因此, 最终的输出结果是一个形状为 $m\times n\times k\times (c + 4)$ 的 tesor, $k$ 就代表了 $k$ 个 default box, $(c+4)$ 代表了每个 box 的分类得分和坐标偏移量.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SSD/fig2.jpg?x-oss-process=style/blog_img" alt="SSD%2Ffig2.jpg"></div></p>
<h3 id="SSD-中如何计算-default-box-的大小"><a href="#SSD-中如何计算-default-box-的大小" class="headerlink" title="SSD 中如何计算 default box 的大小"></a><a href="../计算机视觉-SSD-ECCV2016/#SSD 中如何计算 default box 的大小">SSD 中如何计算 default box 的大小</a></h3><p>假如feature maps数量为 $m$, 那么每一个feature map中的default box的尺寸大小计算如下:</p>
<script type="math/tex; mode=display">s_k = s_{min} + \frac{s_{max} - s_{min}}{m-1}(k-1), k\in [1,m]</script><p>上式中, $s_{min} = 0.2 , s_{max} = 0.9$. 对于原文中的设置 $m=6 (4, 6, 6, 6, 4, 4)$, 因此就有 $s = \{0.2, 0.34, 0.48, 0.62, 0.76, 0.9\}$<br>然后, 几个不同的aspect ratio, 用 $a_r$ 表示: $a_r = {1,2,3,1/2,1/3}$, 则每一个default boxes 的width 和height就可以得到( $w_k^a h_k^a=a_r$ ):</p>
<script type="math/tex; mode=display">w_k^a = s_k \sqrt{a_r}</script><script type="math/tex; mode=display">h_k^a = \frac{s_k}{\sqrt {a_r}}</script><p>对于宽高比为1的 default box, 我们额外添加了一个 scale 为 $s_k’ = \sqrt{s_k s_{k+1}}$ 的 box, 因此 feature map 上的每一个像素点都对应着6个 default boxes (<strong>per feature map localtion</strong>).</p>
<h3 id="SSD-使用了哪些数据增广方法"><a href="#SSD-使用了哪些数据增广方法" class="headerlink" title="SSD 使用了哪些数据增广方法?"></a>SSD 使用了哪些数据增广方法?</h3><p>水平翻转, 随机裁剪+颜色扭曲(random crop &amp; color distortion), 随机采集区域块(randomly sample a patch, 目标是为了获取小目标训练样本)</p>
<h3 id="为什么SSD不直接使用浅层的特征图谱-而非要额外增加卷积层-这样不是增加模型的复杂度了吗"><a href="#为什么SSD不直接使用浅层的特征图谱-而非要额外增加卷积层-这样不是增加模型的复杂度了吗" class="headerlink" title="为什么SSD不直接使用浅层的特征图谱, 而非要额外增加卷积层, 这样不是增加模型的复杂度了吗?"></a>为什么SSD不直接使用浅层的特征图谱, 而非要额外增加卷积层, 这样不是增加模型的复杂度了吗?</h3><p>FPN: 理想情况下, SSD 的特征金字塔是从多个卷积层输出的特征图谱得到的, 因此它的计算成本几乎为零. 但是为了避免使用到那些表征能力不强的低阶特征图谱(浅层), SSD 只使用了深层的特征图谱(conv4_3), 同时在 backbone 网络的后面又添加了几层卷积层来提取高表征能力的特征图谱. 但是这样就使得 SSD 错过了那些低阶特征的信息, 这些低阶特征中往往包含了高阶特征不具有的信息, 如小物体的特征信息, 这也是为什么 SSD 对小物体不敏感的原因之一.</p>
<h3 id="SSD-PyTorch-源码实现"><a href="#SSD-PyTorch-源码实现" class="headerlink" title="SSD PyTorch 源码实现"></a>SSD PyTorch 源码实现</h3><p>点击下方链接跳转至源码实现解析<br><a href="../PyTorch-SSD">SSD PyTorch 源码实现解析</a></p>
<p><span id="RefineDet"></span></p>
<h2 id="RefineDet"><a href="#RefineDet" class="headerlink" title="RefineDet"></a><a href="../计算机视觉-RefineDet-CVPR2018">RefineDet</a></h2><h3 id="RefineDet-简介"><a href="#RefineDet-简介" class="headerlink" title="RefineDet 简介"></a>RefineDet 简介</h3><p>One-Stage 比 Two-Stage 精度低的主要原因有三点:</p>
<ol>
<li>正负样本的极度不均衡</li>
<li>bbox 回归不够精细</li>
<li>分类和回归过程比较精简(不像 Faster R-CNN 那样, 先进性 RPN 二分类, 再进行 Detection 多分类)</li>
</ol>
<p>为此, RefineDet 提出将 Two-Stage 模型中的一些思想集成到 One-Stage 框架中, 以此来综合 One-Stage 和 Two-Stage 各自的优势. 具体来说, RefineDet 在 SSD 模型的基础上, 将 SSD 的 forward 流程分成了两大部分, 分别是 ARM(Anchor Refinement Module) 和 ODM(Object Detection Module),  <strong>ARM 的作用有两点</strong>: (1), 移除 negative anchors 解决正负样本不均衡问题 (2), 粗略的调整 anchors 的 locations 和 sizes, 为后续的回归器提供更好地初始 anchors. 而 <strong>ODM 的作用是</strong>: 将 ARM refine 后的 anchor 作为输入, 进一步的提升 bbox reg 和 multi-class pred 的精度. 这两个模块的设计思想分别来自于 Two-Stage 网络的 RPN 网络和 Detection 网络. 与 Two-Stage 的不同之处在于 RefineDet ARM 和 ODM 的具体实现上.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RefineDet/fig1.jpg?x-oss-process=style/blog_img" alt="RefineDet%2Ffig1.jpg"></div></p>
<p>RefineDet 的网络框架设计和 SSD 类似, 首先在常规的 SSD 金字塔特征图谱上(即: conv4_3, conv5_3, conv_fc7, conv6_2) 产生预定义的固定数量的 default boxes, 然后每一个特征图都会共享两个子网络分支, 分别为 anchor 的回归网络和正负样本的二分类预测网络. <strong>这一步产生的负样本置信度高于 0.99 的 anchor 不会传入 ODM 阶段, 起到了预先去除 easy negative examples 的作用.</strong> 然后, 利用一个 Transfer Connection Block 结构将 down-top 的特征图谱和 top-down 的特征图谱结合, 并利用 ARM 产生的 anchor 作为输入, 传入 ODM 中进行第二次的 bbox regression 和物体类别的多分类预测.</p>
<p>Transfer Connection Block 结构如下图所示: ARM 的特征图谱经过两个卷积层, top-down 的特征图谱有反卷积层得到, 二者进行 Eltw sum 后, 输出的图谱再经过一层卷积, 最终产生在 ODM 中进行预测的特征图谱</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RefineDet/fig2.jpg?x-oss-process=style/blog_img" alt="RefineDet%2Ffig2.jpg"></div></p>
<h3 id="refinedet-使用了-two-stage-的边框回归过程-为什么还说它是-one-stage-模型"><a href="#refinedet-使用了-two-stage-的边框回归过程-为什么还说它是-one-stage-模型" class="headerlink" title="refinedet 使用了 two-stage 的边框回归过程, 为什么还说它是 one-stage 模型?"></a>refinedet 使用了 two-stage 的边框回归过程, 为什么还说它是 one-stage 模型?</h3><p>RefineDet 是 One-Stage 和 Two-Stage 的结合<br>虽然有了 anchor refine 的步骤, 但是不想 Faster R-CNN 那样, proposals 的生成和最终 bbox 的预测有很明显的分割, 因此, 勉强算 One-Stage.</p>
<p><span id="RFBNet"></span></p>
<h2 id="RFBNet"><a href="#RFBNet" class="headerlink" title="RFBNet"></a><a href="../计算机视觉-RFBNet-ECCV2018">RFBNet</a></h2><h3 id="RFBNet-简介"><a href="#RFBNet-简介" class="headerlink" title="RFBNet 简介"></a>RFBNet 简介</h3><p>RFBNet 从感受野的角度出发, 提出了利用空洞卷积(Dilated Conv)来构建 RFB(Receptive Field Block), 其核心思想是利用具有不同空洞间距的 Dilated Conv 来集成不同范围的感受野, 以便可以在同一个特征图谱上获得更大尺度范围内的信息</p>
<p>RFB 模块结构如下两图所示</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig2.jpg?x-oss-process=style/blog_img" alt="RFBNet%2Ffig2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig4.jpg?x-oss-process=style/blog_img" alt="RFBNet%2Ffig4.jpg"></div></p>
<p>RFB 与其他具有多尺度感受野的工作之间的区别</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig3.jpg?x-oss-process=style/blog_img" alt="RFBNet%2Ffig3.jpg"></div></p>
<p>RFB Net Detection Architecture</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/RFBNet/fig5.jpg?x-oss-process=style/blog_img" alt="RFBNet%2Ffig5.jpg"></div></p>
<p><span id="TridentNet"></span></p>
<h2 id="TridentNet"><a href="#TridentNet" class="headerlink" title="TridentNet"></a><a href="../计算机视觉-TridentNet-CVPR2019">TridentNet</a></h2><h3 id="TridentNet-简介"><a href="#TridentNet-简介" class="headerlink" title="TridentNet 简介"></a>TridentNet 简介</h3><p>RFBNet 虽然考虑了 Dilated Conv, 但是它的 RFB 结构较为复杂, 带来的计算量也很高. 为此, TridentNet 从不同的角度来使用 Dilated Conv, 对于目标检测问题来说, 不同的特征图谱感受野对于不同的物体尺寸, 其检测效果是不同的, 简单来说, 更大的 receptive field 对于大物体的性能会更好, 更小的 receptive filed 对于小物体的性能会更好. 因此, TridentNet 提出了 Trident Block 结构, 它具有以下三个特点</p>
<ol>
<li>构造了多路并行的 Trident 分支, 每个 Trident Block 分支具有不同的感受野大小, 用于检测不同尺度的物体</li>
<li>在每一个 Trident Block 当中, 它们的参数是共享的, 区别仅在于去感受野的大小不同;</li>
<li>对于每一个 branch, 训练和测试都只负责一定尺度范围内的样本(借鉴 SNIP), 这样避免了极端 scale 对检测性能的影响.</li>
</ol>
<p>补充: 实际上, 在测试阶段，我们可以只保留一个branch来近似完整TridentNet的结果，后面我们做了充分的对比实验来寻找了这样single branch approximation的最佳setting，一般而言，这样的近似只会降低0.5到1点map，但是和baseline比起来不会引入任何额外的计算和参数。</p>
<p><span id="M2Det"></span></p>
<h2 id="M2Det"><a href="#M2Det" class="headerlink" title="M2Det"></a><a href="../计算机视觉-M2Det-AAAI2019">M2Det</a></h2><p>M2Det 从特征金字塔的构建角度出发, 认为现有的 sota 的特征金字塔的构建方式存在两个缺点, 一是直接简单利用了 backbone 网络固有的金字塔式的特征图谱来构建, 但这些 backbone 实际上是针对分类任务设计的, 因此不足以表示针对目标检测任务的特征. 二是构建的金字塔中每一个尺度的特征仅仅来自于 backbone 中单一层级(level)的特征. 这样一来, 小尺度的特征图谱往往缺少浅层低级的语义信息, 而大尺度的特征图谱又缺少深层的高级语义信息(同尺寸的物体可能本身所需的语义层级不同, 如人需要深层的高级语义信息才能识别, 而交通灯只需要浅层的低级语义信息就可以识别). 因此, 作者就提出了融合多个层级特征的 MLFPN (multi-level FPN).  MLFPN 主要由三个模块组成, 分别是: 特征融合模块(Feature Fusion Module, FFM), 简化的 U-shape 模块(Thinned U-shape Module, TUM), 以及尺度特征聚合模块(Scale-wise Feature Aggregation Module, SFAM). <strong>首先</strong>, FFMv1 融合了 backbone 网络中浅层和深层的特征来生成 base feature, 具体来说就是 VGGNet 的 conv4_3 和 conv5_3. <strong>其次</strong>, 若干个 TUMs 和 FFMv2 交替连接. 具体的说, 每一个 TUM 都会生成多个不同尺度的 feature maps. FFMv2 融合了 base feature 和前一个 TUM 输出个最大的 feature map. 融合后的 feature maps 会被送到下一个 TUM 中.  最终, SFAM 会通过按照尺度划分的特征连接操作(scale-wise feature concatenation operation)和通道注意力机制(channel-wise attention mechanism)来聚集 multi-level multi-scale features, 形成最终的特征金字塔结构. 最后用两个卷积层在特征金字塔上分别进行回归和分类预测. 可以看出, 整体的流程和 SSD 类似, 不同之处就在于特征金字塔的构建方式.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/M2Det/fig1.jpg?x-oss-process=style/blog_img" alt="M2Det%2Ffig1.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/M2Det/fig2.jpg?x-oss-process=style/blog_img" alt="M2Det%2Ffig2.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/M2Det/fig4.jpg?x-oss-process=style/blog_img" alt="M2Det%2Ffig4.jpg"></div></p>
<p><span id="YOLOv1"></span></p>
<h2 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a><a href="../计算机视觉-YOLOv1-CVPR2016">YOLOv1</a></h2><p><a href="http://caffecn.cn/?/question/1842" target="_blank" rel="noopener">http://caffecn.cn/?/question/1842</a></p>
<p>YOLOv1 首先将图像分成 $S\times S$ 的格子(cell), 如果一个目标物体的中心落入格子, 那么该格子就负责检测该目标. 每一个格子都会预测 B 个 bbox, 每一个 bbox 包含 5 个值, 分别是坐标和置信度(表示是否包含物体). YOLOv1 的损失函数综合了坐标, 分类标签和分类置信度三部分, 都使用了平方和损失进行计算, 并且通过不同的权重系数平衡了 loss 之间的贡献度.</p>
<p>YOLOv1 的缺点: YOLO 的每一个网络只预测两个 boxes 和一套分类概率值(供两个 boxes 共享), 这导致模型对相邻目标的预测准确率下降, 因此, YOLO 对成群的目标识别准确率低</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/YOLOv1/fig3.jpg?x-oss-process=style/blog_img" alt="YOLOv1%2Ffig3.jpg"></div></p>
<p>从图中可以看出, YOLO网络的输出网格是 7×7 大小的, 另外, 输出的channel数目30, 在每一个cell内, 前20个元素是每个类别的概率值, 然后2个元素对应2个边界框的置信度, 最后8个元素时2个边界框的 $(x,y,w,h)$.(每个cell会预测两个框, 最后选择IOU较大的来复杂物体的预测)</p>
<p><strong>PS:</strong><br>注一: YOLO中采用 $S\times S$ 的网格划分来确定候选框, 这实际上是一种很粗糙的选框方式, 同时也导致了YOLO在面对小目标物以及群落目标物时, 性能较差.(因为YOLOv1的同一个cell无法预测多个目标, 也就是说YOLOv1理论上最多检测出49个物体).</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/YOLOv1/fig2.jpg?x-oss-process=style/blog_img" alt="YOLOv1%2Ffig2.jpg"><br><span id="YOLOv2"></span></div></p>
<h2 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a><a href="../计算机视觉-YOLOv2-CVPR2017">YOLOv2</a></h2><p>YOLOv2(也叫做 YOLO9000)</p>
<p>YOLOv1 对于 bbox 的定位不是很好, 同时在精度上和同类网络还有一定差距, 所以 YOLOv2 对于速度和精度都做了很大的优化, 并且吸收了同类网络的有点, 主要包括以下几点:</p>
<ul>
<li><strong>提高图片分辨率</strong>: 将预训练阶段的输入图片的分辨率扩大到 $448\times 448$, mAP 提升了 4%.</li>
<li><strong>使用 BN</strong>: 利用 BN 起到正则化作用, 从而舍弃了 dropout 层, mAP 提升了 2.4%</li>
<li><strong>引入 anchor</strong>: 在检测时使用了 $416\times 416$ 的图片大小, YOLOv2模型下采样的总步长是 32, 因此最终得到的特征图大小为 $13\times 13$, 维度控制成奇数, 这样特征图谱恰好有一个中心位置, 对于一些大物体, 它们中心点往往落入图片中心位置, 此时使用特征图的一个中心点来预测这些物体的边界框相对容易些. <strong>YOLOv1 上一个 cell 的两个 boxes 共用一套分类概率值, 而 YOLOv2 的每个 anchor box 都会单独预测一套坐标, 一个置信度, 和一套分类概率值.</strong> (这和 SSD 类似, 不过 SSD 没有预测置信度, 而是将背景作为一个类进行处理). <strong>YOLO 中一个 anchor 只会与一个 gt 匹配, 这与 Faster R-CNN 和 SSD 有所区别.</strong></li>
<li><strong>用 k-mean 来确定 anchor 的初始形状</strong>: 根据训练集的聚类分析结果, 选取 $k$ 个聚类中心作为 anchor 的初始形状设置.</li>
<li><strong>Direct location prediction</strong>: Faster R-CNN 中的偏移公式是无约束的, 这样预测的边界框就可能落在图片的任何位置, 这导致模型训练时的不稳定性, 需要训练很长时间才能预测出正确的偏移量. 因此, YOLOv2 沿用了 YOLOv1 的方法, 预测的是边界框中心相对于网格单元位置的位置坐标. 综合 anchor + kmean + driect, mAP 提升了 5%.</li>
<li><strong>利用 ResNet 的 identity mapping 获得细粒度的特征图谱(多尺度图谱方法)</strong>: YOLOv2 的输入图片大小为 $416\times 416$, 最终 max pooling 以后得到 $13\times 13$ 大小的特征图谱, 这样的图谱预测大物体相对足够, 但是对于小物体还需要更精细的特征图, 因此 YOLOv2 利用 identity mapping 的思想将前一段的特征图谱 $26\times 26 \times 512$ reshape 成 $13\times 13 \times 2048$ 的特征图谱, 然后与原来的 $13\times 13 \times 1024$ 的特征图谱连接在一起形成 $13\times 13 \times 3072$ 的特征图谱, 然后在此特征图谱上进行预测. 该方法提升 mAP 1%.</li>
<li><strong>Multi-Scale Training</strong>: 在训练过程中, 每隔一定(10)的 iterations 之后就随机改变输入图片的大小, 图片大小为一系列 32 倍数的值(因为总的 stride 为 32): {320, 352, …, 608}.</li>
<li>Darknet-19: YOLOv2 采用了一个新的模型(特征提取器), 包括 19 个卷积层和 5 个 maxpooling 层(卷积层分配为 1,1,3,3,5,5+1). <strong>Darknet-19 与 VGG16 模型的设计原则是一致的, 主要采用 $3\times 3$ 卷积和 $2\times 2$ 的 maxpooling 层.</strong> Darknet-19 最终采用 global avgpooling 做预测, 并且在 $3\times 3$ 卷积之间利用 $1\times 1$ 卷积来压缩特征图的通道数以降低模型的计算量和参数.</li>
</ul>
<p>YOLO 利用 anchor 会生成 $13\times 13\times 5 = 845$ 个候选区域框, 相比于YOLOv1的98个, 多多了, 所以会大大提高召回率, 但是会降低准确率. 下降的原因, 个人觉得是YOLO在采用anchor box获取候选框的同时, 依然采用YOLOv1的训练方法, YOLOv2的损失函数是一个非常复杂的形式, 导致其在更新参数时很容易顾不过来, 因此其出错的概率也相应提升.</p>
<p>YOLOv2的训练包含三个阶段: 预训练(224), finetune(448), 检测模型训练</p>
<p><strong>对样本的处理方式:</strong> 和 YOLOv1 相同, 对于训练图片中的 ground truth, 如果其中心落在了某个 cell 内, 那么该 cell 内的 5 个 anchor 就负责预测该物体, 并且最终只有一个边界框会与之匹配, 其他的会被 NMS 掉. 所以 YOLOv2 同样假定每个 cell 至多含有一个 ground truth, 而在实际上基本不会出现多于一个的情况. <strong>与 gt 匹配的 anchor 会计算坐标误差, 置信度误差, 和分类误差, 而其他的边框则只计算置信度误差.</strong></p>
<p><strong>损失函数</strong> YOLOv2 的 loss 形式如下:</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/YOLOv2/loss.jpg?x-oss-process=style/blog_img" alt="YOLOv2%2Floss.jpg"></div></p>
<p><span id="YOLOv3"></span></p>
<h2 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a><a href="../计算机视觉-YOLOv3-Arxiv2018">YOLOv3</a></h2><p>YOLOv3 加入了更多被验证过的有效技术, 使得 YOLO 模型的 mAP 可以与 SSD 系列相媲美, 同时速度依然很快(约为 SSD 的三倍). YOLOv3 的改进主要如下:</p>
<ul>
<li><strong>Bounding Box Prediction</strong>: YOLOv3 使用了和 YOLOv2 相同的 bbox 回归策略, 都是预测相对于 cell 左上角的偏移量进行回归. <strong>YOLO 中每个 gt box 只会与负责它的 cell 中的一个 anchor box 匹配(IoU 最大), 其他的 anchor box 只会计算 objectness 置信度损失, 而不会计算坐标和分类损失.</strong></li>
<li><strong>类别预测</strong>: 由于有的物体可能不止属于一个标签, 如 “人” 和 “女人”. 因此, 为了减少类别之间的对抗性, YOLOv3 没有使用 softmax 计算分类损失, 而是采用了 <strong>二值交叉熵</strong> 来预测类别(属于某类或其他类).</li>
<li><strong>采用特征金字塔</strong>: 使用类似于 FPN 的方法(upsample and top-down)提取到不同尺度的特征图谱(文中使用了3个), 在每个尺度的特征图谱上的都会预测三个boxes, 因此, 每个尺度的特征图谱的输出为: $N\times N \times [3\times (4+1+80)]$. 在确定 anchor 大小时, 利用 kmean 聚类选出 9 个聚类中心, 然后划分成 3 个簇分别赋给 3 个尺度的特征图谱.</li>
<li><strong>Darknet-53</strong>: 使用了残差模块的思想, 提出了层数为 53 的 Darknet-53 网络 (1, 2, 8, 8, 4). 在 ImageNet 上, Darknet-53 is better than ResNet-101 and 1.5x faster. Darknet-53 has similar performance to ResNet-152 and is 2x faster.</li>
</ul>
<p>在使用了 anchor 和 multi-scale 之后, YOLOv3 在小物体的检测上有所改善.<br>Focal Loss 对 YOLOv3 不起作用的原因可能是 YOLO 的选框策略和损失函数(objectness+cls)使得 YOLO 在背景样本不均衡问题上影响较小.</p>
<h3 id="YOLO-系列为什么速度这么快"><a href="#YOLO-系列为什么速度这么快" class="headerlink" title="YOLO 系列为什么速度这么快?"></a>YOLO 系列为什么速度这么快?</h3><p>首先 YOLO 是 one-stage 的, 它对 bbox 的分类和回归同时进行的, 其次, 在不使用 anchor 时, YOLO 是 anchor free 的, 他所需的计算量远远小于基于 anchor 的 Faster R-CNN 和基于 Default Box 的 SSD.</p>
<p><span id="OHEM"></span></p>
<h2 id="OHEM"><a href="#OHEM" class="headerlink" title="OHEM"></a><a href="../计算机视觉-OHEM-CVPR2016">OHEM</a></h2><p><strong>提出了一种在线的难样例挖掘算法:</strong><br>作者根据每个 RoI 的 loss 的大小来决定哪些是难样例, 哪些是简单样例, 通过这种方法, 可以更高效的训练网络, 并且可以使得网络收敛到更好的解. 同时, OHEM 还具有以下两个优点:</p>
<ul>
<li><strong>消除FastR-CNN系列模型中的一些不必要这参数</strong> , 这些参数大多都是为了解决难样例问题服务的, 在使用 OHEM 以后, 不仅无需在对这些超参数进行调优, 同时还能获得更好的性能表现.</li>
<li><strong>OHEM算法可以与其他多种提升模型精度的trick相结合</strong>, 对于大多数模型(R-CNN系列), 在使用了OHEM以后, 都能够获得精度上的提高, 可以看做是一种普适性的提升精度的方法.</li>
</ul>
<p>注: 在实现OHEM上, 作者为了提升速度和效率, 特意设计了两个RoI网络, 以减少无用的计算.</p>
<p><span id="Focal Loss"></span></p>
<h2 id="Focal-Loss-1"><a href="#Focal-Loss-1" class="headerlink" title="Focal Loss"></a><a href="../计算机视觉-FocalLoss-ICCV2017">Focal Loss</a></h2><h3 id="Focal-Loss-简介"><a href="#Focal-Loss-简介" class="headerlink" title="Focal Loss 简介"></a>Focal Loss 简介</h3><p>one stage 方法由于没有对候选框的预先过滤策略, 因此后产生大量的 easy negative examples(易分类的背景区域), 这些 easy negative examples 由于数量众多, 最终会对loss有很大的贡献, 从而导致优化的时候过度关注这些 easy examples, 这样会收敛到一个不够好的结果.</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FocalLoss/fig1.jpg?x-oss-process=style/blog_img" alt="FocalLoss%2Ffig1.jpg"></div></p>
<p>Focal Loss 正是为了解决这个问题而提出的, 它的核心思想非常直接, 就是在优化的过程中抑制这些 easy (negative) examples 对最终 Loss 的贡献程度. 所谓 easy example 指的就是那些预测概率与真实概率十分相近的样本. 这些样本已经被网络很容易的正确分类了, 所以应该适当减少他们的loss以降低他们对参数更新的影响程度. 以下面的公式为例, 当真实标签为1时, 如果预测概率(假设二分类) $p$ 接近于1, 则此样本是easy样本, 因此, 前面的 $(1-p)^{\gamma}$ , 就会非常小, 起到了抑制易分类样本的作用.</p>
<script type="math/tex; mode=display">FL(p) = \begin{cases} -(1-p)^{\gamma}log(p) & 当y=1 \\ -p^{\gamma}log(1-p) & 当y=0 \end{cases}</script><p>对上式可以进行如下简写:</p>
<script type="math/tex; mode=display">p_t = \begin{cases} p & \text{if y = 1} \\ 1-p & \text{otherwise} \end{cases} \tag 2</script><script type="math/tex; mode=display">FL(p_t) = -(1-p_t)^{\gamma}log(p_t) \tag 4</script><p><strong>注1:</strong> $\gamma$ 的值越大, 则 easy example 对于loss的贡献程度越小, 当 $\gamma = 0$ 时, 会退化成普通的交叉熵函数.</p>
<p><strong>注2:</strong> 实际中在使用 $\gamma$ 参数的情况下, 还会用了另一个参数 $\alpha$ , 该参数的主要作用是调节正负样本的权重, $\alpha$ 的取值通常会根据正负样本的实际比例决定, 如下所示:</p>
<script type="math/tex; mode=display">FL(p) = \begin{cases} -\alpha (1-p)^{\gamma}log(p) & 当y=1 \\ -(1-\alpha) p^{\gamma}log(1-p) & 当y=0 \end{cases}</script><p>通常也可缩写为:</p>
<script type="math/tex; mode=display">FL(p_t) = -\alpha_t(1-p_t)^\gamma log(p_t) \tag 5</script><p>下图可以看出, 对于易分类样本来说, 绝大部分都是负样本, 因为 FocalLoss 对于正样本的抑制效果并不明显, 而对负样本的抑制效果会随着 $\gamma$ 参数的升高而迅速增大</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FocalLoss/fig4.jpg?x-oss-process=style/blog_img" alt="FocalLoss%2Ffig4.jpg"></div></p>
<h3 id="Focal-Loss-的两个参数各自的作用是什么-二者有什么关系"><a href="#Focal-Loss-的两个参数各自的作用是什么-二者有什么关系" class="headerlink" title="Focal Loss 的两个参数各自的作用是什么? 二者有什么关系?"></a>Focal Loss 的两个参数各自的作用是什么? 二者有什么关系?</h3><p>$\gamma$ 决定了对 easy example 的抑制力度, 当 $\gamma$ 的值越大时, 则 easy example 对于 loss 的贡献程度就越小, 而 $\gamma$ 的值为 0 时, 相对于不施加抑制, Focal Loss 推退化成普通的交叉熵函数</p>
<p>$\alpha$ 的作用是调节正负样本的权重, $\alpha$ 的取值通常会根据正负样本的实例比例决定. 对于样本数量较多的类别, 其对应的权重应该较小. <strong>但是当 $\alpha$ 参数和 $\gamma$ 参数共同使用时, 则需要综合考虑样本的数量和 $\gamma$ 的取值</strong></p>
<p>二者关系: 当 $\gamma$ 取值较小时, FocalLoss 对易分类样本的抑制力度不强, 此时由于存在大量的易分类负样本, 因此 $\alpha$ 应当对正样本施加更多的权重($\gamma = 0$ 时, $\alpha$ 最优值为 0.75). 当 $\gamma 取值较小时$ Focal Loss 对易分类负样本抑制力度变强, 使得原本在数量上不在优势的前景区域或许在对 loss 的贡献度上反超了背景区域, 因此, 需要对前景区域赋予更低的权重.$\gamma = 0$ 时, $\alpha$ 最优值为 0.25$</p>
<h3 id="R-CNN-系列为什么不用-Focal-Loss-进行优化"><a href="#R-CNN-系列为什么不用-Focal-Loss-进行优化" class="headerlink" title="R-CNN 系列为什么不用 Focal Loss 进行优化"></a>R-CNN 系列为什么不用 Focal Loss 进行优化</h3><p>因为 Focal Loss 主要解决的问题是正负样本框的极度不均衡问题.<br>而 Two-Stage 网络, 通常会利用 RPN 和启发式的采样算法来过滤掉大量的负样本, 因而不存在正负样本不均衡的问题.</p>
<ul>
<li>RPN 会将物体候选框的数量降低很多, 移除了大量的易分类负样本(背景框)</li>
<li>采用了 biased-minibatch 的采样策略, 比如, 保证正样本和负样本的比例为 1:3 进行训练 (这其实相等于起到了 $\alpha$ 因子的作用)</li>
</ul>
<h3 id="RetinaNet-的结构"><a href="#RetinaNet-的结构" class="headerlink" title="RetinaNet 的结构"></a>RetinaNet 的结构</h3><p>RetinaNet 由一个backbone网络和两个子网络组成。backbone网络是现成的，主要负责计算卷积特征图谱。第一个子网络负责物体分类任务，第二个子网络负责bounding box回归任务，它们都是在backbone网络输出的卷积图谱上进行计算的。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/FocalLoss/fig3.jpg?x-oss-process=style/blog_img" alt="FocalLoss%2Ffig3.jpg"></div></p>
<ul>
<li><strong>Backbone:</strong> RetinaNet 选用 FPN 作为 backbone 网络, 对P3到P7使用了不同大小的anchors.</li>
<li><strong>分类子网:</strong> 由一个简单的 FCN 组合, <strong>分类子网的参数在所有的金字塔层级都是共享的</strong>，对于规定的金字塔层级, 其输出的特征图谱的通道数为 $C$, 则分类子网由 4 个 3x3 的卷积层构成, 每个卷积层的输入输出通道数都是 $C$, 尺寸大小维持不变, 并且均使用 ReLU 作为激活函数, 最后接上一层输出通道数为 $KA$ 的 3x3 卷积, 并利用 Sigmoid 激活函数在每个 location 上输出 $KA$ 的二值预测概率. 通常情况下 $C=256, A=9$</li>
<li><strong>回归子网:</strong> 整体结构与分类子网相同, 唯一区别在于最后一层卷积层的输出通道数变成了 $4A$, 因为它要在每一个 location 上为每一个 anchor 预测 4 个坐标偏移量. <strong>回归子网的参数在金字塔层级上也是共享的</strong>. <strong>注意, 从回归子网的通道数就可以看出, RetinaNet 的 bbox 预测是类别不可知(class-agnostic)的</strong></li>
</ul>
<p><span id="DenseBox"></span></p>
<h2 id="DenseBox"><a href="#DenseBox" class="headerlink" title="DenseBox"></a><a href="../">DenseBox</a></h2><p><span id="CornerNet"></span></p>
<h2 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a><a href="../">CornerNet</a></h2><p>在 CornerNet 中, 舍弃了 anchor 的设定, 转而利用一对角点来表示物体, 卷积网络会预测输出两组 heatmap, 分别代表了 bbox 的左上角和右下角. 为了能够很好的将角点进行分组, CornerNet 同时还会为每个 corner 预测一个 embedding vector, 当一组角点的 embedding vector 的点积越小时, 则说明这组角点被分到同一组的可能性更高. 如果距离小于阈值，则生成对象边界框。为边界框分配置信度分数，该分数等于 corner pairs 的平均分数。</p>
<p>CornerNet 会预测两组热图, 每一组热图都具有 $C$ 个通道, $C$ 代表了物体类别的数量, 热图的尺寸为 $H\times W$. <strong>注意, 这里没有背景的通道(background channel)</strong>. 每一个 channel 都是一个二值的 mask, 用于指示某个类别的角点位置.<br>对于每个角点来说, 都会有一个 gt positive location, 而其他的 locations 都将是负样本. 由于 gt positive location 周围的像素点比其它的像素点更接近 gt, 所以, 我们不能对所有的负样本给予同样的标签, 因此, CornerNet 使用了一个非规范的高斯分布来产生对应的 GT 样本, 其正中心位于 gt positive location, $\sigma$ 的值根据 GT 物体的大小来确定, 大约是物体半径的三分之一.</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet/fig1.jpg?x-oss-process=style/blog_img" alt="CornerNet%2Ffig1.jpg"></div></p>
<p>为了帮助卷积网络更好的定位 bbox 的 corner, CornerNet 提出了 Corner Pooling Layer. 对于给定的特征图谱, Corner Pooling 为分别进行自下而上的 max pooling 和自右向左的 max pooling, 然后将二者的结果相加, 即可得到左上角的 pooling 结果, 对于右下角的 pooling 操作也是类似的.</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet/fig3.jpg?x-oss-process=style/blog_img" alt="CornerNet%2Ffig3.jpg"></div></p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet/fig6.jpg?x-oss-process=style/blog_img" alt="CornerNet%2Ffig6.jpg"></div></p>
<h3 id="检测-Corner-相对于检测-bbox-的优势"><a href="#检测-Corner-相对于检测-bbox-的优势" class="headerlink" title="检测 Corner 相对于检测 bbox 的优势"></a>检测 Corner 相对于检测 bbox 的优势</h3><ol>
<li>定位 anchor box 更难一些, 因为它需要依赖物体的4个边界才能确定, 但是 corner 只需要物体的两个边界就可以确定, corner pooling 也是如此, 它编码了关于 corner 的一些明确的先验知识.</li>
<li>Corners 提供了一种更加有效的方式来密集的对 boxes 的空间进行离散化: 我们只需要 $O(wh)$ 的 corners, 而 anchor boxes 需要的复杂度是 $O(w^2h^2)$.</li>
</ol>
<p><span id="CornerNet-Lite"></span></p>
<h2 id="CornerNet-Lite"><a href="#CornerNet-Lite" class="headerlink" title="CornerNet-Lite"></a><a href="../计算机视觉-CornerNetLite-Arxiv2019">CornerNet-Lite</a></h2><p>CornerNet-Lite 描述<br>基于关键点的对象检测[53,56,26]是一类通过检测和分组其关键点来生成对象边界框的方法。 CornerNet [26]是其中最先进的技术，可以检测并分组边界框的左上角和右下角; 它使用堆叠 HourGlass 网络[39]来预测角落的热图，然后使用 associate embeddings [38]对它们进行分组。 CornerNet 允许简化设计，无需使用 anchor box[46]，并且在 One-Stage 探测器中实现了COCO [32]的 SOTA 精度。</p>
<p>CorNetNet 虽然精度较高, 但是其计算量的高处理成本也是它的一个明显的缺点. 为此, CornerNet-Lite 提出了两种变体, 来改善计算量:</p>
<ol>
<li>CornerNet-Saccade: 使用 <strong>缩小的图像</strong> 来预测 3 个 attention maps, 分别用于小, 中, 大物体. attention 模块通过 3x3 conv-relu + 1x1 conv-sigmoid 来实现, 然后利用获得的粗糙的 locations 信息, 从原始图像中截取相应大小的 crop 进行精细化的预测, 对于不同大小的物体, 通常具有不同的放大比例, 小对象需要放的更大些, 中对象次之, 大对象最后. 通过这种方式, 大幅降低了 location 的采样空间. 加速了检测速率.</li>
</ol>
<p>CorNetNet 虽然精度较高, 但是其计算量的高处理成本也是它的一个明显的缺点. 为此, CornerNet-Lite 提出了两种变体, 来改善计算量:</p>
<ol>
<li>CornerNet-Saccade: 使用 <strong>缩小的图像</strong> 来预测 3 个 attention maps, 分别用于小, 中, 大物体. attention 模块通过 3x3 conv-relu + 1x1 conv-sigmoid 来实现, 然后利用获得的粗糙的 locations 信息, 从原始图像中截取相应大小的 crop 进行精细化的预测, 对于不同大小的物体, 通常具有不同的放大比例, 小对象需要放的更大些, 中对象次之, 大对象最后. 通过这种方式, 大幅降低了 location 的采样空间. 加速了检测速率.</li>
<li>CornerNet-Squeeze, 主要改进如下:<ol>
<li><strong>受 SqueezeNet 启发:</strong> 将 Residual Block 替换成 Fire Module(1x1 + 1x1 &amp; 3x3)</li>
<li><strong>受 MobileNet 启发:</strong> 利用 3x3 的 Dwise Conv 替换 Fire Module 中第二层的 3x3 卷积<br><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CornerNet-Lite/tab1.jpg?x-oss-process=style/blog_img" alt="CornerNet-Lite%2Ftab1.jpg"></div></li>
</ol>
</li>
</ol>
<h3 id="CornerNet-Squeeze-Saccade"><a href="#CornerNet-Squeeze-Saccade" class="headerlink" title="CornerNet-Squeeze-Saccade"></a>CornerNet-Squeeze-Saccade</h3><p>当二者同时使用时, 模型本身的容量被限制过多, 使得模型无法在检测物体的同时良好的预测 attention map, 因此效果不好.</p>
<p><span id="FSAF"></span></p>
<h2 id="FSAF"><a href="#FSAF" class="headerlink" title="FSAF"></a><a href="../计算机视觉-FSAF-CVPR2019">FSAF</a></h2><h3 id="FSAF-简介"><a href="#FSAF-简介" class="headerlink" title="FSAF 简介"></a>FSAF 简介</h3><p><strong>有了 FSAF, 就不需要为 anchor 定义 anchor scale 和 aspect ratio 了, 金字塔层级的选择由 FASF 决定</strong></p>
<p>以 RetinaNet 为基础，<strong>FSAF模块仅为每个金字塔层引入两个额外的conv层</strong>，如图4中虚线 feature maps 所示。这两个网络层分别负责 anchor free 分支的分类和回归预测。<strong>更具体地说，我们会在分类子网中最后的 feature map 上附加了一个带有 $K$ 个 filter 的 3×3 conv 层，后面是 sigmoid 函数, 用于二分类，该网络层与 anchor based 分支的网络层并行。它为K个对象类预测对象会出现在每个空间位置的概率。同样的，回归子网中的 feature map 上也附加了一个 3×3 conv 层，带有 4 个filter，然后是ReLU[26]函数。它负责预测以 anchor free 方式编码的 bbox 偏移量。</strong> 因此，anchor free 和 anchor based 分支可以以多任务的方式联合工作，共享每个金字塔级别的特性。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FSAF/fig4.jpg?x-oss-process=style/blog_img" alt="FSAF%2Ffig4.jpg"></div></p>
<h3 id="Groud-truch-and-Loss"><a href="#Groud-truch-and-Loss" class="headerlink" title="Groud-truch and Loss"></a>Groud-truch and Loss</h3><p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FSAF/fig5.jpg?x-oss-process=style/blog_img" alt="FSAF%2Ffig5.jpg"></div></p>
<ul>
<li><strong>Classification Output:</strong> 针对分类输出的 gt 是 $K$ 个 maps，每个 map 对应一个类。每个实例以三种方式影响第 $k$ 个 gt map:<ol>
<li>物体的中心区域(bbox 的 0.2 倍)被定义为 positive region(白色), 表示该实例的存在性。</li>
<li>物体中心区域外围的部分区域(bbox 的 0.5 倍再去除中央部分)被定义为 ignoring region(灰色) <strong>这意味着该区域的梯度不会传播回网络</strong>。其余区域由黑色填充, 表示没有对象</li>
<li>相邻金字塔层 $(b^{l-1}_i, b^{l+1}_i)$ 中的 ignoring region 如果存在，那么在当前层级上也将忽略该区域。<br>注意，如果两个实例的 effective box 在一个特征级别上重叠，则较小的实例具有更高的优先级。我们使用 Focal Loss 作为损失函数进行监督(正负样本), 超参数 $\alpha = 0.25$, $\gamma = 2.0$。图像 anchor free 分支的分类总损失是所有非忽略区域上的 Focal Loss 之和，由 <strong>所有 effective box 区域内的像素总数归一化</strong></li>
</ol>
</li>
<li><strong>Box Regression Output:</strong> 回归输出的 gt 是 4 个与类别无关的 offset maps。实例只影响 offset maps 上的 effective 区域。<strong>对于 effective region 中的每一个像素 location $(i, j)$</strong>, 我们将其映射成一个四维向量, 分别表示当前像素和 origin region 顶, 左, 右, 底之间的距离. 采用 IoU Loss 进行优化, <strong>并且只考虑 effective region 内的损失.</strong> 将4个 offset maps 上(i, j)处的四维向量设置为 $d^l_{i,j}/S$，每个 map 对应一个维度。$S$ 是一个归一化常量，我们根据经验选择 $S = 4.0$</li>
</ul>
<p>在 Inference 过程中，很容易从分类和回归输出中解码 predicted boxes。在每个像素位置 $(i, j)$,假设 predicted offsets 为$[\hat o_{t_{i,j}}, \hat o_{l_{i,j}}, \hat o_{b_{i,j}}, \hat o_{r_{i,j}}]$。然后 predicted distances 为 $[S\hat o_{t_{i,j}}, S\hat o_{l_{i,j}}, S\hat o_{b_{i,j}}, S\hat o_{r_{i,j}}]$。predicted projected box 左上角和右下角分别为 $i-S\hat o_{t_{i,j}}, j-S\hat o_{l_{i,j}}$ 和 $i+S\hat o_{b_{i,j}}, j+S\hat o_{r_{i,j}}$。我们进一步将投影框放大 $2^l$，得到图像平面中的最终框。框的置信度和类别由分类输出图上位置 $(i, j)$ 处的 $k$ 维向量的最大得分和对应的类决定。</p>
<h3 id="Online-Feature-Selection"><a href="#Online-Feature-Selection" class="headerlink" title="Online Feature Selection"></a>Online Feature Selection</h3><p>在训练阶段, 每个实例都会通过特征金字塔所有层级 $l$ 的 forward 计算. 在此基础上，对所有 anchor free 支路计算 $L^I_{FL}(l)$ 和 $L^I_{IoU}(l)$。最后，<strong>我们选择产生最小损失和的金字塔级 $P_l$ 作为学习实例的最佳目标层级</strong>，即</p>
<script type="math/tex; mode=display">l^* = \arg \min_l L^I_{FL}(l) + F^I_{IoU}(l)</script><p><strong>然后, 我们只更新这个最佳目标层级上的梯度, 这就达到了在线选择特征的目的</strong></p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FSAF/fig6.jpg?x-oss-process=style/blog_img" alt="FSAF%2Ffig6.jpg"></div></p>
<p><strong>在 inference 阶段, 我们不需要进行特征选择来决定金字塔层级, 因此最合适的特征金字塔自然会输出较高的置信度分数, 我们只需取最大者即可.</strong></p>
<p>FASF 和 Anchor-based 结合使用时, 能有更好的效果. 增加的计算量很少, 只有 $l*2$ 个卷积层. 二者再结合时, 会合并各自的 box predictions, 然后利用阈值为 0.5 的 NMS, 得到最终的检测结果.</p>
<p><span id="FoveaBox"></span></p>
<h2 id="FoveaBox"><a href="#FoveaBox" class="headerlink" title="FoveaBox"></a><a href="../计算机视觉-FoveaBox-CVPR2019">FoveaBox</a></h2><h3 id="FoveaBox-简介"><a href="#FoveaBox-简介" class="headerlink" title="FoveaBox 简介"></a>FoveaBox 简介</h3><p>FoveaBox 整体架构基于 RetinaNet. 由一个 backbone 网络和两个 subnet 组成, FoveaBox 的输出通道数是 RetinaNet 的 $1/A$</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FoveaBox/fig4.jpg?x-oss-process=style/blog_img" alt="FoveaBox%2Ffig4.jpg"></div></p>
<p>它的主要特点有两个:</p>
<ol>
<li>class 分支负责预测类别敏感的语义图, 将其看做是目标存在的概率. score map 上的 positive area 定义为原始四边形的缩小版(见图3). $R^{pos} = (x_1’’, y_1’’, x_2’’, y_2’’)$. 其中 $\sigma_1$ 是收缩因子。 在训练的时候, positive area 内的每个 cell 都用相应的目标类标签进行标注以进行训练。 对于负样本的定义，我们引入另一个缩小因子 $\sigma_2$ 并使用等式(4)生成 $R^{neg}$。negative area 是整个 feature map 中不包括 $R^{neg}$ 的区域(<strong>注意, 这里没有写错, 通常 $R^{neg}$ 会比 $R^{pos}$ 大一圈, 而除了这两处之外的其他地方, 皆认为是 background, 即负样本</strong>)。 如果单元格未分配，则在训练期间将忽略该单元格。 <strong>positive area 通常占整个特征图的一小部分，因此我们使用Focal Loss [28]来训练该分支的 target $L{cls}$。</strong></li>
</ol>
<script type="math/tex; mode=display">\begin{cases}x_1'' = c_x' - 0.5(x_2'-x_1')\sigma_1, \\ y_1'' = c_y' - 0.5(y_2' - y_1')\sigma_1, \\ x_2'' = c_x' + 0.5(x_2' - x_1')\sigma_1, \\ y_2'' = c_y' + 0.5(y_2' - y_1')\sigma_1, \end{cases} \tag 4</script><p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FoveaBox/fig3.jpg?x-oss-process=style/blog_img" alt="FoveaBox%2Ffig3.jpg"></div></p>
<ol>
<li>box regression 分支负责</li>
</ol>
<p>FoveaBox 的 box regression 分支学习的是 <strong>中心坐标与 gt box $G = (x1, y1, x2, y2)$ 四条边界之间的距离</strong>. 它首先将 feature map 上的坐标映射到原始图像上, 然后计算出 <strong>归一化(用 $z =\sqrt{S_l}$ 后的偏移量, 并将其映射到对数空间上</strong>, FoveaBox 学习的就是这样的一个 transformations, 具体如下所示. (用 smooth L1 损失训练)</p>
<script type="math/tex; mode=display">\begin{cases}t_{x1} = \log \frac{2^l(x+0.5) - x_1}{z}, \\ t_{y1} = \log \frac{2^l(y+0.5) - y_1}{z}, \\ t_{x2} = \log\frac{x_2 - 2^l(x+0.5)}{z}, \\ t_{y2} = \log\frac{y_2 - 2^l(y+0.5)}{z} \end{cases} \tag 5</script><h3 id="FoveaBox-的-Scale-Assignment"><a href="#FoveaBox-的-Scale-Assignment" class="headerlink" title="FoveaBox 的 Scale Assignment"></a>FoveaBox 的 Scale Assignment</h3><p>在FoveaBox中，每个特征金字塔会学习响应特定尺度的对象。 金字塔等级 $l$ 的目标框的有效比例范围计算为</p>
<script type="math/tex; mode=display">[S_l / \eta^2, S_l \cdot \eta^2] \tag 2</script><p>其中 $\eta$ 是根据经验设置的，以控制每个金字塔的比例范围。注意，<strong>一个对象可能被网络的多个金字塔检测到，这与以前只将对象映射到一个特征金字塔的做法不同(FPN, MaskRCNN).</strong></p>
<h3 id="FoveaBox-与其他模型的比较"><a href="#FoveaBox-与其他模型的比较" class="headerlink" title="FoveaBox 与其他模型的比较"></a>FoveaBox 与其他模型的比较</h3><ul>
<li><p>RetinaNet: FoveaBox 每个位置只会预测 1 个物体, 因此复杂度是 RetinaNet 的 $1/A$</p>
</li>
<li><p>Anchor-based: FoveaBox 没有预设 box 的性状, 因此对于那些性状比较极端的物体来说, 效果更好</p>
</li>
<li><p>RPN: FoveaBox 本身还可以将模型的 head 修改成 class agnostic scheme, 将其作用 region proposals 使用, 可以进一步提升 two-stage 的性能</p>
</li>
<li><p>Guided-Anchoring: FoveaBox 不依赖于 center 进行预测, 而是依赖于每个情景位置的物体上下左右边界, 这对于哪些 (x, y) 不在目标中心的 bbox 来说, 鲁棒性更好.</p>
</li>
<li><p>FASF</p>
<ol>
<li>FASF 的依靠在线特征选择模块为每个实例和锚点选择合适的特征, FoveaBox 依然采用根据尺度进行分配的策略.  个人观点: 是否可以将 FASF 模型加入到 FoveaBox 中, 进一步提升性能</li>
<li>FASF 使用 IoU Loss, FoveaBox 使用 Smooth L1, 后者相对简洁</li>
<li>FoveaBox 整体性能略强于 FSAF</li>
</ol>
</li>
<li><p>CornerNet, CenterNet, ExtremeNet 等:<br>FoveaBox 与其他基于 points 的 anchor-free 模型的一大优势: FoveaBox 直接将每个实例和 bbox 关联在一起, 不需要额外的分组方案来将 points 组合成特定实例.</p>
</li>
</ul>
<p><span id="FCOS"></span></p>
<h2 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a><a href="../计算机视觉-FCOS-CVPR2019">FCOS</a></h2><h3 id="FCOS-简介"><a href="#FCOS-简介" class="headerlink" title="FCOS 简介"></a>FCOS 简介</h3><p>FCOS 利用全卷积网络, 对于特征图谱上到每一点 $(x, y)$, 先将他映射回原始图片中, 差不多刚好位于 $(x, y)$ 的感受野中心附近. 然后, <strong>如果位置 $(x,y)$ 落入到任何 GT Box $B_i$ 内部, 那么就将其视为正样本</strong>, 并且该位置的类标签就是该 GT Box 的类标签 $c^\ast $, 否则它就是负样本并且 $c^\ast = 0$（背景类）。 对于点 $(x, y)$ 的回归目标, 将其定义为当前点当 GT Box 四条边界的距离, 因此，如果位置 $(x,y)$ 与边界框 $B_i$ 相关联，则该位置的训练回归目标可以表示为:</p>
<script type="math/tex; mode=display">\begin{cases}l^{\ast} = x - x^{(i)}_0, t^\ast = y - y^{(i)}_0 \\ r^\ast = x^{(i)}_1 - x, b^\ast = y^{(i)}_1 - y  \end{cases} \tag 1</script><p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig1.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig1.jpg"></div></p>
<h3 id="目标重叠问题"><a href="#目标重叠问题" class="headerlink" title="目标重叠问题"></a>目标重叠问题</h3><p>FCOS 存在的一个问题是如果目标存在重叠, 那么特征图谱上的点就会同时落入多个物体的边界框, 此时这个点就变成了模糊样本, 对于模糊样本, 我们可以利用 FPN 来解决.</p>
<p>与 anchor based detectors 根据物体尺寸决定匹配层级不同，<strong>FCOS 直接限制边界框回归的范围</strong>。 更具体地说，我们首先计算所有特征级别上每个位置的回归值 $l^\ast，t^\ast，r^\ast$ 和 $b^\ast$。 接下来, 根据这个回归值大小与预设的特征级别大小的关系, 将其分配到不同的特征级别上去</p>
<p>由于 <strong>具有不同大小的对象被分配给不同的特征级别</strong> 并且 <strong>大多数重叠发生在具有显著不同大小的对象之间</strong>，因此多级预测可以在很大程度上减轻上述模糊样本带来的干扰.</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/FCOS/fig2.jpg?x-oss-process=style/blog_img" alt="FCOS%2Ffig2.jpg"></div></p>
<p><strong>这里存在一个假设前提, 就是发生重叠的物体尺寸大小必须不同, 只有这样才能将目标分别匹配到不同的金字塔层级上进行预测. 否则, 依然会产生漏检问题.</strong></p>
<h3 id="Center-ness"><a href="#Center-ness" class="headerlink" title="Center-ness"></a>Center-ness</h3><p>FCOS 中, 正样本的数量虽然多, 但是 <strong>远离物体中心的正样本位置会生成大量的低质量预测框</strong>. 为此 FCOS 在分类分支上, 添加一个并行的单层网络分支, 它负责预测每个位置 <strong>处于物体中心的概率(center-ness)</strong>, 具体形式定义为 <strong>从该位置到该位置所负责对象的中心的距离</strong>, 如下所示</p>
<script type="math/tex; mode=display">centerness^\ast = \sqrt{\frac{\min(l^\ast, r^\ast)}{\max(l^\ast, r^\ast)} \times \frac{\min(t^\ast, b^\ast)}{\max(t^\ast, b^\ast)}} \tag 3</script><p>当某个点处于中心位置时, 该位置距离 bbox 的左右, 上下距离相等, 所以 center-ness 的值就接近于 1. 最终每个位置的预测框的置信度 <strong>将通过 center-ness 与类别得分的乘积共同给出</strong>. 这样一来, 可以通过 NMS 来消除大量的低质量预测框</p>
<p>基于 anchor 的检测器使用两个 IOU 阈值 $T_{low}$ 和 $T_{high}$ 将 anchor boxes 标记为负、忽略和正样本，center-ness 可以看作是一个 <strong>软阈值</strong>。它是在网络训练中学习的，不需要调整。此外，利用该策略，我们的检测器仍然可以将任何落在 GT Box 中的位置视为正样本.</p>
<p>可以注意到，也可以使用预测的回归向量计算中心，而不引入额外的中心分支。然而，如表5所示，<strong>从回归向量计算的中心不能改善性能</strong>，因此需要单独学习的中心。</p>
<h3 id="FCOS-与其他模型的比较"><a href="#FCOS-与其他模型的比较" class="headerlink" title="FCOS 与其他模型的比较"></a>FCOS 与其他模型的比较</h3><p>FCOS 对于正样本的定义, 使用在训练时, 正样本的数量可以大幅增加, 很大程度上缓解了正负样本不均衡的问题.</p>
<p><span id="ExtremeNet"></span></p>
<h2 id="ExtremeNet"><a href="#ExtremeNet" class="headerlink" title="ExtremeNet"></a><a href="../ExtremeNet-CVPR2019">ExtremeNet</a></h2><p>对 CornerNet 的一种扩展, ExtremeNet 会预测四个 extreme points(最上, 最左, 最下, 最右) 和一个中心点. 将检测任务当做是关键点估计来做.</p>
<p>ExtremeNet 使用 HourGlass Net 来检测每个类的五个关键点(四个极值点和一个中心)。offset predictions 与类别无关，但与 extreme point 有关。中心点映射没有 offset predictions。因此，ExtremeNet 的输出为 $5×C$ 的 heatmap 和 $4×2$ 的偏移图，其中 $C$ 为类别数(COCO 中 $C = 80$)。Figure 3 shows an overview. 一旦提取出极值点，我们就把它们分组成纯几何形式的检测形式。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/ExtremeNet/fig3.jpg?x-oss-process=style/blog_img" alt="ExtremeNet%2Ffig3.jpg"></div></p>
<h3 id="ExtremeNet-的分组策略"><a href="#ExtremeNet-的分组策略" class="headerlink" title="ExtremeNet 的分组策略"></a>ExtremeNet 的分组策略</h3><p>Center Grouping</p>
<ol>
<li>将五个 heatmap 作为输入, 对于给定的 heatmap, 检测所有的值大于 $\tau p$ 的极大值(大于周围 3x3)坐标, 将其记为该 heatmap 的峰值.</li>
<li>对于给定的任意组合的四个极值点 $(t, b, r, l)$, 计算它们的几何中心 $c =\frac{(l_x + r_x)}{2}, \frac{(t_y + b_y)}{2}$。 <strong>如果在预测的 center heatmap $\hat Y^{(c)}$ 中该中心具有较高响应 $\hat Y^{(c)}_{cx,cy} \geq \tau c$，则我们就将这些极值点作为一个有效的组合.</strong>。</li>
<li>以暴力搜索的方式对所有关键点 $t，b，r，l$ 的进行四元组的枚举。我们独立地提取每个类别的检测。我们在所有实验中设定 $\tau p= 0.1$ 和 $\tau c = 0.1$。</li>
</ol>
<p>复杂度是 $O(n^4)$, 但是在 COCO 上 $n\leq 40$, 因此勉强可以接受.</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/ExtremeNet/fig2.jpg?x-oss-process=style/blog_img" alt="ExtremeNet%2Ffig2.jpg"></div></p>
<h3 id="Ghost-box-suppression-分组存在的问题"><a href="#Ghost-box-suppression-分组存在的问题" class="headerlink" title="Ghost box suppression(分组存在的问题)"></a>Ghost box suppression(分组存在的问题)</h3><p>ExtremeNet 的分组策略存在一些显而易见的问题,</p>
<p>中心分组会对 <strong>相同大小的三个等间距共线</strong> 物体给出高置信度 FP 检测(<strong>仔细想一想就能知道, 如果三个相同大小的物体共线, 那么左右两边的物体的左右极值点很有可能会被分组到中间物体上, 这是非常错误的!</strong>)。 中心物体在这里有两个选择，一是提交正确的小方框，二是预测一个包含其邻居极值点的更大的方框(FP)。 我们将这些 FP 检测称为 Ghost box。 正如我们将在实验中展示的那样，这些鬼盒很少见，但仍然是 <strong>a consistent error mode of our grouping.</strong></p>
<p>我们提供了一个简单的后处理步骤来删除 ghost box。根据定义， ghost box 会包含许多其他较小的检测框。为了阻止 ghost box ，我们使用了一种 soft-nms 的变种, 即 <strong>如果某一个大边界框中包含的所有框的得分之和超过其自身得分的3倍，则将这个大边界框(可能是 ghost box)得分除以2</strong>。这种非极大值抑制类似于标准的基于重叠的非极大值抑制，但是会惩罚潜在的 ghost box，而不是多个重叠的 box。</p>
<h3 id="Edge-aggregation"><a href="#Edge-aggregation" class="headerlink" title="Edge aggregation"></a>Edge aggregation</h3><p><strong>极值点并不总是唯一定义的。 如果物体的垂直或水平边缘形成极值点（例如，汽车的顶部），沿着该边缘的任何点都可以被认为是极值点。 因此，我们的网络会沿对象的任意对齐边缘产生多处弱响应，而不是单个强峰响应。 这种弱响应存在两个问题：第一，较弱的响应可能低于我们的峰值选择阈值 $\tau p$，我们将完全错过极值点。 其次，即使我们检测到关键点，其得分也会低于具有强烈峰值响应的轻微旋转对象。</strong></p>
<p>我们使用 <strong>边缘聚合</strong> 来解决这个问题。对于提取为局部最大值的每个极值点，我们在垂直方向(对于左和右极值点)或水平方向(对于顶部和底部关键字点)汇总其得分。我们对所有 <strong>单调递减</strong> 的分数进行聚合，并在聚合方向上以 <strong>局部最小值</strong> 停止聚合。具体地说, 我们令 $m$ 是一个 extreme point, $N_i^{(m)} = \hat Y_{m_x+i, m_y}$ 该点的垂直线段或水平线段. 令 $i_0 &lt; 0$ 和 $0 &lt; i_1$ 为两个最近的局部最小点, 于是我们更新 extreme point 的值为 $\tilde Y_m = \hat Y_m + \lambda_{aggr} sum^{i_1}_{i=i_0} N^{(m)}_i$, 其中, $\lambda_{aggr}$ 是 aggregation weight, 在我们的实验中, 取其值为 $0.1$. 效果如图 4 所示.</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/ExtremeNet/fig4.jpg?x-oss-process=style/blog_img" alt="ExtremeNet%2Ffig4.jpg"></div></p>
<h3 id="extreme-points-与-corner-points-的区别"><a href="#extreme-points-与-corner-points-的区别" class="headerlink" title="extreme points 与 corner points 的区别"></a>extreme points 与 corner points 的区别</h3><p>corner points 是 bbox 的左上角和右下角, 大多数情况下, corner points 都位于物体的外面, 因此缺少足够的物体特征.</p>
<p>extreme points 定义在物体上, 因此具有更丰富的物体特征, 因此检测难度相对较低.</p>
<p><span id="CenterNet"></span></p>
<h2 id="CenterNet"><a href="#CenterNet" class="headerlink" title="CenterNet"></a><a href="../计算机视觉-CenterNet-Triplets-CVPR2019">CenterNet</a></h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>CornerNet 的表现受到 <strong>其获取物体全局信息的能力相对较弱</strong> 的限制.</p>
<p>CenterNet(Triplets) 将靠近几何中心的区域，作为一个额外的关键点.</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig2.jpg?x-oss-process=style/blog_img" alt="CenterNet_Triplets%2Ffig2.jpg"></div></p>
<p>整个网络架构如图2所示。CenterNet <strong>通过中心关键点和一对角点来表示每个物体</strong>。具体来说，我们在 CornerNet 的基础上 <strong>为中心关键点嵌入了热图</strong>，并 <strong>预测了中心关键点的偏移</strong>。然后，我们使用 CornerNet 中提出的方法生成 $top-k$ 的边界框。<strong>但是，为了有效滤除不正确的边界框，我们会利用检测到的中心关键点并采用以下程序：</strong></p>
<ol>
<li>根据中心关键点自身的分数也选择 $top-k$ 个点;</li>
<li>使用相应的 offsets 将这些中心关键点重新映射到输入图像上;</li>
<li>为每个边界框定义一个中心区域，并检查中心区域是否包含中心关键点。请注意，检查过的中心关键点的类标签应与边界框的类标签相同;</li>
<li>如果在中心区域检测到中心关键点，我们将保留这个预测到的边界框。边界框的分数将由三个点的平均分数代替，即左上角，右下角和中心关键点。如果在其中心区域中未检测到中心关键点，则将删除该边界框。</li>
</ol>
<p>边界框中的中心区域的大小会影响检测结果。例如，<strong>较小的中心区域导致小边界框的召回率较低，而较大的中心区域导致较大的边界框的精度较低。</strong> 因此，我们提出了一种可以 <strong>感知尺度(scale-aware)的中心区域</strong>，以便自适应地拟合边界框的大小。<strong>尺度感知的中心区域倾向于为小的边界框生成相对大的中心区域，而对于大的边界框则生成相对小的中心区域。</strong> 假设我们想确定是否需要保留边界框 $i$。 设 $tl_x$ 和 $tl_y$ 表示角点 $i$ 的 top-left corner 坐标, $br_x$ 和 $br_y$ 表示角点 $i$ 的 bottom-right corner 坐标。定义中心区域为 $j$, 设 $ctl_x$ 和 $ctl_y$ 表示 $j$ 的左上角坐标, $cbr_x$ 和 $cbr_y$ 表示 $j$ 的右下角坐标。那么 $tl_x，tl_y，br_x，br_y，ctl_x，ctl_y，cbr_x$ 和 $cbr_y$ 应该满足以下关系：</p>
<script type="math/tex; mode=display">\begin{cases} ctl_x = \frac{(n+1)tl_x + (n-1)br_x}{2n} \\ ctl_y = \frac{(n+1)tl_y + (n-1)br_y}{2n} \\ cbr_x = \frac{(n-1)tl_x + (n+1)br_x}{2n} \\ cbr_y = \frac{(n-1)tl_y + (n+1)br_y)}{2n} \end{cases} \tag 1</script><p>其中 $n$ 是奇数，它决定了中心区域 $j$ 的比例。 在本文中，对于小于或大于 150 的边界框的比例，$n$ 被设置为 3 和 5。 图3示出了当 $n = 3$ 且 $n = 5$ 时的两个中心区域。 根据等式（1），我们可以确定尺度感知的中心区域，然后检查中心区域是否包含中心关键点。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig3.jpg?x-oss-process=style/blog_img" alt="CenterNet_Triplets%2Ffig3.jpg"></div></p>
<h3 id="Enriching-Center-and-Corner-Information"><a href="#Enriching-Center-and-Corner-Information" class="headerlink" title="Enriching Center and Corner Information"></a>Enriching Center and Corner Information</h3><p><strong>Center Pooling:</strong> 物体的几何中心不一定传达可识别性非常强的视觉图案（例如，人的头部包含特征很强的视觉图案，但是中心关键点通常位于人体的中间）。为了解决这个问题，我们提出了使用 <strong>Center Pooling</strong> 来捕获更丰富和更易识别的视觉模式。图4（a）显示了 Center Pooing 的原理。  Center Pooling 的详细过程如下：backbone 输出一个特征图，并确定特征图中的某个像素点是否是中心关键点，我们需要在 <strong>水平和垂直方向上找到最大值并将它们加在一起</strong>。通过这样做， Center Pooling 有助于更好地检测中心关键点。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig4.jpg?x-oss-process=style/blog_img" alt="CenterNet_Triplets%2Ffig4.jpg"></div></p>
<p><strong>Cascade Corner Pooling:</strong> Corners 通常位于物体外部，因此缺乏物体的局部外观特征。Corner-Net 使用 Corner Pooling 来解决这个问题。Corner Pooling 的原理如图4（b）所示。<strong>Corner Pooling 旨在找到边界方向上的最大值，以便确定角点</strong>。 但是，<strong>它会使得 corners 对边缘变得敏感(不稳定)</strong>。 为了解决这个问题，我们需要让角落 “看到” 物体的 visual pattern。 Cascade Corner Pooling 的原理如图4（c）所示。<strong>它首先沿着边界查找边界最大值，然后沿着边界最大值的位置向物体内部 “看” 以找到内部最大值，最后，将两个最大值一起添加。</strong> 通过这样做，<strong>Corners 可以同时获得边界信息和物体的 visual pattern。</strong></p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Triplets/fig5.jpg?x-oss-process=style/blog_img" alt="CenterNet_Triplets%2Ffig5.jpg"></div></p>
<p>通过在不同方向组合 Corner Pooling [20]，可以轻松实现 Center Pooling 和 Cascade Corner Pooling。 图5（a）显示了 Center Pooling 模块的结构。为了在一个方向（例如水平方向）上取最大值，我们只需要将左池和右池连接起来即可找到任意一点在水平方向上的最大值。 图5（b）显示了 Cascade Top Corner Pooling 模块的结构。与 CornerNet 中的 Top Corner Pooling 相比，我们在 Top Corner Pooling 之前添加了一个 Left Corner Pooling.</p>
<p><span id="CenterNet Objects as Points"></span></p>
<h2 id="CenterNet-Objects-as-Points"><a href="#CenterNet-Objects-as-Points" class="headerlink" title="CenterNet(Objects as Points)"></a><a href="../计算机视觉-CenterNet-Points-CVPR2019">CenterNet(Objects as Points)</a></h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>令 $(x^{(k)}_1, y^{(k)}_1, x^{(k)}_2, y^{(k)}_2)$ 代表物体 $k$ 的 bbox, 其类别为 $c_k$. 它的 center points 为 $p_k = (\frac{x^{(k)}_1 + x^{(k)}_2}{2}, \frac{y^{(k)}_1 + y^{(k)}_2}{2})$, 我们利用 keypoint estimator $\hat Y$ 去预测所有的 center points. 除此以外, 我们还会回归物体 $k$ 的尺寸 $s_k = (x^{(k)}_2 - x^{(k)}_1, y^{(k)}_2 - y^{(k)}_1)$. 为了控制计算成本, 我们对所有的物体类别都使用 single size 的预测, $\hat S \in R^{\frac{W}{R} \times \frac{H}{R} \times 2}$. 我们在 center point 使用类似公式2的 L1 损失:</p>
<script type="math/tex; mode=display">L_{size} = \frac{1}{N} \sum_{k=1}^N |\hat S_{P_k} - s_k | \tag 3</script><p>我们不对 scale 进行归一化, 而是直接使用 raw pixel coordinates. 然后我们利用一个常量 $\lambda_{size}$ 将 loss 缩放, 整体的训练损失为:</p>
<script type="math/tex; mode=display">L_{det} = L_k + \lambda_{size} L_{size} + \lambda_{off} L_{off}\tag 4</script><p>除非另有说明，否则我们在所有实验中都设置 $\lambda_{size}= 0.1$, $\lambda_{off}= 1$。 <strong>我们使用同一个网络来预测关键点 $\hat Y$，offsets $\hat O$ 和 size $\hat S$.</strong> 整个网络在每个位置的输出总共 $C + 4$ 维。 所有的输出都共享同一个 fully-convolutional backbone 网络。对于每种 modality，backbone 的特征会通过单独的3×3卷积，ReLU和另一个1×1卷积进行传播。 图4显示了网络输出的 overview。</p>
<p><div style="width: 500px; margin: auto"><img src="http://zerozone-blog.oss-cn-beijing.aliyuncs.com/CenterNet_Points/fig4.jpg?x-oss-process=style/blog_img" alt="CenterNet_Points%2Ffig4.jpg"></div></p>
<p><strong>From points to bounding boxes:</strong> 在 Inference 阶段，我们首先 <strong>独立</strong> 地提取 <strong>每个类别</strong> 热图中的峰值。我们会检测其值大于或等于其8个连接邻居的所有响应, 并保持前100个峰值(每个类别都会提取 100 个峰值)。令 $\hat P_c$ 为 $c$ 类的 $n$ 个检测中心点的集合。 每个关键点位置由整数坐标 $(x_i，y_i)$ 给出。我们使用关键点值 $\hat Y_{x_i y_i c}$ 作为其检测置信度的度量，并在该位置处生成边界框</p>
<script type="math/tex; mode=display">(\hat x_i+\delta \hat x_i - \hat w_i / 2, \hat y_i + \delta \hat y_i - \hat h_i / 2, \hat x_i + \delta \hat x_i + \hat w_i / 2, \hat y_i + \delta \hat y_i + \hat h_i / 2)</script><p>上式中, $(\delta \hat x_i, \delta \hat y_i) = \hat O_{\hat x_i, \hat y_i}$ 代表 offset prediction, $\hat w_i, \hat h_i = \hat S_{\hat x_i, \hat y_i}$ 代表 size prediction. 所有的输出都直接从 keypoint estimation产生，无需基于IoU的非最大值抑制（NMS）或其他后处理。 峰值关键点提取可以看做是 NMS 的替代方案，并且可以使用3×3最大池化操作在设备上有效地实现。</p>
<h3 id="CenterNet-Points-与其他-One-Stage-方法的区别"><a href="#CenterNet-Points-与其他-One-Stage-方法的区别" class="headerlink" title="CenterNet(Points) 与其他 One-Stage 方法的区别"></a>CenterNet(Points) 与其他 One-Stage 方法的区别</h3><p>我们的方法与基于 anchor 的一阶段方法密切相关[33,36,43]。中心点可以看作是一个与形状无关的 anchor（见图3）。 但是，有一些重要区别:</p>
<ol>
<li>我们的 CenterNet 仅根据 location 分配 anchor，而不是 box overlap[18]。我们没有区分前景和背景的手动阈值[18]。</li>
<li>每个物体只有一个 positive anchor，因此不需要 NMS。 我们只是在关键点热图中提取局部峰值[4,39]。</li>
<li>与传统物体探测器[21,22]（输出步幅为16）相比，CenterNet使用更大的输出分辨率（输出步幅为4）。 这消除了对多个 anchor 的需求[47]。</li>
</ol>
<h2 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h2><p>RefineDet, SNIP, SNIPPER, Fitness NMS, RFBNet, M2Det, TridentNet, HTC, NAS-FPN(为开源)</p>
<p>Guided Anchoring<br>Libra R-CNN<br>Hybrid Task Cascade</p>
<h1 id="常见问题篇"><a href="#常见问题篇" class="headerlink" title="常见问题篇"></a>常见问题篇</h1><p><span id="目前的 SOTA 目标检测模型"></span></p>
<h2 id="目前的-SOTA-目标检测模型"><a href="#目前的-SOTA-目标检测模型" class="headerlink" title="目前的 SOTA 目标检测模型"></a>目前的 SOTA 目标检测模型</h2><p>比较经典的, 比如 R-CNN 系列, YOLO, 以及 SSD 等属于比较具有代表性的一些模型. 在此基础上, 从 17 年开始, 又有很多新的模型出现, 按照不同的关注角度来分, 大致有这么几类.<br>首先是对卷积网络的特征金字塔的构建和生成进行优化和改进的模型, 比如 FPN 和 M2Det 等.<br>其次是从感受野的角度进行优化的模型, 比如 Deformable ConvNet, RFBNet.<br>还有从 bounding box 回归角度进行优化的 RefineDet.<br>然后还有从损失函数及样本不均衡角度进行优化, 使用 Focal Loss 的 RetinaNet等.</p>
<ul>
<li>在 feature map 的多尺度金字塔特征上进行优化: SSD, FPN, PFPNet, M2Det</li>
<li>在 bbox 回归上进行优化: RefineDet</li>
<li>然后也可以在感受野上进行优化: DCN, RFBNet</li>
<li>以另一种角度来选取 bbox: CornerNet</li>
<li>在 anchor 的生成上进行优化: CornerNet, MetaAnchor</li>
<li>对 NMS 和 IoU 上进行优化: Soft-NMS, Sofer-NMS, Fitness NMS, Relation Network, IoUNet, Cascade R-CNN</li>
<li>在 backbone 网络上优化: DetNet</li>
<li>在损失函数和样本不均衡上进行优化: Focal Loss, Gradient Harmonized(梯度均衡)</li>
<li>针对移动端设备: Pelee</li>
<li>多尺度问题: SNIP, SNIPER</li>
<li>小目标检测: STDNet, Augmentation for small od.</li>
<li>超大目标检测: HKRM</li>
</ul>
<p>FPN,<br>RefineDet, RFBNet<br>STDN</p>
<p><span id="SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别"></span></p>
<h2 id="SSD-FPN-RefineDet-PFPNet-STDN-M2Det-等特征金字塔的区别"><a href="#SSD-FPN-RefineDet-PFPNet-STDN-M2Det-等特征金字塔的区别" class="headerlink" title="SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别"></a>SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别</h2><p>SSD 是通过使用 backbone(VGG16) 的最后两个卷积段的特征图谱和 4 个额外卷积层特征图谱来构建特征金字塔的.<br>FPN 是通过融合深层和浅层的特征图谱来构建特征金字塔. 主要做法是将最后一层特征图谱进行不断的上采样得到 top-down 结构的特征图谱, 然后与原始的 down-top 结构的特征图谱相结合, 从而得到新的表征能力更强的特征金字塔结构.<br>M2Det 主要是从现在特征金字塔的一些缺点出发进行优化, 它认为, 现有金字塔结构中每一个尺度的特征仅仅来自于 backbone 中单一层级(level)的特征. 这样一来, 小尺度的特征图谱往往缺少浅层低级的语义信息, 而大尺度的特征图谱又缺少深层的高级语义信息. 因此, 作者就提出了融合多个层级特征的 Multi-Level FPN (MLFPN). 从而可以让小尺寸的特征图谱上包含有更多的低级语义信息, 让大尺寸的特征图谱包含更多的高级语义信息.</p>
<p><span id="常用的训练 Trick"></span></p>
<h2 id="常用的训练-Trick"><a href="#常用的训练-Trick" class="headerlink" title="常用的训练 Trick"></a>常用的训练 Trick</h2><p><span id="常用的数据增广方法"></span></p>
<h2 id="常用的数据增广方法"><a href="#常用的数据增广方法" class="headerlink" title="常用的数据增广方法"></a>常用的数据增广方法</h2><ul>
<li>颜色变换(Convert Color): BGR 与 HSV 格式随机切换. HSV 模型的三维表示是从 RGB 立方体以旧换新儿来的, 设想从 RGB 沿立方体对角线的白色顶点向黑色顶点观察, 就可以看到立方体的六边形外形. 六边形边界表示色彩, 水平轴表示纯度, 明度沿垂直测量.</li>
<li>随机对比度和亮度: 像素最大最小值的差值影响对比度, 像素的整体大小影响亮度, 通过公式 $g(i, j) = a\times f(i,j) + b$ 控制, $a$ 影响的是对比度, $b$ 影响的图像的亮度.</li>
<li>随机饱和度(Random Saturation): 先将图片转换成 HSV 格式, 然后对 S 通道乘以一个随机值</li>
<li>随机色度(Random Hue): 先将图片转换成 HSV 格式, 然后对 H 通过进行修改</li>
<li>随机交换通道, 增加噪声</li>
</ul>
<h2 id="FCN-是如何降低计算量的"><a href="#FCN-是如何降低计算量的" class="headerlink" title="FCN 是如何降低计算量的"></a>FCN 是如何降低计算量的</h2><p>面对 $384\times 384$ 的图像, 让含全连接层的初始卷积神经网络以 32 像素的步长独立对图像中的 $224\times 224$ 块进行多次评价, 其效果和使用全卷积网络进行一次评价时相同的. 后者通过权值共享起到了加速计算的作用.</p>
<p><span id="简单说一下 PyTorch 和 TensorFlow 的区别"></span></p>
<h2 id="PyTorch-和-TensorFlow-的区别"><a href="#PyTorch-和-TensorFlow-的区别" class="headerlink" title="PyTorch 和 TensorFlow 的区别"></a>PyTorch 和 TensorFlow 的区别</h2><p>两个框架虽然都是在张量上运行, 并且将模型都看做是一个有向非循环图(DAG), 但是二者对于图的定义不同. TensorFlow 是基于静态计算图, 因此是先定义再运行, 一次定义多次运行, 而 PyTorch 是基于动态计算图的, 是在运行过程中被定义的, 在运行的时候进行构建, 可以多次构建多次运行.<br>动态图的还有一个好处就是比较容易调试, 在 PyTorch 中, 代码报错的地方, 往往就是代码错写的地方, 而静态图需要先根据代码生成 Graph 对象, 然后在 session.run() 时报错, 但是这种报错几乎很难直接找到代码中对应的出错段落.</p>
<p>链接】Variable和Tensor合并后，PyTorch的代码要怎么改<br><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/80105285" target="_blank" rel="noopener">https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/80105285</a></p>
<p><span id="目标检测方向继续改进或者优化的地方"></span></p>
<h2 id="目标检测领域方向可以继续改进或者优化的地方"><a href="#目标检测领域方向可以继续改进或者优化的地方" class="headerlink" title="目标检测领域方向可以继续改进或者优化的地方"></a>目标检测领域方向可以继续改进或者优化的地方</h2><ol>
<li><p>首先是精确度和速度的考量, 相对于精度较高的 Faster RCNN, R-FCN 相关系列模型来说, 个人觉得速度更快的 YOLO 系列和 SSD 系列的模型在实际场景应用中会更加实用, 近年来的主要代表有 RefineDet, RFBNet 等都是基于 SSD 模型的研究.</p>
</li>
<li><p>其次是目标的选框步骤, 从最开始的 Region Based , Anchor Based 到现在的基于角点, 甚至基于 segmentation, 包括 semantic segmentation 和 instance segmentation. 今年比较有代表性的就是 CornerNet. 就目前来说, 目标的选框方法很多还是基于 RPN 的 anchor 思想, 所以, 未来的研究中, 新的更好的目标选框方法依旧是研究的一个重要方向.</p>
</li>
<li><p>多尺度问题(尺度变换问题), 目标常见的三种思路, 采用专门设计的尺度变换模块, STDN: Scale-Transferrable Object Detection.</p>
</li>
</ol>
<h2 id="浅层特征图检测小目标，为什么不同时也检测大目标"><a href="#浅层特征图检测小目标，为什么不同时也检测大目标" class="headerlink" title="浅层特征图检测小目标，为什么不同时也检测大目标"></a>浅层特征图检测小目标，为什么不同时也检测大目标</h2><p>浅层感受野较小, 并且语义信息比较低级<br>深层感受野较大, 包含更多高级语义信息</p>
<p><a href="https://www.zhihu.com/question/305729744/answer/555781620" target="_blank" rel="noopener">onestage目标检测算法中,浅层特征图检测小目标</a></p>
<p><span id="GPU 两个重要指标之间的关系"></span></p>
<h2 id="GPU-两个重要指标之间的关系"><a href="#GPU-两个重要指标之间的关系" class="headerlink" title="GPU 两个重要指标之间的关系"></a>GPU 两个重要指标之间的关系</h2><p>科普帖,深度学习中GPU和显存分析: <a href="https://zhuanlan.zhihu.com/p/31558973" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31558973</a></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_nvidia-smi.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgpu_nvidia-smi.jpg"></div></p>
<p>上图是<code>nvidia-smi</code>命令的输出, 其中最重要的两个指标:</p>
<ul>
<li>显存占用</li>
<li>GPU 利用率</li>
</ul>
<p>显存占用和GPU利用率是两个不一样的东西，显卡是由GPU计算单元和显存等组成的，显存和GPU的关系有点类似于内存和CPU的关系。</p>
<p>显存可以看成是空间，类似于内存</p>
<ul>
<li>显存用于存放模型，数据</li>
<li>显存越大，所能运行的网络也就越大<br>GPU计算单元类似于CPU中的核，用来进行数值计算。衡量计算量的单位是flop: the number of floating-point multiplication-adds，浮点数先乘后加算一个flop。计算能力越强大，速度越快。衡量计算能力的单位是flops: 每秒能执行的flop数量</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>*<span class="number">2</span> + <span class="number">3</span>  <span class="comment"># 2 flop</span></span><br><span class="line"><span class="number">1</span>*<span class="number">2</span> + <span class="number">3</span>*<span class="number">4</span> + <span class="number">4</span>*<span class="number">5</span> <span class="comment"># 5 flop</span></span><br></pre></td></tr></table></figure>
<p><span id="GPU 及其显存占用"></span></p>
<h2 id="神经网络显存占用"><a href="#神经网络显存占用" class="headerlink" title="神经网络显存占用"></a>神经网络显存占用</h2><p>神经网络模型占用显存的部分包括:</p>
<ul>
<li>模型的输入</li>
<li>模型中间网络层的输出</li>
<li>模型参数</li>
<li>模型参数的梯度</li>
<li>优化器动量</li>
</ul>
<p>首先, 要确定运算过程中使用的数据类型, 常用的数据类型有: int8, int16, float16, float32 等. <strong>通常, 在训练时, 我们为了更高的精度需要使用 float32 的数据类型, 而在预测时, 我们为了压缩模型的大小会选用 float16 的数据类型(预测时几乎不会带来精度损失)</strong>.<br>距离来说, 一个 $(32, 3, 256, 256)$ 的 tensor, 如果使用的是 float32 的数据类型, 那么它的显存占用约为 <strong>24M</strong>, 而如果使用的是 float16 的数据类型, 那么它的显存占用约为 <strong>12M</strong>.</p>
<p>下面来逐一分析:</p>
<ol>
<li><strong>模型的输入:</strong> 一般就是图片的分辨率大小, 另外需要注意的时, 一般不需要计算模型输入(图片)的梯度.</li>
<li><strong>模型中间网络层的输出:</strong> 这部分显存主要是指每个网络层输出的 feature map, 它的形状和模型的具体结构有关, 越大越厚的 feature map 占用的显存越多.</li>
<li><strong>模型参数:</strong> 只有有参数的层, 才会占用显存, 这些网络层一般包括卷积层, 全连接层, BN 层等等. 其它的不包括参数的层如激活层, 池化层, Dropout 层等均不占用显存. 在 PyTorch 中, 当执行完<code>model=MyModel().cuda()</code>之后就会占用相应的显存, 占用的显存大小基本与下面计算的差不多(会稍大一些, 因为还存在一些其它的开销):<ul>
<li>Linear(M-&gt;N): $M\times N$</li>
<li>Conv2d(Cin, Cout, K): $C_{in} \times C_{out} \times K\times K$</li>
<li>BatchNorm(N): $2N$</li>
<li>Embedding(N, W): $N\times W$</li>
</ul>
</li>
<li><strong>模型参数的梯度:</strong> 在训练的时候, 由于需要进行反向传播的原因, 我们还需要保存每个参数对应的梯度, 这部分的显存占用和参数占用的显存大小一致. 通常来说, 神经网络的每一层输入输出都需要保存下来, 用来进行反向传播. 但是在某些特殊情况下, 我们可以不保存输入. 比如 ReLU, 在 PyTorch 中, 使用<code>nn.ReLU(inplace=True)</code>能将激活函数的输出直接覆盖保存于模型的输入之中, 节省不少内存(此时的反向传播:$y = relu(x) -&gt; dx = dy.copy(); dx[y&lt;=0]=0$).</li>
<li><strong>优化器的动量:</strong> 不同的优化器需要的信息量不同, 对于普通的 SGD 来说, 仅仅需要参数的梯度就足够了, 因此 <strong>总显存(参数+梯度+动量)</strong> 占用为参数显存的 <strong>两倍</strong>, 但是对于 Momentum-SGD, 不仅需要梯度, 还需要动量, 因此总显存占用为参数显存的 <strong>三倍</strong>, 对于 Adam 优化器, 需要更多的动量信息, 总显存占用为参数显存的 <strong>四倍</strong>.</li>
</ol>
<p>在深度学习神经网络的显存占用中, 我们可以得到: <strong><script type="math/tex">显存占用 = 模型显存占用 + batchsize \times 每个样本的显存占用</script></strong>. 可以看出, 显存并不是简单的和 batch_size 成正比, 尤其是在模型自身比较大的情况下.</p>
<p><span id="节省显存的方法"></span></p>
<h2 id="节省显存的方法"><a href="#节省显存的方法" class="headerlink" title="节省显存的方法"></a>节省显存的方法</h2><ul>
<li>降低 batch_size</li>
<li>进行下采样, 降低 feature map 的大小</li>
<li>减少全连接层</li>
<li>将激活层设置成<code>inplace=True</code></li>
<li>使用字节数更少的数据类型(float8, float16等)</li>
</ul>
<p>显存计算, PyTorch 显存跟踪: <a href="https://www.cnblogs.com/kk17/p/10262688.html" target="_blank" rel="noopener">https://www.cnblogs.com/kk17/p/10262688.html</a><br><a href="https://oldpan.me/archives/how-to-calculate-gpu-memory" target="_blank" rel="noopener">https://oldpan.me/archives/how-to-calculate-gpu-memory</a></p>
<p><span id="各个网络层的参数量"></span></p>
<h2 id="各个网络层的参数量"><a href="#各个网络层的参数量" class="headerlink" title="各个网络层的参数量"></a>各个网络层的参数量</h2><p>同时计算 weight 和 bias</p>
<ul>
<li>Linear (M-&gt;N): $(1+M)\times N$</li>
<li>Conv2d (Cin, Cout, Kw, Kh): $C_{out}\times (C_{in} \times K_w\times K_h + 1)$</li>
<li>BatchNorm (N): $2N$</li>
<li>Embedding (N, W): $N\times W$</li>
</ul>
<p><span id="FLOPs 计算"></span></p>
<h2 id="FLOPs-计算"><a href="#FLOPs-计算" class="headerlink" title="FLOPs 计算"></a>FLOPs 计算</h2><p>FLOPs: Floating point operations, 通常是指浮点运算的次数, 乘法和加法都各自算作一次运算. Paper 里比较流行的单位是 GFLOPs, 即 $10^9$ FLOPs<br>FLOPS: Floating Point Operations Per Second, 代表算力<br>Mult-Adds: 统计乘法加法对, 因此 FLOPs 通常是 Mult-Adds 的两倍, 但是有时候, 也会用 FLOPs 直接指代 Mult-Adds (ShuffleNet 论文中就是这么干的)</p>
<ol>
<li><strong>矩阵乘法:</strong> $M\times N$ 和 $N\times P$ 的两矩阵相乘, 最终输出的矩阵形状为 $M\times P$, 计算 $M\times P$ 上一个元素需要 $N$ 次乘法, 以及 $N-1$ 次加法, 因此, 矩阵乘法所需的总的 FLOPs 为:<script type="math/tex; mode=display">(N + N - 1) \times M \times P</script></li>
<li><strong>卷积层:</strong> ($C_{in}, C_{out}, K_w, K_h, W_{out}, H_{out}$)(输入输出通道数, 卷积核尺寸, 输出特征图谱尺寸): 首先计算得到 output feature map 上一个像素的值需要的 FLOPs 为 $C_{in}\cdot K_w \cdot K_h$ 次乘法, 以及 $C_{in}\cdot K_w \cdot K_h - 1$ 次加法, 如果算上 bias 加法, 则为 $C_{in}\cdot K_w \cdot K_h$ 次加法, 然后根据输出的 tensor 的 shape, 可以知道总共需要计算的像素点数为 $C_{out}\cdot W_{out} \cdot H_{out}$. 于是可以得到一个卷积层总共的 FLOPs 为:<script type="math/tex; mode=display">(2\times C_{in}\cdot K_w \cdot K_h - 1) \times (C_{out}\cdot W_{out} \cdot H_{out}) \tag{不计 bias}</script><script type="math/tex; mode=display">(2 \times C_{in}\cdot K_w \cdot K_h) \times (C_{out}\cdot W_{out} \cdot H_{out}) \tag{计入 bias}</script></li>
<li><strong>全连接层:</strong> (M, N) (输入神经元数量, 输出神经元数量). 输出一个神经元需要 $M$ 次乘法, $M-1$ 次加法, 如果算上 bias, 就是 $M$ 次加法, 扩展到输出 N 个神经元, 所需要 FLOPs 为:<script type="math/tex; mode=display">(2\times M - 1)\times \tag{不计 bias}N</script><script type="math/tex; mode=display">(2\times M)\times N \tag{计入 bias}</script></li>
</ol>
<p><span id="除 FLOPs 外其他影响模型速度的因素"></span></p>
<h2 id="除-FLOPs-外其他影响模型速度的因素"><a href="#除-FLOPs-外其他影响模型速度的因素" class="headerlink" title="除 FLOPs 外其他影响模型速度的因素"></a>除 FLOPs 外其他影响模型速度的因素</h2><ul>
<li>MAC (Memory Access Cost): FLOPs 与 MAC 并非完全一致, 例如 element-wise 操作 FLOPs 较低, 但是却具有更多的 MAC, 另外, Pointwise Conv 和 Group Conv 也会根据不同的超参而具有不同的的 MAC, 即使控制他们的 FLOPs 不变.</li>
<li>计算并行度: 碎片化程度较高的 building block (如 Inception 系列和 NAS 结构), 其并行度比较低, 进而影响速度</li>
<li>硬件平台的优化: 硬件平台对于不同尺寸的卷积核, 有时会有不一样的优化程度.</li>
<li>内存带宽, 见下文</li>
</ul>
<p><a href="https://www.jiqizhixin.com/articles/2017-09-11-2" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-09-11-2</a> (内存带宽与计算能力，谁才是决定深度学习执行性能的关键)</p>
<p><strong>结论:</strong> 模型的执行速度除了和总的计算量有关外, 还与运算强度(每单位数据执行的运算操作)有关, 比如 1x1 卷积虽然参数量是 3x3 卷积的 1/9. 但是由于运算强度降低, 导致计算性能下降, 因此其执行速度并不能降低到 1/9. 在进行算法的速度优化的时候, 可以参考 Roofline 模型曲线, 进而决定到底是应该增大带宽还是应该降低计算量.</p>
<p><strong>分析:</strong></p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_bottle.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgpu_bottle.jpg"></div></p>
<p>内存带宽对于硬件系统的性能影响如上图所示。如果把内存比做瓶子，运算单元比作杯子，那么数据就是瓶子里的各色颗粒，而内存接口就是瓶口，通过瓶口数据才能进入杯子被消费（处理）掉。而内存带宽就是瓶口的宽度了。瓶口宽度越窄，则数据需要越多时间才能进入杯子（处理单元）。正所谓「巧妇难为无米之炊」，如果带宽有限，那么即使处理单元无限快，在大多数时候也是处理单元在空等数据，造成了计算力的浪费.<br>算法对于内存带宽的需求通常使用「运算强度 (operational intensity，或称 arithmetic intensity)」这个量来表示，单位是 OPs/byte。这个量的意思是，在算法中平均每读入单位数据，能支持多少次运算操作。运算强度越大，则表示单位数据能支持更多次运算，也就是说算法对于内存带宽的要求越低。所以，运算强度大是好事！<br>我们来举一个例子。对于步长（stride）为 1 的 3x3 卷积运算，假设输入数据平面大小为 64x64。简单起见，假设输入和输出 feature 都为 1。这时候，总共需要进行 62x62 次卷积运算，每次卷积需要做 3x3=9 次乘加运算，所以总共的计算次数为 34596，而数据量为（假设数据和卷积核都用单精度浮点数 2byte）：64x64x2（输入数据）+ 3x3x2（卷积核数据）= 8210 byte，所以运算强度为 34596/8210=4.21。如果我们换成 1x1 卷积，那么总的计算次数变成了 64x64=4096，而所需的数据量为 64x64x2 + 1x1x2=8194。显然，切换为 1x1 卷积可以把计算量降低接近 9 倍，但是运算强度也降低为 0.5，即对于内存带宽的需求也上升了接近 9 倍。因此，如果内存带宽无法满足 1x1 卷积计算，那么切换成 1x1 卷积计算虽然降低了接近 9 倍计算量，但是无法把计算速度提升 9 倍。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_RooflineModel.jpg?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgpu_RooflineModel.jpg"></div></p>
<p>典型的 Roofline 曲线模型如上图所示，坐标轴分别是计算性能（纵轴）和算法的运算强度（横轴）。Roofline 曲线分成了两部分：左边的上升区，以及右边的饱和区。当算法的运算强度较小时，曲线处于上升区，即计算性能实际被内存带宽所限制，有很多计算处理单元是闲置的。随着算法运算强度上升，即在相同数量的数据下算法可以完成更多运算，于是闲置的运算单元越来越少，这时候计算性能就会上升。然后，随着运算强度越来越高，闲置的计算单元越来越少，最后所有计算单元都被用上了，Roofline 曲线就进入了饱和区，此时运算强度再变大也没有更多的计算单元可用了，于是计算性能不再上升，或者说计算性能遇到了由计算能力（而非内存带宽）决定的「屋顶」（roof）。拿之前 3x3 和 1x1 卷积的例子来说，3x3 卷积可能在 roofline 曲线右边的饱和区，而 1x1 卷积由于运算强度下降，有可能到了 roofline 左边的上升区，这样 1x1 卷积在计算时的计算性能就会下降无法到达峰值性能。虽然 1x1 卷积的计算量下降了接近 9 倍，但是由于计算性能下降，因此实际的计算时间并不是 3x3 卷积的九分之一。</p>
<p>显然，一个计算系统的内存带宽如果很宽，则算法不需要运算强度很大也能轻易碰到计算能力上限决定的「屋顶」。在下图中，计算能力不变，而随着内存带宽的上升，达到计算力屋顶所需的运算强度也越低。</p>
<p><div style="width: 500px; margin: auto"><img src="https://zerozone-blog.oss-cn-beijing.aliyuncs.com/SummaryOfComputerVision/gpu_RooflineModel2.png?x-oss-process=style/blog_img" alt="SummaryOfComputerVision%2Fgpu_RooflineModel2.png"></div></p>
<p>Roofline 模型在算法-硬件协同设计中非常有用，可以确定算法和硬件优化的方向：到底应该增加内存带宽／减小内存带宽需求，还是提升计算能力／降低计算量？如果算法在 roofline 曲线的上升区，那么我们应该增加内存带宽／减小内存带宽需求，提升计算能力／降低计算量对于这类情况并没有帮助。反之亦然。</p>
<h2 id="Anchor-的作用"><a href="#Anchor-的作用" class="headerlink" title="Anchor 的作用"></a>Anchor 的作用</h2><p>结论: 为了将 bbox 的 scale 和 aspect ratio 划分到若干个固定的子空间中, 降低问题的复杂度, 同时也降低模型的学习难度. 另一方面, anchor 可以解决多个物体 overlap 过大导致检测结果容易丢失的问题(YOLO 单个 cell 只负责单个物体, Overlap 过大的多个物体可能会落在同一个 cell 中).</p>
<p>Faster RCNN 引入 Anchor 的 motivation:</p>
<blockquote>
<p>In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel “anchor” boxes that serve as references at <strong>multiple scales and aspect ratios</strong>. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios.</p>
</blockquote>
<p>可见, Anchor 机制要解决的问题是变化范围较大的 bbox 的 scale 和 aspect rotios 问题. 之前的解决方法都是利用 pyramids of images(耗时) 或者 pyramids of filters(传统图像处理). 此外, Anchor 机制还顺便解决了另外一个重要的问题: gt box 之间如果 overlap 较大, 那么它们就会落到一个 cell 上, 从而导致个别 gt box 丢失. 而 anchor 机制不同 scale 和 aspect ratio 的 anchor 会负责各自的 gt box, 即使有多个框映射到同一个 cell, 也不会导致 gt box 丢失.</p>
<h2 id="R-CNN-系列类别冲突问题"><a href="#R-CNN-系列类别冲突问题" class="headerlink" title="R-CNN 系列类别冲突问题"></a>R-CNN 系列类别冲突问题</h2><p>怎么解决:<br>R-CNN 由于在进行框的筛选时, 是类别不可知的, 也就是说如果有两个不同类的物体, 重合度比较高, 那么很有可能其中一个物体的框就被 NMS 掉了. 解决办法有:</p>
<ol>
<li>对于每一个类别, 分别进行框的筛选, 也就是变成类别可知的;</li>
<li>还没想出来</li>
</ol>
<h2 id="Anchor-的数量"><a href="#Anchor-的数量" class="headerlink" title="Anchor 的数量"></a>Anchor 的数量</h2><p><strong>当增加超过 6~9 个 anchor 后模型并没有显示出进一步的收益。 性能饱和意味着认为定义的、密度过大的 anchor 并没有呈现出巨大的优势。</strong><br>过于密集的 anchor 不仅会增加 前景-背景 的优化难度，而且还可能导致模糊位置定义问题。 对于每个输出空间的 location 来说，其 anchors 的标签根据与 GT 的 IoU 值定义。 其中，有一些 anchor 被定义为 positive samples，而另一些则被定义为 negative samples。 但是，它们共享这相同的 feature maps。因此分类器不仅需要区分不同位置的样本，还需要在同一位置区分不同的 anchor。</p>
<p><strong>anchor 的数量并不是越多越好</strong>, 虽然直观上来说, anchor 越多时, 可以覆盖越好的 gt box, 但是, 当 anchor 的数量过多时, 一方面由于每一个点上产生的 anchor 实际上共享了一块相似的特征, 但是这些 anchor 有一部分作为正样本, 而另一部分作为负样本, 因此, 对于神经网络来说, 他要通过不断学习来区分这些样本, 不仅如此, 他还要将这些样本与其他点产生的 anchor 区分开, 虽然 anchor 数量的增多, 学习的难度也就慢慢增多, 最终甚至会出现掉点的现象, 个人认为不会掉的特别多, 因为毕竟更多的 anchor 可以覆盖更多的局部最优解. 但是 anchor 会导致计算量的大幅度上升, 因此不建议设置过多 anchor.</p>
<p>FoveaBox 的优势(就 anchor 来说): (1) 神经网络的输出维度大大降低(1/A), 学习起来相对简单直接; (2) 不会出现 anchor 之间互相矛盾的现象 (3) 没有了 anchor 后, 检测网络变的更加简单直接, 扩展性更好</p>
<p>详细见 FoveaBox 分析</p>
<h2 id="Anchor-based-方法和-Anchor-free-方法比较"><a href="#Anchor-based-方法和-Anchor-free-方法比较" class="headerlink" title="Anchor based 方法和 Anchor free 方法比较"></a>Anchor based 方法和 Anchor free 方法比较</h2><ul>
<li>Anchor-based 方法处理的尺度范围小(基于 anchor 的设定), 但是更加精准(学习难度小); Anchor-free 方法覆盖的尺度范围大(没有认为设定的 anchor), 但是检测小尺度的能力较低.</li>
<li>Anchor-based 需要根据不同的任务设置更多的超参(anchor scale, aspect ratio)</li>
<li>Anchor-base 方法通常是数据集敏感的, 每一种超参只适用于特定的数据集</li>
<li>Anchor-base 方法需要使用特定的方法来解决前背景不平衡问题</li>
<li>Anchor-based 方法产生的预选框要在训练阶段和 gt box 进行 IoU 的计算才能确定正负样本, 这会占用一定的内存和运行时间, 降低训练速度. 此外, 对于 SSD 类的算法来说, 为了使其能够更为有效的检测小目标, 通常会在更大尺度的特征图上生成候选框, 这就导致候选框的数量大幅增加.</li>
<li>针对不同的任务, 所有的超参都需要重新调节, 这样一个模型的迁移能力就体现不出来了.</li>
<li>anchor-free 的方法能够在精度上媲美 anchor-based 的方法，最大的功劳我觉得应该归于 FPN，其次归于 Focal Loss。（内心OS：RetinaNet 赛高）。在每个位置只预测一个框的情况下，FPN 的结构对尺度起到了很好的弥补，FocalLoss 则是对中心区域的预测有很大帮助</li>
</ul>
<h2 id="CNN-的平移不变性"><a href="#CNN-的平移不变性" class="headerlink" title="CNN 的平移不变性"></a>CNN 的平移不变性</h2><p>首先, 有两个概念需要区分:</p>
<ul>
<li>平移不变性(translation invariant): 对于同样的 input, 其 output 保持不变.</li>
<li>平移等变性(translation equivalence): 如果 innput 平移, 那么其 output 也会发生相应的平移, <strong>但是 output 的取值应该保持不变</strong>.<br>对于分类(classification)任务来说, 我们希望网络具有平移不变性, 因为对于一个物体的平移来说, 不应该改变这个物体的类别. 但是对于检测(detection), 分割(segmentation)任务来说, 我们希望网络具有平移等价性, 即当物体的位置发生变化时, 我们输出的框的位置也应该发生相应的变化, 但是它的取值, 即框的大小, 框的类别应该保持不变.</li>
</ul>
<p>常见网络层对于平移不变性和平移等价性的影响:</p>
<ul>
<li>卷积层: 从卷积的运算定义来看, 卷积应该具有一定的平移等价性, 但是这种等价性需要在物体平移量为 stride 的整数倍时才 <strong>严格成立</strong>(不是整数倍时, 也具有一定程度的平移等价性). 而对于平移不变性来说, 只有当卷积核的值分布比较均匀(值相差不大), 而输入的 feature map 上的值也有很多值分布均匀(值相差不大)的区域时来具有比较弱的平移不变性, 因为这个时候 feature map 上的微小移动对于输出的改变较少.</li>
<li>池化层: 目前大部分的观点都认为, 卷积网络的平移不变性主要是通过池化层实现的, 其中最大池化相对于均值池化来说带有更强的平移不变性, 全局池化层则具有更强的平移不变性. 由于池化操作往往会忽略核内元素的位置, 因此通常认为池化层是不具有平移等价性. 另外, 个人觉得正是因为池化层具有一定程度的平移不变性, 且几乎不具有平移等价性, 因此在处理一些需要平移等价性的任务(比如检测, 分割)的时候, 会经常使用 stride 为 2 的卷积层来代替池化层, 而在分类任务中, 往往会使用池化和全局池化来消除平移的影响. 在 18 年的一篇论文(Why do deep convolutional networks generalize so poorly to small image)中也提到了 stride 为 2 的降采样操作会使得 CNN 网络丢失平移不变性.</li>
<li>全连接层: 从全连接层的计算规则来看, FC <strong>即不具有</strong> 平移不变性 <strong>也不具有</strong> 平移等价性. 因为当参与计算的向量值发生偏移时, 很明显 FC 的输出结果会发生较大的变化, 尤其适当 FC 上的神经元权重相差较大时. (等价性也要求结果保持相对不变). 但是, 我们不能就此断定添加了 FC 层的网络就丧失了平移不变性, 18 年的那篇文章里面给出的实验结果发现 VGG 的平移不变性均好于 ResNet 和 InceptionResNetV2, 文中作者的分析是因为 VGG 具有更大的 max pooling 层而另外两个网络相对较少, 同时后面两个网络更深. 但是还有一点要注意的是, <strong>VGG 有全连接层</strong> 而其他两个没有, 这说明全连接层并不是破坏 CNN 平移不变性的主导因素.</li>
</ul>
<p>我个人认为评价一个网络层是不是具有平移不变性或者平移等价性, 一方面要看它的结构特点, 另一方面也要着重看一下这个网络层学习到的参数, 不同的参数值具有的平移不变性和等价性的强度有较大区别, <strong>而参数的值又来自于数据</strong>, 所以很多时候, 想办法拿到更好的数据也是非常关键的一步. 有文章(也是18那篇)也分析了目前的大部分数据集中的数据, 都具有一定的偏向性, 这种偏向性也就导致训练出来的参数具有一定的偏向性, 不能说这种偏向性不好, 但是它对于网络的泛化能力也确实有一定的影响.</p>
<h2 id="小目标检测"><a href="#小目标检测" class="headerlink" title="小目标检测"></a>小目标检测</h2><ol>
<li>增大imgsize, data-augmentation</li>
<li>fpn, <strong>多层级, 多尺度</strong>(注意二者区别)的特征融合. 将深层的特征图谱进行上采样, 补充损失的信息, 对速度影响较大.</li>
<li>利用浅层的特征图谱进行预测</li>
<li>利用 context 信息, 建立小物体和 context 的关系; 利用 dilated conv 扩大感受野. 对于这一点, 存在一些争议, 根据 DCVv2 的实验看出, 冗余的 context 实际上会对检测效果造成不小的干扰</li>
<li>将小目标进行放大, 可以利用一些超分辨率的方法提升小目标的分辨率, 或者一些自动修补的方法修复小目标损失的特征信息</li>
<li>在 bbox 的准确度上进行优化: iou loss, cascade rcnn</li>
<li>snip / sniper</li>
<li>在 anchor 层面进行优化(待定)</li>
<li>利用物体间的关系, 增加检测效果, relation network 思路</li>
</ol>
<p><a href="https://www.zhihu.com/question/272322209" target="_blank" rel="noopener">深度学习在小目标检测方面有什么进展</a></p>
<p><a href="https://www.zhihu.com/question/269877902" target="_blank" rel="noopener">小目标检测</a></p>
<p><a href="https://www.zhihu.com/question/280393440/answer/414216621" target="_blank" rel="noopener">小目标检测</a></p>
<h2 id="样本不均衡问题"><a href="#样本不均衡问题" class="headerlink" title="样本不均衡问题"></a>样本不均衡问题</h2><ul>
<li>Data augmentation</li>
<li>OHEM</li>
<li>Focal Loss</li>
<li>GHM</li>
</ul>
<h2 id="遮挡问题"><a href="#遮挡问题" class="headerlink" title="遮挡问题"></a>遮挡问题</h2><p><a href="https://zhuanlan.zhihu.com/p/43655912" target="_blank" rel="noopener">遮挡问题</a></p>
<ul>
<li>非目标遮挡(occlusion)<ul>
<li>目前较难解决, 通常的做法是堆数据(数据增广), 以及利用更强的 feature</li>
</ul>
</li>
<li>目标遮挡(crowded)<ul>
<li><a href="#CoupleNet">CoupleNet</a>: 利用 PSRoI Pooling 和 RoI Pooling, 分别提取 local feature 和 global feature, 来综合决定最终的预测结果, 可以在一定程度上解决遮挡问题.</li>
</ul>
</li>
</ul>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/知识点梳理/" rel="tag"><i class="fa fa-tag"></i> 知识点梳理</a>
          
            <a href="/tags/计算机视觉/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a>
          
            <a href="/tags/面试/" rel="tag"><i class="fa fa-tag"></i> 面试</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/z_post/计算机视觉-目标检测训练策略/" rel="prev" title="计算机视觉-目标检测训练策略">
                <i class="fa fa-chevron-left"></i> 计算机视觉-目标检测训练策略
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/z_post/面试-算法刷题-LeetCode-record/" rel="next" title="LeetCode 算法题(记录总结)">
                LeetCode 算法题(记录总结) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar_zz.png" alt="ZeroZone">
            
              <p class="site-author-name" itemprop="name">ZeroZone</p>
              <p class="site-description motion-element" itemprop="description">吾乃闪耀的芝士蛋挞!</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">270</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">42</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/hellozhaozheng" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:hellozhaozheng@foxmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/ksws0292756" title="零域CSDN博客" target="_blank">零域CSDN博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://xinghanzzy.github.io/" title="BoXiao的博客" target="_blank">BoXiao的博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://oldpan.me/" title="Oldpan的博客" target="_blank">Oldpan的博客</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#章节导航篇"><span class="nav-text">章节导航篇</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习篇"><span class="nav-text">机器学习篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TP-TN-FP-FN-及各种比值代表的含义"><span class="nav-text">TP, TN, FP, FN 及各种比值代表的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PR-ROC-AUC"><span class="nav-text">PR, ROC, AUC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PR-曲线"><span class="nav-text">PR 曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ROC-曲线"><span class="nav-text">ROC 曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#绘制-ROC-曲线"><span class="nav-text">绘制 ROC 曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AUC-的含义及计算"><span class="nav-text">AUC 的含义及计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归"><span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归和线性回归的定义"><span class="nav-text">逻辑回归和线性回归的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归和线性回归的区别和联系"><span class="nav-text">逻辑回归和线性回归的区别和联系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对于一个二分类问题-如果数据集中存在一些离异值-在不清洗数据的情况下-选择逻辑回归还是-SVM-为什么"><span class="nav-text">对于一个二分类问题, 如果数据集中存在一些离异值, 在不清洗数据的情况下, 选择逻辑回归还是 SVM? 为什么?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归与-SVM-的区别是什么"><span class="nav-text">逻辑回归与 SVM 的区别是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归和-SVM-哪个是参数模型-哪个是非参数模型"><span class="nav-text">逻辑回归和 SVM 哪个是参数模型, 哪个是非参数模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归和-SVM-分别适合在什么情况下使用"><span class="nav-text">逻辑回归和 SVM 分别适合在什么情况下使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KNN"><span class="nav-text">KNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-KNN-算法的原理"><span class="nav-text">简述 KNN 算法的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN-算法进行分类和回归时的区别"><span class="nav-text">KNN 算法进行分类和回归时的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN-算法的三要素"><span class="nav-text">KNN 算法的三要素</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN-算法是否可微"><span class="nav-text">KNN 算法是否可微</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编程实现-KNN-算法"><span class="nav-text">编程实现 KNN 算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机"><span class="nav-text">支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-SVM-的基本概念和原理"><span class="nav-text">简述 SVM 的基本概念和原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM-推导过程"><span class="nav-text">SVM 推导过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM-如何解决线性不可分问题"><span class="nav-text">SVM 如何解决线性不可分问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么SVM的分类结果仅依赖于支持向量"><span class="nav-text">为什么SVM的分类结果仅依赖于支持向量?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何选取核函数"><span class="nav-text">如何选取核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么说高斯核函数将原始特征空间映射成了无限维空间"><span class="nav-text">为什么说高斯核函数将原始特征空间映射成了无限维空间?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数中不同参数的影响"><span class="nav-text">核函数中不同参数的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#既然深度学习技术性能表现以及全面超越-SVM-SVM-还有存在的必要吗"><span class="nav-text">既然深度学习技术性能表现以及全面超越 SVM, SVM 还有存在的必要吗?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树"><span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#降维"><span class="nav-text">降维</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类"><span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-K-Means-聚类的原理"><span class="nav-text">简述 K-Means 聚类的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Means-算法的优点和缺点"><span class="nav-text">K-Means 算法的优点和缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Means-实现流程"><span class="nav-text">K-Means 实现流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Means-常规实现代码"><span class="nav-text">K-Means 常规实现代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Means-实现-anchor-划分"><span class="nav-text">K-Means 实现 anchor 划分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost"><span class="nav-text">XGBoost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bagging"><span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#待整理"><span class="nav-text">待整理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习篇"><span class="nav-text">深度学习篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#优化方法"><span class="nav-text">优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述各种优化方法的概念及其优缺点"><span class="nav-text">简述各种优化方法的概念及其优缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#各损失函数更新动画"><span class="nav-text">各损失函数更新动画</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何选择合适的优化方法"><span class="nav-text">如何选择合适的优化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一阶矩-二阶矩的计算方法及其代表的含义"><span class="nav-text">一阶矩, 二阶矩的计算方法及其代表的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-Adam-中使用的指数加权滑动平均法"><span class="nav-text">简述 Adam 中使用的指数加权滑动平均法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-算法的原理机制"><span class="nav-text">Adam 算法的原理机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-算法与相关的-AdaGrad-和-RMSprop-方法有什么区别"><span class="nav-text">Adam 算法与相关的 AdaGrad 和 RMSprop 方法有什么区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-算法如何调参-其常用的参数配置"><span class="nav-text">Adam 算法如何调参, 其常用的参数配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-实现优化的过程和权重更新规则"><span class="nav-text">Adam 实现优化的过程和权重更新规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-的初始化偏差修正的推导"><span class="nav-text">Adam 的初始化偏差修正的推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-的扩展形式-AdaMax"><span class="nav-text">Adam 的扩展形式: AdaMax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#各种优化方法的源码实现"><span class="nav-text">各种优化方法的源码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考文献及其他"><span class="nav-text">参考文献及其他</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#初始化方法"><span class="nav-text">初始化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#各个初始化方法的形式"><span class="nav-text">各个初始化方法的形式,</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Xavier-初始化推导"><span class="nav-text">Xavier 初始化推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练时是否可以将全部参数初始化为-0"><span class="nav-text">训练时是否可以将全部参数初始化为 0</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#绝对值损失-L1"><span class="nav-text">绝对值损失(L1)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#平方损失-L2"><span class="nav-text">平方损失(L2)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-交叉熵"><span class="nav-text">Softmax 交叉熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交叉熵损失"><span class="nav-text">交叉熵损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Smooth-L1"><span class="nav-text">Smooth L1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Focal-Loss"><span class="nav-text">Focal Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DR-Loss"><span class="nav-text">DR Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#写出多层感知机的平方误差和交叉熵误差损失函数"><span class="nav-text">写出多层感知机的平方误差和交叉熵误差损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式"><span class="nav-text">推导平方误差和交叉熵误差损失函数的各层参数更新的梯度计算公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#写出常用的激活函数的公式及其导数形式"><span class="nav-text">写出常用的激活函数的公式及其导数形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简单画出常用激活函数的图像"><span class="nav-text">简单画出常用激活函数的图像</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么需要激活函数"><span class="nav-text">为什么需要激活函数?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#标准说法"><span class="nav-text">标准说法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#更形象的解释"><span class="nav-text">更形象的解释</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#举例说明为什么激活函数可以解决-XOR-问题"><span class="nav-text">举例说明为什么激活函数可以解决 XOR 问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#各个激活函数的优缺点和适用场景是什么"><span class="nav-text">各个激活函数的优缺点和适用场景是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid-激活函数和-Softmax-激活函数的区别"><span class="nav-text">Sigmoid 激活函数和 Softmax 激活函数的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么情况下-ReLU-的神经元会死亡-为什么-可以复活吗"><span class="nav-text">什么情况下 ReLU 的神经元会死亡? 为什么? 可以复活吗?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何解决-ReLU-神经元死亡问题"><span class="nav-text">如何解决 ReLU 神经元死亡问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#谈谈-ReLU6"><span class="nav-text">谈谈 ReLU6</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数的使用原则"><span class="nav-text">激活函数的使用原则</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化"><span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-L1-正则和-L2-正则的形式"><span class="nav-text">简述 L1 正则和 L2 正则的形式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L1-正则"><span class="nav-text">L1 正则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-正则"><span class="nav-text">L2 正则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-正则和-L2-正则的特点是什么-各有什么优势"><span class="nav-text">L1 正则和 L2 正则的特点是什么? 各有什么优势?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1-和-L2-的区别有哪些"><span class="nav-text">L1 和 L2 的区别有哪些?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1正则化使模型参数稀疏的原理是什么"><span class="nav-text">L1正则化使模型参数稀疏的原理是什么?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-L1-和-L2-分别对应拉普拉斯先验和高斯先验"><span class="nav-text">为什么 L1 和 L2 分别对应拉普拉斯先验和高斯先验?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么权重矩阵稀疏可以防止过拟合"><span class="nav-text">为什么权重矩阵稀疏可以防止过拟合?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为何权重参数-w-减小就可以防止过拟合"><span class="nav-text">为何权重参数 $w$ 减小就可以防止过拟合?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L0-范式和-L1-范式都能实现稀疏-为什么不选择用-L0-而要用-L1"><span class="nav-text">L0 范式和 L1 范式都能实现稀疏, 为什么不选择用 L0 而要用 L1?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么说-L2-范式可以优化计算"><span class="nav-text">为什么说 L2 范式可以优化计算?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则项前面的系数一般怎么设置"><span class="nav-text">正则项前面的系数一般怎么设置?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#归一化"><span class="nav-text">归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要进行归一化"><span class="nav-text">为什么要进行归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Normalization"><span class="nav-text">Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-BN-的原理"><span class="nav-text">简述 BN 的原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-解决了什么问题"><span class="nav-text">BN 解决了什么问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-BN-有什么好处"><span class="nav-text">使用 BN 有什么好处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-放在不同位置的区别"><span class="nav-text">BN 放在不同位置的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-中-batch-的大小对网络性能有什么影响"><span class="nav-text">BN 中 batch 的大小对网络性能有什么影响</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-中线性偏移的参数个数怎么计算的"><span class="nav-text">BN 中线性偏移的参数个数怎么计算的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-中的使用的均值和方差是如何求得的"><span class="nav-text">BN 中的使用的均值和方差是如何求得的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在多卡训练使用-BN-时-需要注意什么问题"><span class="nav-text">在多卡训练使用 BN 时, 需要注意什么问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BN-在-Inference-阶段的加速"><span class="nav-text">BN 在 Inference 阶段的加速</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用-BN-时-前一层的卷积网络需不需要偏置项-为什么"><span class="nav-text">使用 BN 时, 前一层的卷积网络需不需要偏置项, 为什么</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Group-Normalization"><span class="nav-text">Group Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-GN-的原理"><span class="nav-text">简述 GN 的原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么-GN-效果好"><span class="nav-text">为什么 GN 效果好</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#简述-BN-LN-IN-GN-的区别"><span class="nav-text">简述 BN, LN, IN, GN 的区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GN-中线性偏移的参数个数怎么计算的"><span class="nav-text">GN 中线性偏移的参数个数怎么计算的</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-Normalization"><span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Instance-Normalization"><span class="nav-text">Instance Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Switchable-Normalization"><span class="nav-text">Switchable Normalization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#感受野"><span class="nav-text">感受野</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#感受野的计算公式"><span class="nav-text">感受野的计算公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#理论感受野和有效感受野的区别"><span class="nav-text">理论感受野和有效感受野的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标检测中的-anchor-的设置和感受野的大小之间有什么关系"><span class="nav-text">目标检测中的 anchor 的设置和感受野的大小之间有什么关系?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#全连接层"><span class="nav-text">全连接层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#全连接层的作用是什么"><span class="nav-text">全连接层的作用是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#将全连接层转换成卷积层由什么好处"><span class="nav-text">将全连接层转换成卷积层由什么好处</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推导两层全连接网络的反向传播公式"><span class="nav-text">推导两层全连接网络的反向传播公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层"><span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积层的计算公式"><span class="nav-text">卷积层的计算公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-1x1-卷积层的作用"><span class="nav-text">简述 1x1 卷积层的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积操作的本质特性包括稀疏交互和参数共享-具体解释这两种特性及其作用"><span class="nav-text">卷积操作的本质特性包括稀疏交互和参数共享, 具体解释这两种特性及其作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积层实现如何优化"><span class="nav-text">卷积层实现如何优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#im2col-优化"><span class="nav-text">im2col 优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#空间组合优化算法"><span class="nav-text">空间组合优化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Winograd-优化算法"><span class="nav-text">Winograd 优化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#量化神经网络的优化方法"><span class="nav-text">量化神经网络的优化方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参考"><span class="nav-text">参考</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实现矩阵乘法并简述其优化方法"><span class="nav-text">实现矩阵乘法并简述其优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Strassen-算法"><span class="nav-text">Strassen 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Winograd-算法"><span class="nav-text">Winograd 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#通过划分降低访存次数"><span class="nav-text">通过划分降低访存次数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化内存布局进一步降低访存"><span class="nav-text">优化内存布局进一步降低访存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络量化中的矩阵乘法优化"><span class="nav-text">神经网络量化中的矩阵乘法优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积核的大小如何确定"><span class="nav-text">卷积核的大小如何确定</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化层"><span class="nav-text">池化层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#池化层的作用是什么"><span class="nav-text">池化层的作用是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#池化层反向传播的梯度时如何求的"><span class="nav-text">池化层反向传播的梯度时如何求的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最大池化和平均池化有什么异同-分别适用于什么场景"><span class="nav-text">最大池化和平均池化有什么异同, 分别适用于什么场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#全局平均池化层-GAP-的作用"><span class="nav-text">全局平均池化层(GAP)的作用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反卷积层"><span class="nav-text">反卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#反卷积和双线性插值的区别-各自的优势"><span class="nav-text">反卷积和双线性插值的区别, 各自的优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反卷积计算公式"><span class="nav-text">反卷积计算公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#空洞卷积"><span class="nav-text">空洞卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#空洞卷积的作用"><span class="nav-text">空洞卷积的作用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练问题"><span class="nav-text">训练问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消失和梯度爆炸的产生原因及解决方法"><span class="nav-text">梯度消失和梯度爆炸的产生原因及解决方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在图像分类任务中-训练数据不足会带来什么问题-如何缓解数据量不足带来的问题"><span class="nav-text">在图像分类任务中, 训练数据不足会带来什么问题, 如何缓解数据量不足带来的问题?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何解决数据不均衡问题"><span class="nav-text">如何解决数据不均衡问题?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练不收敛的具体表现是什么-可能的原因是什么-如何解决"><span class="nav-text">训练不收敛的具体表现是什么? 可能的原因是什么? 如何解决?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练过程中出现-Nan-值是什么原因-如何解决"><span class="nav-text">训练过程中出现 Nan 值是什么原因? 如何解决?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合是什么-如何处理过拟合"><span class="nav-text">过拟合是什么? 如何处理过拟合?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#欠拟合是什么-如何处理欠拟合"><span class="nav-text">欠拟合是什么? 如何处理欠拟合?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-的实现方式在训练阶段和测试阶段有什么不同-如何保持训练和测试阶段的一致性"><span class="nav-text">Dropout 的实现方式在训练阶段和测试阶段有什么不同? 如何保持训练和测试阶段的一致性?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-为什么可以起到防止过拟合的作用"><span class="nav-text">Dropout 为什么可以起到防止过拟合的作用?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的数据增强方法"><span class="nav-text">常用的数据增强方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的训练-Trick-有哪些-分别介绍"><span class="nav-text">常用的训练 Trick 有哪些, 分别介绍</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#网络结构篇"><span class="nav-text">网络结构篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGGNet"><span class="nav-text">VGGNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InceptionV1-GoogLeNet"><span class="nav-text">InceptionV1 (GoogLeNet)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述一下-GoogLeNet-采用多个卷积核的原因"><span class="nav-text">简述一下 GoogLeNet 采用多个卷积核的原因</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-中为什么使用-1×1-卷积层"><span class="nav-text">Inception 中为什么使用 1×1 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-中为什么使用全局平均池化层"><span class="nav-text">Inception 中为什么使用全局平均池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么使用侧枝"><span class="nav-text">为什么使用侧枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GoogLeNet-在哪些地方使用了全连接层"><span class="nav-text">GoogLeNet 在哪些地方使用了全连接层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InceptionV2-3"><span class="nav-text">InceptionV2/3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-InceptionV2-相比于-GoogLeNet-有什么区别"><span class="nav-text">简述 InceptionV2 相比于 GoogLeNet 有什么区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-InceptionV3-相比于-GoogLeNet-有什么区别"><span class="nav-text">简述 InceptionV3 相比于 GoogLeNet 有什么区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Xception"><span class="nav-text">Xception</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-Xception-的特点"><span class="nav-text">简述 Xception 的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Xception-中使用的-“extreme”-inception-module-的-MobileNet-中使用的传统的-Depthwise-Separable-Conv-有什么区别"><span class="nav-text">Xception 中使用的 “extreme” inception module 的 MobileNet 中使用的传统的 Depthwise Separable Conv 有什么区别?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#InceptionV4-and-Inception-ResNet"><span class="nav-text">InceptionV4 and Inception ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#InceptionV4-做了哪些改进"><span class="nav-text">InceptionV4 做了哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-ResNet-v1-做了哪些改进"><span class="nav-text">Inception-ResNet-v1 做了哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-ResNet-v2-做了哪些改进"><span class="nav-text">Inception-ResNet-v2 做了哪些改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNet"><span class="nav-text">ResNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简单介绍一下-ResNet"><span class="nav-text">简单介绍一下 ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-ResNet-的提出动机"><span class="nav-text">简述 ResNet 的提出动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet-中可以使用哪些短接方式"><span class="nav-text">ResNet 中可以使用哪些短接方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何理解所谓的残差-F-x-比原始目标-H-x-更容易优化"><span class="nav-text">如何理解所谓的残差 $F(x)$ 比原始目标 $H(x)$ 更容易优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么恒等映射x之前的系数是1-而不是其他的值-比如0-5"><span class="nav-text">为什么恒等映射x之前的系数是1,而不是其他的值, 比如0.5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet-为什么好"><span class="nav-text">ResNet 为什么好</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet-残差模块中激活层应该如何放置"><span class="nav-text">ResNet 残差模块中激活层应该如何放置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ResNeXt"><span class="nav-text">ResNeXt</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNeXt-在-ResNet-上做了哪些改进"><span class="nav-text">ResNeXt 在 ResNet 上做了哪些改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseNet"><span class="nav-text">DenseNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-DenseNet-的原理"><span class="nav-text">简述 DenseNet 的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DenseBlock-每一层输出的特征图谱是怎么构成的-如何确定输出通道数"><span class="nav-text">DenseBlock 每一层输出的特征图谱是怎么构成的, 如何确定输出通道数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transition-Layer-怎么构成的-有什么作用"><span class="nav-text">Transition Layer 怎么构成的, 有什么作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深层的网络层输入的特征图谱很大-怎么解决"><span class="nav-text">深层的网络层输入的特征图谱很大, 怎么解决</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SqueezeNet"><span class="nav-text">SqueezeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-SqueezeNet-的原理"><span class="nav-text">简述 SqueezeNet 的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SqueezeNet-的网络结构"><span class="nav-text">SqueezeNet 的网络结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNet"><span class="nav-text">MobileNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-MobileNet"><span class="nav-text">简述 MobileNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推导-Separable-Conv-的参数量和计算量"><span class="nav-text">推导 Separable Conv 的参数量和计算量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNets-为什么快"><span class="nav-text">MobileNets 为什么快</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MobileNetV2"><span class="nav-text">MobileNetV2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNetV2-做了哪些改进"><span class="nav-text">MobileNetV2 做了哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNetV1-2-中为什么使用-ReLU6-哪些层后面不用-ReLU-激活-为什么"><span class="nav-text">MobileNetV1/2 中为什么使用 ReLU6? 哪些层后面不用 ReLU 激活? 为什么?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MobileNetV2-的网络结构及与其他轻量级网络的区别"><span class="nav-text">MobileNetV2 的网络结构及与其他轻量级网络的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ShuffleNet"><span class="nav-text">ShuffleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-ShuffleNet"><span class="nav-text">简述 ShuffleNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleNet-的网络结构"><span class="nav-text">ShuffleNet 的网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleNet-的计算量推导"><span class="nav-text">ShuffleNet 的计算量推导</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ShuffleNetV2"><span class="nav-text">ShuffleNetV2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleNetV2-做了哪些改进"><span class="nav-text">ShuffleNetV2 做了哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleNetV2-的网络结构"><span class="nav-text">ShuffleNetV2 的网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#现有的轻量级网络存在哪些问题"><span class="nav-text">现有的轻量级网络存在哪些问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-ShuffleNetV1-2-和-MobileNetV1-2-的区别"><span class="nav-text">简述 ShuffleNetV1/2 和 MobileNetV1/2 的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SENet"><span class="nav-text">SENet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-SENet-的原理"><span class="nav-text">简述 SENet 的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SE-Block-放置的位置是否会严重影响性能"><span class="nav-text">SE-Block 放置的位置是否会严重影响性能?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SENet-的网络结构"><span class="nav-text">SENet 的网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#除了-SE-Block-还有哪些其他的-Attention-机制"><span class="nav-text">除了 SE-Block, 还有哪些其他的 Attention 机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Convolutional-Block-Attention-Module-CBAM"><span class="nav-text">Convolutional Block Attention Module (CBAM)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Block-Attention-Module-BAM"><span class="nav-text">Block Attention Module (BAM)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#目标检测篇"><span class="nav-text">目标检测篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#IoU"><span class="nav-text">IoU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mAP"><span class="nav-text">mAP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本概念-1"><span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AP-计算示例"><span class="nav-text">AP 计算示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mAP-计算代码实现"><span class="nav-text">mAP 计算代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mmAP"><span class="nav-text">mmAP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NMS"><span class="nav-text">NMS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简述-NMS-的原理"><span class="nav-text">简述 NMS 的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NMS-算法源码实现"><span class="nav-text">NMS 算法源码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Soft-NMS-简介"><span class="nav-text">Soft-NMS 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Soft-NMS-算法源码实现"><span class="nav-text">Soft-NMS 算法源码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他的-NMS-算法"><span class="nav-text">其他的 NMS 算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#R-CNN"><span class="nav-text">R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#R-CNN-简介"><span class="nav-text">R-CNN 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Selective-Search-简介"><span class="nav-text">Selective Search 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要-R-CNN-使用-SVM-而不用更加方便的-Softmax-分类器"><span class="nav-text">为什么要 R-CNN 使用 SVM 而不用更加方便的 Softmax 分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bounding-Box-的回归方式简介"><span class="nav-text">Bounding Box 的回归方式简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bounding-box-回归的时候-为什么不直接对坐标回归-而是采用偏移量和缩放度"><span class="nav-text">Bounding box 回归的时候, 为什么不直接对坐标回归, 而是采用偏移量和缩放度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么当-Region-Proposals-和-Ground-Truth-较接近时-即-IoU-较大时-可以认为是边框回归函数是线性变换"><span class="nav-text">为什么当 Region Proposals 和 Ground Truth 较接近时, 即 IoU 较大时, 可以认为是边框回归函数是线性变换?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#R-CNN-缺点"><span class="nav-text">R-CNN 缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SPPNet"><span class="nav-text">SPPNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SPPNet-简介"><span class="nav-text">SPPNet 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SPPNet-缺点"><span class="nav-text">SPPNet 缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN"><span class="nav-text">Fast R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fast-R-CNN-有哪些改进"><span class="nav-text">Fast R-CNN 有哪些改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoI-Pooling-简介"><span class="nav-text">RoI Pooling 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoI-Pooling-如何进行反向传播"><span class="nav-text">RoI Pooling 如何进行反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-RoI-Pooling-比-SPP-效果好"><span class="nav-text">为什么 RoI Pooling 比 SPP 效果好</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fast-R-CNN-的-Multi-task-Loss"><span class="nav-text">Fast R-CNN 的 Multi-task Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Smooth-L1-相比于-L2-损失-在回归时有什么优势"><span class="nav-text">Smooth L1 相比于 L2 损失, 在回归时有什么优势</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVG-奇异值分解简介"><span class="nav-text">SVG 奇异值分解简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-text">Faster R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Faster-R-CNN-简介"><span class="nav-text">Faster R-CNN 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正负样本标定策略"><span class="nav-text">正负样本标定策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数-1"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共享参数训练方法"><span class="nav-text">共享参数训练方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如果两个物体重合度很高-Faster-R-CNN-会怎么样"><span class="nav-text">如果两个物体重合度很高, Faster R-CNN 会怎么样?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Mask-R-CNN"><span class="nav-text">Mask R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mask-R-CNN-简介"><span class="nav-text">Mask R-CNN 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoI-Align"><span class="nav-text">RoI Align</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mask-分支怎么实现的"><span class="nav-text">Mask 分支怎么实现的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他"><span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FPN"><span class="nav-text">FPN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FPN-简介"><span class="nav-text">FPN 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FPN-的使用-RPN-Fast-R-CNN"><span class="nav-text">FPN 的使用(RPN, Fast R-CNN)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FCN"><span class="nav-text">FCN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FCN-简介"><span class="nav-text">FCN 简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#R-FCN"><span class="nav-text">R-FCN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#R-FCN-简介"><span class="nav-text">R-FCN 简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CoupleNet"><span class="nav-text">CoupleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CoupleNet-简介"><span class="nav-text">CoupleNet 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CoupleNet-是怎么结合-Local-Feature-和-Global-Feature-的"><span class="nav-text">CoupleNet 是怎么结合 Local Feature 和 Global Feature 的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CoupleNet-为什么可以解决遮挡类问题"><span class="nav-text">CoupleNet 为什么可以解决遮挡类问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deformable-ConvNets-V1"><span class="nav-text">Deformable ConvNets V1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deformable-ConvNets-V1-简介"><span class="nav-text">Deformable ConvNets V1 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deformable-模块具体是怎么实现的"><span class="nav-text">Deformable 模块具体是怎么实现的</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deformable-ConvNets-V2"><span class="nav-text">Deformable ConvNets V2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deformable-ConvNets-V1-存在哪些问题"><span class="nav-text">Deformable ConvNets V1 存在哪些问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deformable-ConvNets-V2-做了哪些改进"><span class="nav-text">Deformable ConvNets V2 做了哪些改进</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cascade-R-CNN"><span class="nav-text">Cascade R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cascade-R-CNN-简介"><span class="nav-text">Cascade R-CNN 简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SNIP"><span class="nav-text">SNIP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SNIP-简介"><span class="nav-text">SNIP 简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SNIPER"><span class="nav-text">SNIPER</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SNIPER-简介"><span class="nav-text">SNIPER 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SNIPER-小结"><span class="nav-text">SNIPER 小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SSD"><span class="nav-text">SSD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SSD-简介"><span class="nav-text">SSD 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSD-中如何计算-default-box-的大小"><span class="nav-text">SSD 中如何计算 default box 的大小</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSD-使用了哪些数据增广方法"><span class="nav-text">SSD 使用了哪些数据增广方法?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么SSD不直接使用浅层的特征图谱-而非要额外增加卷积层-这样不是增加模型的复杂度了吗"><span class="nav-text">为什么SSD不直接使用浅层的特征图谱, 而非要额外增加卷积层, 这样不是增加模型的复杂度了吗?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSD-PyTorch-源码实现"><span class="nav-text">SSD PyTorch 源码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RefineDet"><span class="nav-text">RefineDet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RefineDet-简介"><span class="nav-text">RefineDet 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#refinedet-使用了-two-stage-的边框回归过程-为什么还说它是-one-stage-模型"><span class="nav-text">refinedet 使用了 two-stage 的边框回归过程, 为什么还说它是 one-stage 模型?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RFBNet"><span class="nav-text">RFBNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RFBNet-简介"><span class="nav-text">RFBNet 简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TridentNet"><span class="nav-text">TridentNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TridentNet-简介"><span class="nav-text">TridentNet 简介</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#M2Det"><span class="nav-text">M2Det</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv1"><span class="nav-text">YOLOv1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv2"><span class="nav-text">YOLOv2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv3"><span class="nav-text">YOLOv3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#YOLO-系列为什么速度这么快"><span class="nav-text">YOLO 系列为什么速度这么快?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OHEM"><span class="nav-text">OHEM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Focal-Loss-1"><span class="nav-text">Focal Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Focal-Loss-简介"><span class="nav-text">Focal Loss 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Focal-Loss-的两个参数各自的作用是什么-二者有什么关系"><span class="nav-text">Focal Loss 的两个参数各自的作用是什么? 二者有什么关系?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#R-CNN-系列为什么不用-Focal-Loss-进行优化"><span class="nav-text">R-CNN 系列为什么不用 Focal Loss 进行优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RetinaNet-的结构"><span class="nav-text">RetinaNet 的结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseBox"><span class="nav-text">DenseBox</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CornerNet"><span class="nav-text">CornerNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#检测-Corner-相对于检测-bbox-的优势"><span class="nav-text">检测 Corner 相对于检测 bbox 的优势</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CornerNet-Lite"><span class="nav-text">CornerNet-Lite</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CornerNet-Squeeze-Saccade"><span class="nav-text">CornerNet-Squeeze-Saccade</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FSAF"><span class="nav-text">FSAF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FSAF-简介"><span class="nav-text">FSAF 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Groud-truch-and-Loss"><span class="nav-text">Groud-truch and Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Online-Feature-Selection"><span class="nav-text">Online Feature Selection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FoveaBox"><span class="nav-text">FoveaBox</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FoveaBox-简介"><span class="nav-text">FoveaBox 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FoveaBox-的-Scale-Assignment"><span class="nav-text">FoveaBox 的 Scale Assignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FoveaBox-与其他模型的比较"><span class="nav-text">FoveaBox 与其他模型的比较</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FCOS"><span class="nav-text">FCOS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FCOS-简介"><span class="nav-text">FCOS 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标重叠问题"><span class="nav-text">目标重叠问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Center-ness"><span class="nav-text">Center-ness</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FCOS-与其他模型的比较"><span class="nav-text">FCOS 与其他模型的比较</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ExtremeNet"><span class="nav-text">ExtremeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ExtremeNet-的分组策略"><span class="nav-text">ExtremeNet 的分组策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ghost-box-suppression-分组存在的问题"><span class="nav-text">Ghost box suppression(分组存在的问题)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Edge-aggregation"><span class="nav-text">Edge aggregation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#extreme-points-与-corner-points-的区别"><span class="nav-text">extreme points 与 corner points 的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CenterNet"><span class="nav-text">CenterNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Enriching-Center-and-Corner-Information"><span class="nav-text">Enriching Center and Corner Information</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CenterNet-Objects-as-Points"><span class="nav-text">CenterNet(Objects as Points)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介-1"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CenterNet-Points-与其他-One-Stage-方法的区别"><span class="nav-text">CenterNet(Points) 与其他 One-Stage 方法的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他-1"><span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常见问题篇"><span class="nav-text">常见问题篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#目前的-SOTA-目标检测模型"><span class="nav-text">目前的 SOTA 目标检测模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SSD-FPN-RefineDet-PFPNet-STDN-M2Det-等特征金字塔的区别"><span class="nav-text">SSD, FPN, RefineDet, PFPNet, STDN, M2Det 等特征金字塔的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用的训练-Trick"><span class="nav-text">常用的训练 Trick</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用的数据增广方法"><span class="nav-text">常用的数据增广方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FCN-是如何降低计算量的"><span class="nav-text">FCN 是如何降低计算量的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch-和-TensorFlow-的区别"><span class="nav-text">PyTorch 和 TensorFlow 的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标检测领域方向可以继续改进或者优化的地方"><span class="nav-text">目标检测领域方向可以继续改进或者优化的地方</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#浅层特征图检测小目标，为什么不同时也检测大目标"><span class="nav-text">浅层特征图检测小目标，为什么不同时也检测大目标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU-两个重要指标之间的关系"><span class="nav-text">GPU 两个重要指标之间的关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络显存占用"><span class="nav-text">神经网络显存占用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#节省显存的方法"><span class="nav-text">节省显存的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#各个网络层的参数量"><span class="nav-text">各个网络层的参数量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FLOPs-计算"><span class="nav-text">FLOPs 计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#除-FLOPs-外其他影响模型速度的因素"><span class="nav-text">除 FLOPs 外其他影响模型速度的因素</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Anchor-的作用"><span class="nav-text">Anchor 的作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#R-CNN-系列类别冲突问题"><span class="nav-text">R-CNN 系列类别冲突问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Anchor-的数量"><span class="nav-text">Anchor 的数量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Anchor-based-方法和-Anchor-free-方法比较"><span class="nav-text">Anchor based 方法和 Anchor free 方法比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-的平移不变性"><span class="nav-text">CNN 的平移不变性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小目标检测"><span class="nav-text">小目标检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#样本不均衡问题"><span class="nav-text">样本不均衡问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#遮挡问题"><span class="nav-text">遮挡问题</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZeroZone</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">3.1m</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
    <span title="站点阅读时长">47:25</span>
  
</div>










  <div class="footer-custom">勤练带来力量</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  





  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'o5ny24Rtrv0pjlRYjBoj9rfz-gzGzoHsz',
        appKey: 'o9SAGYkO04n5xjXkeWXaq1pm',
        placeholder: '无需注册即可评论, 支持Markdown(可手动预览), 支持在 Gravatar(https://cn.gravatar.com) 上自定义头像, 评论时只需填写对应邮箱即可显示自定义头像, 邮箱不会暴露在评论处, 大可放心, 由于无登陆选项, 因此邮箱会作为我联系你的唯一方式',
        avatar:'',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
